{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_"
      },
      "source": [
        "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n",
        "\n",
        "> Author: Howard Wang (b08902047@ntu.edu.tw)\n",
        "\n",
        "If there are any questions, please contact mlta-2023-spring@googlegroups.com"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR"
      },
      "source": [
        "# Readme\n",
        "\n",
        "In homework 11, you will need to implement Domain Adversarial Training in Transfer Learning. As shown in the bottom left part of the figure.\n",
        "\n",
        "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
        "\n",
        "> \n",
        "\n",
        "## Scenario and Why Domain Adversarial Training\n",
        "Now we have labeled source data and unlabeled target data, where source data might be relavent to the target data. We now want to train a model with source data only and test it on target data.\n",
        "\n",
        "What problem might occur if we do so? After we have learned Anomaly Detection, we now know that if we test the model with an abnormal data that have never appeared in source data, our trained model is likely to result in poor performance since it is not familiar with the abnormal data.\n",
        "\n",
        "For example, we have a model that contains Feature Extractor and Classifier:\n",
        "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
        "\n",
        "When the model is trained with source data, the feature extractor \n",
        "will extract meaningful features since it is familiar with the distribution of it.It could be seen in the following figure that the blue dots, which is the distribution of source data, has already been clustered into different clusters. Therefore, the Classifier can predict the label based on these clusters.\n",
        "\n",
        "However, when test on the target data, the Feature Extractor will not be able to extract meaningful features that follow the distribution of the source feature distribution, which result in the classifier learned for the source domain will not be able to apply to the target domain.\n",
        "\n",
        "\n",
        "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
        "\n",
        "Based on the above problems, DaNN approaches build mappings between the source (training-time) and the target (test-time) domains, so that the classifier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
        "\n",
        "In DaNN, the authors added a Domain Classifier, which is a deep discriminatively-trained classifeir in the training framework to distinguish the data from different domain by the features extracted by the feature extractor. As the training progresses, the approach promotes a domain classifier that discriminates between the source and the target domains and a feature extractor that can extractor features that are discriminative for the main learning task on the source domain and indiscriminate with respect to the shift between the domains. \n",
        "\n",
        "\n",
        "The feature extractor are likely to outperform the domain classifier as its input are generated by the feature extractor and that the task of domain classification and label classification are not conflict.\n",
        "\n",
        "This method leads to the emergence of features that are domain-invariant and on the same feature distribution."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qnUkspmap3"
      },
      "source": [
        "# Data Introduce\n",
        "\n",
        "Our task contains source data: real photos, and target data: hand-drawn graffiti.\n",
        "\n",
        "We are going to train the model with the photos and the labels, and try to predict what the labels are for hand-drawn graffiti.\n",
        "\n",
        "The data could be downloaded [here](https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip). The code below is for data downloading and visualization.\n",
        "\n",
        "Note that: **The source and target data are all balanced data, you can make use of this information.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF-i0sVlnUbq",
        "outputId": "db9c3dbb-22d6-4c72-d6d5-7fa18322a51e"
      },
      "outputs": [],
      "source": [
        "# # Download dataset\n",
        "# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "\n",
        "# # Download from mirrored dataset link\n",
        "# # !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.1/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "# # !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.2/real_or_drawing.zip\" -O real_or_drawing.zip\n",
        "\n",
        "# # Unzip the files\n",
        "# !unzip real_or_drawing.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "0_uO-ZSDoR6i",
        "outputId": "72879497-ab04-4eac-9a61-aae79a399c96"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x1800 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def no_axis_show(img, title='', cmap=None):\n",
        "  # imshow, and set the interpolation mode to be \"nearest\"\n",
        "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
        "  # do not show the axes in the images.\n",
        "  fig.axes.get_xaxis().set_visible(False)\n",
        "  fig.axes.get_yaxis().set_visible(False)\n",
        "  plt.title(title)\n",
        "\n",
        "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 10, i+1)\n",
        "  fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "3eMs7DbVt4Ee",
        "outputId": "488467f8-66d1-4ee1-814e-46c9d9dc4ca3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x1800 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 10, i+1)\n",
        "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "moXQw9To5TqZ"
      },
      "source": [
        "# Special Domain Knowledge\n",
        "\n",
        "When we graffiti, we usually draw the outline only, therefore we can perform edge detection processing on the source data to make it more similar to the target data.\n",
        "\n",
        "\n",
        "## Canny Edge Detection\n",
        "The implementation of Canny Edge Detection is as follow.\n",
        "The algorithm will not be describe thoroughly here.  If you are interested, please refer to the wiki or [here](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19).\n",
        "\n",
        "We only need two parameters to implement Canny Edge Detection with CV2:  `low_threshold` and `high_threshold`.\n",
        "\n",
        "```cv2.Canny(image, low_threshold, high_threshold)```\n",
        "\n",
        "Simply put, when the edge value exceeds the high_threshold, we determine it as an edge. If the edge value is only above low_threshold, we will then determine whether it is an edge or not.\n",
        "\n",
        "Let's implement it on the source data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "mn2MkDLV7E2-",
        "outputId": "b9294566-ffa8-42da-82a6-0ac7a1c5638b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x1800 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "plt.figure(figsize=(18, 18))\n",
        "\n",
        "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
        "plt.subplot(1, 5, 1)\n",
        "no_axis_show(original_img, title='original')\n",
        "\n",
        "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "plt.subplot(1, 5, 2)\n",
        "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "plt.subplot(1, 5, 2)\n",
        "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
        "plt.subplot(1, 5, 3)\n",
        "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
        "\n",
        "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
        "plt.subplot(1, 5, 4)\n",
        "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
        "\n",
        "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
        "plt.subplot(1, 5, 5)\n",
        "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
        "  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8THSdt_hmwYh"
      },
      "source": [
        "# Data Process\n",
        " \n",
        " \n",
        "The data is suitible for `torchvision.ImageFolder`. You can create a dataset with `torchvision.ImageFolder`. Details for image augmentation please refer to the comments in the following codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WZHIBGknmi8Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        " \n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        " \n",
        "\n",
        "from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn, track\n",
        "import time\n",
        "\n",
        "\n",
        "source_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
        "    transforms.Grayscale(),\n",
        "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
        "    # and then adopt cv2.Canny algorithm.\n",
        "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
        "    # Transform np.array back to the skimage.Image.\n",
        "    transforms.ToPILImage(),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale.\n",
        "    transforms.Grayscale(),\n",
        "    # Resize: size of source data is 32x32, thus we need to \n",
        "    #  enlarge the size of target data from 28x28 to 32x32\n",
        "    transforms.Resize((32, 32)),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        " \n",
        "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
        "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
        " \n",
        "source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
        "target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwDEMrOycs5"
      },
      "source": [
        "# Model\n",
        "\n",
        "Feature Extractor: Classic VGG-like architecture\n",
        "\n",
        "Label Predictor / Domain Classifier: Linear models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3uw2eP09z-pD"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x).squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdBIPhF0Icb"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "Here we use Adam as our optimizor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hrxKelBy0PJ7"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "domain_classifier = DomainClassifier().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
        "optimizer_C = optim.Adam(label_predictor.parameters())\n",
        "optimizer_D = optim.Adam(domain_classifier.parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAE4cqJ0itR"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "\n",
        "## DaNN Implementation\n",
        "\n",
        "In the original paper, Gradient Reversal Layer is used.\n",
        "Feature Extractor, Label Predictor, and Domain Classifier are all trained at the same time. In this code, we train Domain Classifier first, and then train our Feature Extractor (same concept as Generator and Discriminator training process in GAN).\n",
        "\n",
        "## Reminder\n",
        "* Lambda, which controls the domain adversarial loss, is adaptive in the original paper. You can refer to [the original work](https://arxiv.org/pdf/1505.07818.pdf) . Here lambda is set to 0.1.\n",
        "* We do not have the label to target data, you can only evaluate your model by uploading your result to kaggle.:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adaptive_lambda(epoch, num_epoch):\n",
        "    p = epoch / num_epoch\n",
        "    return 2. / (1+np.exp(-10*p)) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2c865516acc64642862cfee4a814a3ae",
            "ea4e9e6cd3794596932e43e41caa30f9"
          ]
        },
        "id": "lRAFFKvX0p9y",
        "outputId": "e2314aa7-0c4b-416a-913b-60298edfb865"
      },
      "outputs": [],
      "source": [
        "# def train_epoch(source_dataloader, target_dataloader, progress, lamb):\n",
        "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source datadataloader\n",
        "        target_dataloader: target datadataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifierloss\n",
        "    # F loss: Feature Extrator & Label Predictorloss\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "    # batch_tqdm = progress.add_task(description=f\"batch_progress\", total=len(source_dataloader))\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        # Mixed the source data and target data, or it'll mislead the running params\n",
        "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        # set domain label of source data to be 1.\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        # Step 1 : train domain classifier\n",
        "        feature = feature_extractor(mixed_data)\n",
        "        # We don't need to train feature extractor in step 1.\n",
        "        # Thus we detach the feature neuron to avoid backpropgation.\n",
        "        domain_logits = domain_classifier(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Step 2 : train feature extractor and label classifier\n",
        "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
        "        domain_logits = domain_classifier(feature)\n",
        "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
        "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
        "        running_F_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "        # progress.advance(batch_tqdm, advance=1)\n",
        "\n",
        "    # progress.remove_task(batch_tqdm)\n",
        "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
        "\n",
        "# num_epochs = 200\n",
        "# # train 200 epochs\n",
        "\n",
        "# with Progress(TextColumn(\"[progress.description]{task.description}\"),\n",
        "#               BarColumn(),\n",
        "#               TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
        "#               TimeRemainingColumn(),\n",
        "#               TimeElapsedColumn()) as progress:\n",
        "#     epoch_tqdm = progress.add_task(description=\"epoch progress\", total=num_epochs)\n",
        "#     for epoch in range(num_epochs):\n",
        "#         lamb = adaptive_lambda(epoch, num_epochs)\n",
        "#         train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, progress, lamb=lamb)\n",
        "            \n",
        "#         progress.advance(epoch_tqdm, advance=1)\n",
        "#         if epoch == num_epochs*10/200:\n",
        "#           torch.save(feature_extractor.state_dict(), f'extractor_model_early.bin')\n",
        "#           torch.save(label_predictor.state_dict(), f'predictor_model_early.bin')\n",
        "#         elif epoch == num_epochs*100/200:\n",
        "#           torch.save(feature_extractor.state_dict(), f'extractor_model_mid.bin')\n",
        "#           torch.save(label_predictor.state_dict(), f'predictor_model_mid.bin')\n",
        "          \n",
        "#         torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n",
        "#         torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n",
        "#         print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch   0: train D loss: 0.6502, train F loss: 0.0262, acc 0.9924\n",
            "epoch   1: train D loss: 0.6062, train F loss: 0.0244, acc 0.9908\n",
            "epoch   2: train D loss: 0.5702, train F loss: 0.0225, acc 0.9922\n",
            "epoch   3: train D loss: 0.5614, train F loss: 0.0270, acc 0.9910\n",
            "epoch   4: train D loss: 0.5397, train F loss: 0.0224, acc 0.9924\n",
            "epoch   5: train D loss: 0.5186, train F loss: 0.0254, acc 0.9918\n",
            "epoch   6: train D loss: 0.4923, train F loss: 0.0202, acc 0.9922\n",
            "epoch   7: train D loss: 0.4862, train F loss: 0.0107, acc 0.9946\n",
            "epoch   8: train D loss: 0.4755, train F loss: 0.0117, acc 0.9944\n",
            "epoch   9: train D loss: 0.4813, train F loss: 0.0161, acc 0.9942\n",
            "epoch  10: train D loss: 0.4775, train F loss: 0.0211, acc 0.9910\n",
            "epoch  11: train D loss: 0.4982, train F loss: 0.0101, acc 0.9954\n",
            "epoch  12: train D loss: 0.4869, train F loss: 0.0253, acc 0.9902\n",
            "epoch  13: train D loss: 0.4989, train F loss: 0.0144, acc 0.9930\n",
            "epoch  14: train D loss: 0.5068, train F loss: 0.0091, acc 0.9948\n",
            "epoch  15: train D loss: 0.5114, train F loss: 0.0072, acc 0.9946\n",
            "epoch  16: train D loss: 0.4973, train F loss: 0.0088, acc 0.9942\n",
            "epoch  17: train D loss: 0.5100, train F loss: 0.0297, acc 0.9890\n",
            "epoch  18: train D loss: 0.5066, train F loss: 0.0181, acc 0.9916\n",
            "epoch  19: train D loss: 0.5206, train F loss: 0.0083, acc 0.9938\n",
            "epoch  20: train D loss: 0.5166, train F loss: 0.0291, acc 0.9886\n",
            "epoch  21: train D loss: 0.5205, train F loss: 0.0072, acc 0.9944\n",
            "epoch  22: train D loss: 0.5054, train F loss: 0.0025, acc 0.9952\n",
            "epoch  23: train D loss: 0.5203, train F loss: 0.0037, acc 0.9950\n",
            "epoch  24: train D loss: 0.5284, train F loss: 0.0096, acc 0.9932\n",
            "epoch  25: train D loss: 0.5312, train F loss: 0.0057, acc 0.9936\n",
            "epoch  26: train D loss: 0.5269, train F loss: 0.0033, acc 0.9942\n",
            "epoch  27: train D loss: 0.5443, train F loss: 0.0097, acc 0.9930\n",
            "epoch  28: train D loss: 0.5481, train F loss: 0.0149, acc 0.9912\n",
            "epoch  29: train D loss: 0.5426, train F loss: 0.0058, acc 0.9934\n",
            "epoch  30: train D loss: 0.5546, train F loss: 0.0154, acc 0.9914\n",
            "epoch  31: train D loss: 0.5422, train F loss: -0.0008, acc 0.9948\n",
            "epoch  32: train D loss: 0.5416, train F loss: 0.0003, acc 0.9948\n",
            "epoch  33: train D loss: 0.5491, train F loss: -0.0064, acc 0.9958\n",
            "epoch  34: train D loss: 0.5562, train F loss: -0.0101, acc 0.9964\n",
            "epoch  35: train D loss: 0.5589, train F loss: -0.0085, acc 0.9958\n",
            "epoch  36: train D loss: 0.5630, train F loss: -0.0070, acc 0.9960\n",
            "epoch  37: train D loss: 0.5731, train F loss: 0.0029, acc 0.9934\n",
            "epoch  38: train D loss: 0.5726, train F loss: 0.0135, acc 0.9912\n",
            "epoch  39: train D loss: 0.5658, train F loss: -0.0079, acc 0.9954\n",
            "epoch  40: train D loss: 0.5716, train F loss: 0.0112, acc 0.9914\n",
            "epoch  41: train D loss: 0.5577, train F loss: -0.0065, acc 0.9944\n",
            "epoch  42: train D loss: 0.5727, train F loss: -0.0027, acc 0.9946\n",
            "epoch  43: train D loss: 0.5591, train F loss: -0.0088, acc 0.9954\n",
            "epoch  44: train D loss: 0.5828, train F loss: -0.0060, acc 0.9940\n",
            "epoch  45: train D loss: 0.5698, train F loss: 0.0098, acc 0.9898\n",
            "epoch  46: train D loss: 0.5765, train F loss: 0.0071, acc 0.9904\n",
            "epoch  47: train D loss: 0.5879, train F loss: -0.0096, acc 0.9952\n",
            "epoch  48: train D loss: 0.5672, train F loss: -0.0124, acc 0.9954\n",
            "epoch  49: train D loss: 0.5694, train F loss: -0.0132, acc 0.9954\n",
            "epoch  50: train D loss: 0.5655, train F loss: -0.0175, acc 0.9962\n",
            "epoch  51: train D loss: 0.5730, train F loss: -0.0154, acc 0.9954\n",
            "epoch  52: train D loss: 0.5708, train F loss: -0.0105, acc 0.9940\n",
            "epoch  53: train D loss: 0.5663, train F loss: -0.0189, acc 0.9958\n",
            "epoch  54: train D loss: 0.5668, train F loss: -0.0195, acc 0.9958\n",
            "epoch  55: train D loss: 0.5827, train F loss: -0.0105, acc 0.9934\n",
            "epoch  56: train D loss: 0.5855, train F loss: -0.0003, acc 0.9904\n",
            "epoch  57: train D loss: 0.5817, train F loss: -0.0096, acc 0.9932\n",
            "epoch  58: train D loss: 0.5856, train F loss: -0.0065, acc 0.9922\n",
            "epoch  59: train D loss: 0.5724, train F loss: -0.0127, acc 0.9946\n",
            "epoch  60: train D loss: 0.5839, train F loss: -0.0168, acc 0.9938\n",
            "epoch  61: train D loss: 0.6117, train F loss: 0.0743, acc 0.9734\n",
            "epoch  62: train D loss: 0.5765, train F loss: -0.0094, acc 0.9916\n",
            "epoch  63: train D loss: 0.5720, train F loss: -0.0159, acc 0.9934\n",
            "epoch  64: train D loss: 0.5626, train F loss: -0.0192, acc 0.9942\n",
            "epoch  65: train D loss: 0.5725, train F loss: -0.0240, acc 0.9962\n",
            "epoch  66: train D loss: 0.5597, train F loss: -0.0263, acc 0.9966\n",
            "epoch  67: train D loss: 0.5797, train F loss: -0.0225, acc 0.9948\n",
            "epoch  68: train D loss: 0.5754, train F loss: -0.0267, acc 0.9956\n",
            "epoch  69: train D loss: 0.5762, train F loss: -0.0234, acc 0.9948\n",
            "epoch  70: train D loss: 0.5857, train F loss: -0.0263, acc 0.9946\n",
            "epoch  71: train D loss: 0.5807, train F loss: -0.0265, acc 0.9954\n",
            "epoch  72: train D loss: 0.5882, train F loss: -0.0285, acc 0.9954\n",
            "epoch  73: train D loss: 0.5999, train F loss: -0.0202, acc 0.9930\n",
            "epoch  74: train D loss: 0.5973, train F loss: -0.0221, acc 0.9930\n",
            "epoch  75: train D loss: 0.5796, train F loss: -0.0291, acc 0.9958\n",
            "epoch  76: train D loss: 0.6094, train F loss: -0.0128, acc 0.9910\n",
            "epoch  77: train D loss: 0.5923, train F loss: -0.0311, acc 0.9954\n",
            "epoch  78: train D loss: 0.5932, train F loss: -0.0320, acc 0.9950\n",
            "epoch  79: train D loss: 0.5858, train F loss: -0.0261, acc 0.9944\n",
            "epoch  80: train D loss: 0.5899, train F loss: -0.0334, acc 0.9962\n",
            "epoch  81: train D loss: 0.6006, train F loss: -0.0316, acc 0.9950\n",
            "epoch  82: train D loss: 0.6046, train F loss: -0.0251, acc 0.9924\n",
            "epoch  83: train D loss: 0.6039, train F loss: -0.0309, acc 0.9944\n",
            "epoch  84: train D loss: 0.6067, train F loss: -0.0353, acc 0.9950\n",
            "epoch  85: train D loss: 0.5956, train F loss: -0.0378, acc 0.9958\n",
            "epoch  86: train D loss: 0.5910, train F loss: -0.0418, acc 0.9966\n",
            "epoch  87: train D loss: 0.6199, train F loss: 0.0016, acc 0.9848\n",
            "epoch  88: train D loss: 0.6115, train F loss: -0.0300, acc 0.9926\n",
            "epoch  89: train D loss: 0.6064, train F loss: -0.0313, acc 0.9918\n",
            "epoch  90: train D loss: 0.6056, train F loss: -0.0382, acc 0.9942\n",
            "epoch  91: train D loss: 0.6012, train F loss: -0.0374, acc 0.9940\n",
            "epoch  92: train D loss: 0.5894, train F loss: -0.0417, acc 0.9960\n",
            "epoch  93: train D loss: 0.6069, train F loss: -0.0417, acc 0.9948\n",
            "epoch  94: train D loss: 0.6089, train F loss: -0.0364, acc 0.9936\n",
            "epoch  95: train D loss: 0.6106, train F loss: -0.0348, acc 0.9930\n",
            "epoch  96: train D loss: 0.5946, train F loss: -0.0379, acc 0.9952\n",
            "epoch  97: train D loss: 0.6017, train F loss: -0.0411, acc 0.9950\n",
            "epoch  98: train D loss: 0.6014, train F loss: -0.0438, acc 0.9952\n",
            "epoch  99: train D loss: 0.6018, train F loss: -0.0415, acc 0.9954\n",
            "epoch 100: train D loss: 0.6089, train F loss: -0.0435, acc 0.9950\n",
            "epoch 101: train D loss: 0.6048, train F loss: -0.0488, acc 0.9956\n",
            "epoch 102: train D loss: 0.5971, train F loss: -0.0431, acc 0.9948\n",
            "epoch 103: train D loss: 0.6029, train F loss: -0.0401, acc 0.9924\n",
            "epoch 104: train D loss: 0.6016, train F loss: -0.0483, acc 0.9952\n",
            "epoch 105: train D loss: 0.6074, train F loss: -0.0413, acc 0.9936\n",
            "epoch 106: train D loss: 0.6067, train F loss: -0.0462, acc 0.9940\n",
            "epoch 107: train D loss: 0.5996, train F loss: -0.0443, acc 0.9946\n",
            "epoch 108: train D loss: 0.6140, train F loss: -0.0543, acc 0.9958\n",
            "epoch 109: train D loss: 0.6146, train F loss: -0.0447, acc 0.9934\n",
            "epoch 110: train D loss: 0.6102, train F loss: -0.0480, acc 0.9948\n",
            "epoch 111: train D loss: 0.6060, train F loss: -0.0496, acc 0.9950\n",
            "epoch 112: train D loss: 0.6113, train F loss: -0.0503, acc 0.9936\n",
            "epoch 113: train D loss: 0.6033, train F loss: -0.0573, acc 0.9964\n",
            "epoch 114: train D loss: 0.6039, train F loss: -0.0572, acc 0.9966\n",
            "epoch 115: train D loss: 0.6179, train F loss: -0.0448, acc 0.9910\n",
            "epoch 116: train D loss: 0.6113, train F loss: -0.0506, acc 0.9940\n",
            "epoch 117: train D loss: 0.6176, train F loss: -0.0512, acc 0.9942\n",
            "epoch 118: train D loss: 0.6159, train F loss: -0.0574, acc 0.9958\n",
            "epoch 119: train D loss: 0.6138, train F loss: -0.0590, acc 0.9956\n",
            "epoch 120: train D loss: 0.6213, train F loss: -0.0543, acc 0.9948\n",
            "epoch 121: train D loss: 0.6326, train F loss: -0.0496, acc 0.9904\n",
            "epoch 122: train D loss: 0.6302, train F loss: -0.0182, acc 0.9848\n",
            "epoch 123: train D loss: 0.6179, train F loss: -0.0582, acc 0.9950\n",
            "epoch 124: train D loss: 0.6160, train F loss: -0.0601, acc 0.9948\n",
            "epoch 125: train D loss: 0.6053, train F loss: -0.0578, acc 0.9954\n",
            "epoch 126: train D loss: 0.6034, train F loss: -0.0549, acc 0.9940\n",
            "epoch 127: train D loss: 0.6049, train F loss: -0.0602, acc 0.9954\n",
            "epoch 128: train D loss: 0.6156, train F loss: -0.0609, acc 0.9946\n",
            "epoch 129: train D loss: 0.6215, train F loss: -0.0649, acc 0.9954\n",
            "epoch 130: train D loss: 0.6217, train F loss: -0.0643, acc 0.9946\n",
            "epoch 131: train D loss: 0.6183, train F loss: -0.0657, acc 0.9950\n",
            "epoch 132: train D loss: 0.6177, train F loss: -0.0565, acc 0.9936\n",
            "epoch 133: train D loss: 0.6114, train F loss: -0.0660, acc 0.9954\n",
            "epoch 134: train D loss: 0.6190, train F loss: -0.0595, acc 0.9934\n",
            "epoch 135: train D loss: 0.6201, train F loss: -0.0684, acc 0.9952\n",
            "epoch 136: train D loss: 0.6295, train F loss: -0.0571, acc 0.9924\n",
            "epoch 137: train D loss: 0.6163, train F loss: -0.0686, acc 0.9952\n",
            "epoch 138: train D loss: 0.6214, train F loss: -0.0708, acc 0.9954\n",
            "epoch 139: train D loss: 0.6275, train F loss: -0.0751, acc 0.9964\n",
            "epoch 140: train D loss: 0.6237, train F loss: -0.0729, acc 0.9956\n",
            "epoch 141: train D loss: 0.6326, train F loss: -0.0578, acc 0.9914\n",
            "epoch 142: train D loss: 0.6332, train F loss: -0.0682, acc 0.9932\n",
            "epoch 143: train D loss: 0.6277, train F loss: -0.0700, acc 0.9944\n",
            "epoch 144: train D loss: 0.6171, train F loss: -0.0628, acc 0.9930\n",
            "epoch 145: train D loss: 0.6144, train F loss: -0.0764, acc 0.9960\n",
            "epoch 146: train D loss: 0.6284, train F loss: -0.0629, acc 0.9932\n",
            "epoch 147: train D loss: 0.6215, train F loss: -0.0761, acc 0.9952\n",
            "epoch 148: train D loss: 0.6438, train F loss: -0.0468, acc 0.9866\n",
            "epoch 149: train D loss: 0.6155, train F loss: -0.0697, acc 0.9938\n",
            "epoch 150: train D loss: 0.6147, train F loss: -0.0723, acc 0.9946\n",
            "epoch 151: train D loss: 0.6192, train F loss: -0.0660, acc 0.9916\n",
            "epoch 152: train D loss: 0.6206, train F loss: -0.0721, acc 0.9940\n",
            "epoch 153: train D loss: 0.6188, train F loss: -0.0811, acc 0.9960\n",
            "epoch 154: train D loss: 0.6122, train F loss: -0.0749, acc 0.9954\n",
            "epoch 155: train D loss: 0.6169, train F loss: -0.0765, acc 0.9962\n",
            "epoch 156: train D loss: 0.6304, train F loss: -0.0538, acc 0.9906\n",
            "epoch 157: train D loss: 0.6199, train F loss: -0.0742, acc 0.9946\n",
            "epoch 158: train D loss: 0.6380, train F loss: -0.0692, acc 0.9914\n",
            "epoch 159: train D loss: 0.6305, train F loss: -0.0774, acc 0.9932\n",
            "epoch 160: train D loss: 0.6191, train F loss: -0.0851, acc 0.9958\n",
            "epoch 161: train D loss: 0.6188, train F loss: -0.0821, acc 0.9952\n",
            "epoch 162: train D loss: 0.6159, train F loss: -0.0869, acc 0.9966\n",
            "epoch 163: train D loss: 0.6223, train F loss: -0.0833, acc 0.9952\n",
            "epoch 164: train D loss: 0.6287, train F loss: -0.0667, acc 0.9912\n",
            "epoch 165: train D loss: 0.6252, train F loss: -0.0855, acc 0.9948\n",
            "epoch 166: train D loss: 0.6197, train F loss: -0.0828, acc 0.9944\n",
            "epoch 167: train D loss: 0.6362, train F loss: -0.0806, acc 0.9922\n",
            "epoch 168: train D loss: 0.6312, train F loss: -0.0892, acc 0.9948\n",
            "epoch 169: train D loss: 0.6253, train F loss: -0.0935, acc 0.9964\n",
            "epoch 170: train D loss: 0.6309, train F loss: -0.0914, acc 0.9956\n",
            "epoch 171: train D loss: 0.6302, train F loss: -0.0882, acc 0.9946\n",
            "epoch 172: train D loss: 0.6260, train F loss: -0.0927, acc 0.9962\n",
            "epoch 173: train D loss: 0.6303, train F loss: -0.0975, acc 0.9972\n",
            "epoch 174: train D loss: 0.6315, train F loss: -0.0788, acc 0.9916\n",
            "epoch 175: train D loss: 0.6268, train F loss: -0.0804, acc 0.9926\n",
            "epoch 176: train D loss: 0.6234, train F loss: -0.0891, acc 0.9948\n",
            "epoch 177: train D loss: 0.6354, train F loss: -0.0951, acc 0.9946\n",
            "epoch 178: train D loss: 0.6274, train F loss: -0.0937, acc 0.9952\n",
            "epoch 179: train D loss: 0.6276, train F loss: -0.1008, acc 0.9972\n",
            "epoch 180: train D loss: 0.6312, train F loss: -0.0968, acc 0.9946\n",
            "epoch 181: train D loss: 0.6378, train F loss: -0.0821, acc 0.9942\n",
            "epoch 182: train D loss: 0.6392, train F loss: -0.0851, acc 0.9918\n",
            "epoch 183: train D loss: 0.6336, train F loss: -0.0950, acc 0.9940\n",
            "epoch 184: train D loss: 0.6322, train F loss: -0.0959, acc 0.9938\n",
            "epoch 185: train D loss: 0.6321, train F loss: -0.1026, acc 0.9962\n",
            "epoch 186: train D loss: 0.6318, train F loss: -0.1011, acc 0.9956\n",
            "epoch 187: train D loss: 0.6405, train F loss: -0.0855, acc 0.9922\n",
            "epoch 188: train D loss: 0.6344, train F loss: -0.0946, acc 0.9922\n",
            "epoch 189: train D loss: 0.6381, train F loss: -0.1015, acc 0.9946\n",
            "epoch 190: train D loss: 0.6341, train F loss: -0.1043, acc 0.9952\n",
            "epoch 191: train D loss: 0.6318, train F loss: -0.1040, acc 0.9958\n",
            "epoch 192: train D loss: 0.6369, train F loss: -0.1038, acc 0.9948\n",
            "epoch 193: train D loss: 0.6332, train F loss: -0.1034, acc 0.9958\n",
            "epoch 194: train D loss: 0.6407, train F loss: -0.1046, acc 0.9942\n",
            "epoch 195: train D loss: 0.6397, train F loss: -0.0999, acc 0.9928\n",
            "epoch 196: train D loss: 0.6343, train F loss: -0.1032, acc 0.9944\n",
            "epoch 197: train D loss: 0.6357, train F loss: -0.0983, acc 0.9916\n",
            "epoch 198: train D loss: 0.6474, train F loss: -0.1049, acc 0.9922\n",
            "epoch 199: train D loss: 0.6363, train F loss: -0.1052, acc 0.9942\n",
            "epoch 200: train D loss: 0.6367, train F loss: -0.1113, acc 0.9962\n",
            "epoch 201: train D loss: 0.6301, train F loss: -0.1133, acc 0.9968\n",
            "epoch 202: train D loss: 0.6408, train F loss: -0.1065, acc 0.9938\n",
            "epoch 203: train D loss: 0.6384, train F loss: -0.1062, acc 0.9938\n",
            "epoch 204: train D loss: 0.6362, train F loss: -0.1103, acc 0.9944\n",
            "epoch 205: train D loss: 0.6417, train F loss: -0.0871, acc 0.9916\n",
            "epoch 206: train D loss: 0.6400, train F loss: -0.1065, acc 0.9918\n",
            "epoch 207: train D loss: 0.6402, train F loss: -0.1046, acc 0.9936\n",
            "epoch 208: train D loss: 0.6305, train F loss: -0.1109, acc 0.9942\n",
            "epoch 209: train D loss: 0.6395, train F loss: -0.1130, acc 0.9946\n",
            "epoch 210: train D loss: 0.6398, train F loss: -0.1150, acc 0.9928\n",
            "epoch 211: train D loss: 0.6418, train F loss: -0.1055, acc 0.9924\n",
            "epoch 212: train D loss: 0.6300, train F loss: -0.1110, acc 0.9932\n",
            "epoch 213: train D loss: 0.6281, train F loss: -0.1129, acc 0.9936\n",
            "epoch 214: train D loss: 0.6408, train F loss: -0.1009, acc 0.9908\n",
            "epoch 215: train D loss: 0.6417, train F loss: -0.1149, acc 0.9938\n",
            "epoch 216: train D loss: 0.6333, train F loss: -0.1182, acc 0.9954\n",
            "epoch 217: train D loss: 0.6376, train F loss: -0.1095, acc 0.9924\n",
            "epoch 218: train D loss: 0.6384, train F loss: -0.1242, acc 0.9964\n",
            "epoch 219: train D loss: 0.6436, train F loss: -0.1083, acc 0.9924\n",
            "epoch 220: train D loss: 0.6415, train F loss: -0.1203, acc 0.9948\n",
            "epoch 221: train D loss: 0.6390, train F loss: -0.1261, acc 0.9962\n",
            "epoch 222: train D loss: 0.6419, train F loss: -0.1178, acc 0.9950\n",
            "epoch 223: train D loss: 0.6445, train F loss: -0.1183, acc 0.9934\n",
            "epoch 224: train D loss: 0.6442, train F loss: -0.1213, acc 0.9946\n",
            "epoch 225: train D loss: 0.6405, train F loss: -0.1279, acc 0.9962\n",
            "epoch 226: train D loss: 0.6330, train F loss: -0.1224, acc 0.9948\n",
            "epoch 227: train D loss: 0.6367, train F loss: -0.1195, acc 0.9934\n",
            "epoch 228: train D loss: 0.6421, train F loss: -0.1261, acc 0.9946\n",
            "epoch 229: train D loss: 0.6449, train F loss: -0.1228, acc 0.9946\n",
            "epoch 230: train D loss: 0.6462, train F loss: -0.1225, acc 0.9940\n",
            "epoch 231: train D loss: 0.6398, train F loss: -0.1291, acc 0.9958\n",
            "epoch 232: train D loss: 0.6377, train F loss: -0.1247, acc 0.9948\n",
            "epoch 233: train D loss: 0.6409, train F loss: -0.1275, acc 0.9948\n",
            "epoch 234: train D loss: 0.6445, train F loss: -0.1186, acc 0.9928\n",
            "epoch 235: train D loss: 0.6406, train F loss: -0.1313, acc 0.9956\n",
            "epoch 236: train D loss: 0.6449, train F loss: -0.1289, acc 0.9934\n",
            "epoch 237: train D loss: 0.6423, train F loss: -0.1282, acc 0.9940\n",
            "epoch 238: train D loss: 0.6450, train F loss: -0.1301, acc 0.9950\n",
            "epoch 239: train D loss: 0.6496, train F loss: -0.1316, acc 0.9954\n",
            "epoch 240: train D loss: 0.6594, train F loss: -0.1034, acc 0.9864\n",
            "epoch 241: train D loss: 0.6430, train F loss: -0.1331, acc 0.9940\n",
            "epoch 242: train D loss: 0.6353, train F loss: -0.1231, acc 0.9924\n",
            "epoch 243: train D loss: 0.6321, train F loss: -0.1316, acc 0.9932\n",
            "epoch 244: train D loss: 0.6355, train F loss: -0.1296, acc 0.9946\n",
            "epoch 245: train D loss: 0.6323, train F loss: -0.1395, acc 0.9966\n",
            "epoch 246: train D loss: 0.6493, train F loss: -0.1399, acc 0.9954\n",
            "epoch 247: train D loss: 0.6483, train F loss: -0.1415, acc 0.9956\n",
            "epoch 248: train D loss: 0.6424, train F loss: -0.1437, acc 0.9958\n",
            "epoch 249: train D loss: 0.6503, train F loss: -0.1401, acc 0.9952\n",
            "epoch 250: train D loss: 0.6390, train F loss: -0.1440, acc 0.9962\n",
            "epoch 251: train D loss: 0.6500, train F loss: -0.1388, acc 0.9936\n",
            "epoch 252: train D loss: 0.6417, train F loss: -0.1451, acc 0.9962\n",
            "epoch 253: train D loss: 0.6525, train F loss: -0.1196, acc 0.9908\n",
            "epoch 254: train D loss: 0.6469, train F loss: -0.1381, acc 0.9926\n",
            "epoch 255: train D loss: 0.6434, train F loss: -0.1422, acc 0.9946\n",
            "epoch 256: train D loss: 0.6474, train F loss: -0.1417, acc 0.9944\n",
            "epoch 257: train D loss: 0.6388, train F loss: -0.1485, acc 0.9966\n",
            "epoch 258: train D loss: 0.6459, train F loss: -0.1454, acc 0.9944\n",
            "epoch 259: train D loss: 0.6507, train F loss: -0.1469, acc 0.9942\n",
            "epoch 260: train D loss: 0.6482, train F loss: -0.1474, acc 0.9952\n",
            "epoch 261: train D loss: 0.6444, train F loss: -0.1294, acc 0.9920\n",
            "epoch 262: train D loss: 0.6515, train F loss: -0.1334, acc 0.9898\n",
            "epoch 263: train D loss: 0.6439, train F loss: -0.1452, acc 0.9934\n",
            "epoch 264: train D loss: 0.6473, train F loss: -0.1441, acc 0.9936\n",
            "epoch 265: train D loss: 0.6521, train F loss: -0.1461, acc 0.9936\n",
            "epoch 266: train D loss: 0.6472, train F loss: -0.1452, acc 0.9928\n",
            "epoch 267: train D loss: 0.6411, train F loss: -0.1503, acc 0.9954\n",
            "epoch 268: train D loss: 0.6452, train F loss: -0.1544, acc 0.9956\n",
            "epoch 269: train D loss: 0.6478, train F loss: -0.1463, acc 0.9936\n",
            "epoch 270: train D loss: 0.6475, train F loss: -0.1430, acc 0.9926\n",
            "epoch 271: train D loss: 0.6448, train F loss: -0.1519, acc 0.9950\n",
            "epoch 272: train D loss: 0.6421, train F loss: -0.1572, acc 0.9964\n",
            "epoch 273: train D loss: 0.6422, train F loss: -0.1512, acc 0.9948\n",
            "epoch 274: train D loss: 0.6437, train F loss: -0.1461, acc 0.9918\n",
            "epoch 275: train D loss: 0.6488, train F loss: -0.1498, acc 0.9928\n",
            "epoch 276: train D loss: 0.6434, train F loss: -0.1499, acc 0.9930\n",
            "epoch 277: train D loss: 0.6471, train F loss: -0.1587, acc 0.9946\n",
            "epoch 278: train D loss: 0.6428, train F loss: -0.1625, acc 0.9962\n",
            "epoch 279: train D loss: 0.6523, train F loss: -0.1653, acc 0.9958\n",
            "epoch 280: train D loss: 0.6484, train F loss: -0.1638, acc 0.9962\n",
            "epoch 281: train D loss: 0.6533, train F loss: -0.1163, acc 0.9854\n",
            "epoch 282: train D loss: 0.6510, train F loss: -0.1368, acc 0.9884\n",
            "epoch 283: train D loss: 0.6492, train F loss: -0.1615, acc 0.9936\n",
            "epoch 284: train D loss: 0.6439, train F loss: -0.1582, acc 0.9950\n",
            "epoch 285: train D loss: 0.6414, train F loss: -0.1533, acc 0.9934\n",
            "epoch 286: train D loss: 0.6462, train F loss: -0.1567, acc 0.9942\n",
            "epoch 287: train D loss: 0.6400, train F loss: -0.1643, acc 0.9960\n",
            "epoch 288: train D loss: 0.6441, train F loss: -0.1680, acc 0.9966\n",
            "epoch 289: train D loss: 0.6411, train F loss: -0.1659, acc 0.9950\n",
            "epoch 290: train D loss: 0.6490, train F loss: -0.1497, acc 0.9932\n",
            "epoch 291: train D loss: 0.6462, train F loss: -0.1597, acc 0.9932\n",
            "epoch 292: train D loss: 0.6456, train F loss: -0.1608, acc 0.9946\n",
            "epoch 293: train D loss: 0.6452, train F loss: -0.1690, acc 0.9956\n",
            "epoch 294: train D loss: 0.6516, train F loss: -0.1691, acc 0.9950\n",
            "epoch 295: train D loss: 0.6392, train F loss: -0.1533, acc 0.9904\n",
            "epoch 296: train D loss: 0.6411, train F loss: -0.1662, acc 0.9946\n",
            "epoch 297: train D loss: 0.6408, train F loss: -0.1628, acc 0.9936\n",
            "epoch 298: train D loss: 0.6522, train F loss: -0.1714, acc 0.9954\n",
            "epoch 299: train D loss: 0.6447, train F loss: -0.1658, acc 0.9946\n",
            "epoch 300: train D loss: 0.6460, train F loss: -0.1686, acc 0.9936\n",
            "epoch 301: train D loss: 0.6427, train F loss: -0.1649, acc 0.9954\n",
            "epoch 302: train D loss: 0.6549, train F loss: -0.1757, acc 0.9956\n",
            "epoch 303: train D loss: 0.6467, train F loss: -0.1742, acc 0.9954\n",
            "epoch 304: train D loss: 0.6367, train F loss: -0.1704, acc 0.9946\n",
            "epoch 305: train D loss: 0.6537, train F loss: -0.1481, acc 0.9908\n",
            "epoch 306: train D loss: 0.6483, train F loss: -0.1747, acc 0.9946\n",
            "epoch 307: train D loss: 0.6464, train F loss: -0.1634, acc 0.9934\n",
            "epoch 308: train D loss: 0.6475, train F loss: -0.1759, acc 0.9954\n",
            "epoch 309: train D loss: 0.6511, train F loss: -0.1783, acc 0.9958\n",
            "epoch 310: train D loss: 0.6535, train F loss: -0.1695, acc 0.9918\n",
            "epoch 311: train D loss: 0.6480, train F loss: -0.1756, acc 0.9940\n",
            "epoch 312: train D loss: 0.6489, train F loss: -0.1744, acc 0.9942\n",
            "epoch 313: train D loss: 0.6526, train F loss: -0.1810, acc 0.9938\n",
            "epoch 314: train D loss: 0.6473, train F loss: -0.1757, acc 0.9950\n",
            "epoch 315: train D loss: 0.6466, train F loss: -0.1806, acc 0.9950\n",
            "epoch 316: train D loss: 0.6510, train F loss: -0.1843, acc 0.9954\n",
            "epoch 317: train D loss: 0.6530, train F loss: -0.1894, acc 0.9968\n",
            "epoch 318: train D loss: 0.6549, train F loss: -0.1849, acc 0.9950\n",
            "epoch 319: train D loss: 0.6569, train F loss: -0.1837, acc 0.9930\n",
            "epoch 320: train D loss: 0.6550, train F loss: -0.1583, acc 0.9878\n",
            "epoch 321: train D loss: 0.6490, train F loss: -0.1729, acc 0.9920\n",
            "epoch 322: train D loss: 0.6586, train F loss: -0.1867, acc 0.9948\n",
            "epoch 323: train D loss: 0.6461, train F loss: -0.1816, acc 0.9934\n",
            "epoch 324: train D loss: 0.6489, train F loss: -0.1737, acc 0.9924\n",
            "epoch 325: train D loss: 0.6611, train F loss: -0.1795, acc 0.9934\n",
            "epoch 326: train D loss: 0.6530, train F loss: -0.1896, acc 0.9954\n",
            "epoch 327: train D loss: 0.6516, train F loss: -0.1857, acc 0.9948\n",
            "epoch 328: train D loss: 0.6571, train F loss: -0.1954, acc 0.9968\n",
            "epoch 329: train D loss: 0.6495, train F loss: -0.1866, acc 0.9944\n",
            "epoch 330: train D loss: 0.6636, train F loss: -0.1914, acc 0.9946\n",
            "epoch 331: train D loss: 0.6555, train F loss: -0.1870, acc 0.9942\n",
            "epoch 332: train D loss: 0.6618, train F loss: -0.1799, acc 0.9910\n",
            "epoch 333: train D loss: 0.6555, train F loss: -0.1941, acc 0.9952\n",
            "epoch 334: train D loss: 0.6563, train F loss: -0.1906, acc 0.9938\n",
            "epoch 335: train D loss: 0.6572, train F loss: -0.1916, acc 0.9950\n",
            "epoch 336: train D loss: 0.6548, train F loss: -0.1965, acc 0.9954\n",
            "epoch 337: train D loss: 0.6575, train F loss: -0.1967, acc 0.9946\n",
            "epoch 338: train D loss: 0.6591, train F loss: -0.1788, acc 0.9888\n",
            "epoch 339: train D loss: 0.6554, train F loss: -0.1967, acc 0.9952\n",
            "epoch 340: train D loss: 0.6561, train F loss: -0.1867, acc 0.9930\n",
            "epoch 341: train D loss: 0.6535, train F loss: -0.1938, acc 0.9946\n",
            "epoch 342: train D loss: 0.6529, train F loss: -0.2001, acc 0.9950\n",
            "epoch 343: train D loss: 0.6599, train F loss: -0.2072, acc 0.9968\n",
            "epoch 344: train D loss: 0.6622, train F loss: -0.2018, acc 0.9946\n",
            "epoch 345: train D loss: 0.6563, train F loss: -0.1851, acc 0.9904\n",
            "epoch 346: train D loss: 0.6576, train F loss: -0.1972, acc 0.9942\n",
            "epoch 347: train D loss: 0.6544, train F loss: -0.1926, acc 0.9924\n",
            "epoch 348: train D loss: 0.6507, train F loss: -0.1976, acc 0.9932\n",
            "epoch 349: train D loss: 0.6564, train F loss: -0.1975, acc 0.9932\n",
            "epoch 350: train D loss: 0.6529, train F loss: -0.1881, acc 0.9912\n",
            "epoch 351: train D loss: 0.6578, train F loss: -0.1912, acc 0.9936\n",
            "epoch 352: train D loss: 0.6562, train F loss: -0.2070, acc 0.9958\n",
            "epoch 353: train D loss: 0.6595, train F loss: -0.2074, acc 0.9948\n",
            "epoch 354: train D loss: 0.6570, train F loss: -0.1995, acc 0.9934\n",
            "epoch 355: train D loss: 0.6553, train F loss: -0.2005, acc 0.9940\n",
            "epoch 356: train D loss: 0.6540, train F loss: -0.2070, acc 0.9946\n",
            "epoch 357: train D loss: 0.6495, train F loss: -0.1880, acc 0.9918\n",
            "epoch 358: train D loss: 0.6629, train F loss: -0.1618, acc 0.9846\n",
            "epoch 359: train D loss: 0.6522, train F loss: -0.2043, acc 0.9944\n",
            "epoch 360: train D loss: 0.6467, train F loss: -0.2084, acc 0.9960\n",
            "epoch 361: train D loss: 0.6650, train F loss: -0.2096, acc 0.9942\n",
            "epoch 362: train D loss: 0.6544, train F loss: -0.2111, acc 0.9956\n",
            "epoch 363: train D loss: 0.6578, train F loss: -0.2109, acc 0.9952\n",
            "epoch 364: train D loss: 0.6562, train F loss: -0.2121, acc 0.9956\n",
            "epoch 365: train D loss: 0.6545, train F loss: -0.2134, acc 0.9960\n",
            "epoch 366: train D loss: 0.6630, train F loss: -0.2001, acc 0.9916\n",
            "epoch 367: train D loss: 0.6502, train F loss: -0.1978, acc 0.9932\n",
            "epoch 368: train D loss: 0.6549, train F loss: -0.2104, acc 0.9938\n",
            "epoch 369: train D loss: 0.6531, train F loss: -0.2092, acc 0.9932\n",
            "epoch 370: train D loss: 0.6548, train F loss: -0.2075, acc 0.9938\n",
            "epoch 371: train D loss: 0.6518, train F loss: -0.2106, acc 0.9942\n",
            "epoch 372: train D loss: 0.6541, train F loss: -0.2160, acc 0.9954\n",
            "epoch 373: train D loss: 0.6536, train F loss: -0.2041, acc 0.9924\n",
            "epoch 374: train D loss: 0.6575, train F loss: -0.2175, acc 0.9952\n",
            "epoch 375: train D loss: 0.6630, train F loss: -0.2210, acc 0.9948\n",
            "epoch 376: train D loss: 0.6526, train F loss: -0.2182, acc 0.9946\n",
            "epoch 377: train D loss: 0.6523, train F loss: -0.2213, acc 0.9966\n",
            "epoch 378: train D loss: 0.6548, train F loss: -0.2147, acc 0.9922\n",
            "epoch 379: train D loss: 0.6566, train F loss: -0.2178, acc 0.9942\n",
            "epoch 380: train D loss: 0.6618, train F loss: -0.2096, acc 0.9920\n",
            "epoch 381: train D loss: 0.6558, train F loss: -0.2155, acc 0.9932\n",
            "epoch 382: train D loss: 0.6582, train F loss: -0.2187, acc 0.9940\n",
            "epoch 383: train D loss: 0.6527, train F loss: -0.2167, acc 0.9946\n",
            "epoch 384: train D loss: 0.6542, train F loss: -0.2192, acc 0.9948\n",
            "epoch 385: train D loss: 0.6613, train F loss: -0.2209, acc 0.9938\n",
            "epoch 386: train D loss: 0.6537, train F loss: -0.2207, acc 0.9944\n",
            "epoch 387: train D loss: 0.6544, train F loss: -0.2281, acc 0.9960\n",
            "epoch 388: train D loss: 0.6585, train F loss: -0.2302, acc 0.9960\n",
            "epoch 389: train D loss: 0.6601, train F loss: -0.2221, acc 0.9928\n",
            "epoch 390: train D loss: 0.6599, train F loss: -0.2303, acc 0.9956\n",
            "epoch 391: train D loss: 0.6657, train F loss: -0.2226, acc 0.9934\n",
            "epoch 392: train D loss: 0.6630, train F loss: -0.2139, acc 0.9910\n",
            "epoch 393: train D loss: 0.6634, train F loss: -0.2211, acc 0.9932\n",
            "epoch 394: train D loss: 0.6588, train F loss: -0.2156, acc 0.9918\n",
            "epoch 395: train D loss: 0.6561, train F loss: -0.2262, acc 0.9942\n",
            "epoch 396: train D loss: 0.6602, train F loss: -0.2316, acc 0.9954\n",
            "epoch 397: train D loss: 0.6554, train F loss: -0.2282, acc 0.9942\n",
            "epoch 398: train D loss: 0.6576, train F loss: -0.2272, acc 0.9940\n",
            "epoch 399: train D loss: 0.6578, train F loss: -0.2326, acc 0.9956\n",
            "epoch 400: train D loss: 0.6619, train F loss: -0.2191, acc 0.9926\n",
            "epoch 401: train D loss: 0.6588, train F loss: -0.2249, acc 0.9928\n",
            "epoch 402: train D loss: 0.6585, train F loss: -0.1297, acc 0.9810\n",
            "epoch 403: train D loss: 0.6606, train F loss: -0.1872, acc 0.9802\n",
            "epoch 404: train D loss: 0.6500, train F loss: -0.2265, acc 0.9936\n",
            "epoch 405: train D loss: 0.6491, train F loss: -0.2260, acc 0.9946\n",
            "epoch 406: train D loss: 0.6585, train F loss: -0.2371, acc 0.9954\n",
            "epoch 407: train D loss: 0.6592, train F loss: -0.2273, acc 0.9940\n",
            "epoch 408: train D loss: 0.6583, train F loss: -0.2345, acc 0.9942\n",
            "epoch 409: train D loss: 0.6586, train F loss: -0.2404, acc 0.9954\n",
            "epoch 410: train D loss: 0.6585, train F loss: -0.2435, acc 0.9966\n",
            "epoch 411: train D loss: 0.6613, train F loss: -0.2363, acc 0.9934\n",
            "epoch 412: train D loss: 0.6531, train F loss: -0.2365, acc 0.9944\n",
            "epoch 413: train D loss: 0.6573, train F loss: -0.2387, acc 0.9946\n",
            "epoch 414: train D loss: 0.6529, train F loss: -0.2395, acc 0.9956\n",
            "epoch 415: train D loss: 0.6568, train F loss: -0.2385, acc 0.9950\n",
            "epoch 416: train D loss: 0.6557, train F loss: -0.2432, acc 0.9952\n",
            "epoch 417: train D loss: 0.6603, train F loss: -0.2437, acc 0.9958\n",
            "epoch 418: train D loss: 0.6602, train F loss: -0.2436, acc 0.9942\n",
            "epoch 419: train D loss: 0.6567, train F loss: -0.2434, acc 0.9954\n",
            "epoch 420: train D loss: 0.6587, train F loss: -0.2434, acc 0.9946\n",
            "epoch 421: train D loss: 0.6595, train F loss: -0.2424, acc 0.9948\n",
            "epoch 422: train D loss: 0.6609, train F loss: -0.2395, acc 0.9932\n",
            "epoch 423: train D loss: 0.6581, train F loss: -0.2398, acc 0.9946\n",
            "epoch 424: train D loss: 0.6594, train F loss: -0.2364, acc 0.9928\n",
            "epoch 425: train D loss: 0.6614, train F loss: -0.2481, acc 0.9956\n",
            "epoch 426: train D loss: 0.6645, train F loss: -0.2535, acc 0.9958\n",
            "epoch 427: train D loss: 0.6610, train F loss: -0.2515, acc 0.9958\n",
            "epoch 428: train D loss: 0.6671, train F loss: -0.2204, acc 0.9922\n",
            "epoch 429: train D loss: 0.6644, train F loss: -0.2291, acc 0.9892\n",
            "epoch 430: train D loss: 0.6630, train F loss: -0.2429, acc 0.9928\n",
            "epoch 431: train D loss: 0.6571, train F loss: -0.2492, acc 0.9954\n",
            "epoch 432: train D loss: 0.6619, train F loss: -0.2525, acc 0.9952\n",
            "epoch 433: train D loss: 0.6577, train F loss: -0.2471, acc 0.9944\n",
            "epoch 434: train D loss: 0.6609, train F loss: -0.2472, acc 0.9924\n",
            "epoch 435: train D loss: 0.6612, train F loss: -0.2501, acc 0.9938\n",
            "epoch 436: train D loss: 0.6626, train F loss: -0.2588, acc 0.9964\n",
            "epoch 437: train D loss: 0.6587, train F loss: -0.2500, acc 0.9944\n",
            "epoch 438: train D loss: 0.6590, train F loss: -0.2447, acc 0.9930\n",
            "epoch 439: train D loss: 0.6638, train F loss: -0.2596, acc 0.9960\n",
            "epoch 440: train D loss: 0.6673, train F loss: -0.2492, acc 0.9932\n",
            "epoch 441: train D loss: 0.6599, train F loss: -0.2590, acc 0.9958\n",
            "epoch 442: train D loss: 0.6546, train F loss: -0.2545, acc 0.9956\n",
            "epoch 443: train D loss: 0.6679, train F loss: -0.2421, acc 0.9918\n",
            "epoch 444: train D loss: 0.6614, train F loss: -0.2561, acc 0.9936\n",
            "epoch 445: train D loss: 0.6644, train F loss: -0.2515, acc 0.9938\n",
            "epoch 446: train D loss: 0.6651, train F loss: -0.2641, acc 0.9958\n",
            "epoch 447: train D loss: 0.6626, train F loss: -0.2637, acc 0.9964\n",
            "epoch 448: train D loss: 0.6617, train F loss: -0.2537, acc 0.9940\n",
            "epoch 449: train D loss: 0.6608, train F loss: -0.2602, acc 0.9946\n",
            "epoch 450: train D loss: 0.6666, train F loss: -0.2613, acc 0.9948\n",
            "epoch 451: train D loss: 0.6672, train F loss: -0.2620, acc 0.9928\n",
            "epoch 452: train D loss: 0.6704, train F loss: -0.2624, acc 0.9938\n",
            "epoch 453: train D loss: 0.6701, train F loss: -0.2591, acc 0.9942\n",
            "epoch 454: train D loss: 0.6723, train F loss: -0.2637, acc 0.9932\n",
            "epoch 455: train D loss: 0.6679, train F loss: -0.2448, acc 0.9890\n",
            "epoch 456: train D loss: 0.6710, train F loss: -0.2604, acc 0.9912\n",
            "epoch 457: train D loss: 0.6657, train F loss: -0.2670, acc 0.9956\n",
            "epoch 458: train D loss: 0.6693, train F loss: -0.2584, acc 0.9914\n",
            "epoch 459: train D loss: 0.6608, train F loss: -0.2643, acc 0.9948\n",
            "epoch 460: train D loss: 0.6629, train F loss: -0.2657, acc 0.9952\n",
            "epoch 461: train D loss: 0.6623, train F loss: -0.2595, acc 0.9924\n",
            "epoch 462: train D loss: 0.6618, train F loss: -0.2645, acc 0.9948\n",
            "epoch 463: train D loss: 0.6592, train F loss: -0.2587, acc 0.9934\n",
            "epoch 464: train D loss: 0.6624, train F loss: -0.2688, acc 0.9954\n",
            "epoch 465: train D loss: 0.6601, train F loss: -0.2718, acc 0.9954\n",
            "epoch 466: train D loss: 0.6660, train F loss: -0.2675, acc 0.9940\n",
            "epoch 467: train D loss: 0.6649, train F loss: -0.2742, acc 0.9952\n",
            "epoch 468: train D loss: 0.6656, train F loss: -0.2645, acc 0.9936\n",
            "epoch 469: train D loss: 0.6683, train F loss: -0.2684, acc 0.9930\n",
            "epoch 470: train D loss: 0.6645, train F loss: -0.2725, acc 0.9942\n",
            "epoch 471: train D loss: 0.6641, train F loss: -0.2712, acc 0.9942\n",
            "epoch 472: train D loss: 0.6637, train F loss: -0.2738, acc 0.9946\n",
            "epoch 473: train D loss: 0.6642, train F loss: -0.2763, acc 0.9950\n",
            "epoch 474: train D loss: 0.6690, train F loss: -0.2682, acc 0.9936\n",
            "epoch 475: train D loss: 0.6643, train F loss: -0.2662, acc 0.9920\n",
            "epoch 476: train D loss: 0.6631, train F loss: -0.2668, acc 0.9916\n",
            "epoch 477: train D loss: 0.6645, train F loss: -0.2791, acc 0.9954\n",
            "epoch 478: train D loss: 0.6735, train F loss: -0.2459, acc 0.9854\n",
            "epoch 479: train D loss: 0.6635, train F loss: -0.2505, acc 0.9900\n",
            "epoch 480: train D loss: 0.6586, train F loss: -0.2665, acc 0.9924\n",
            "epoch 481: train D loss: 0.6591, train F loss: -0.2706, acc 0.9936\n",
            "epoch 482: train D loss: 0.6604, train F loss: -0.2705, acc 0.9948\n",
            "epoch 483: train D loss: 0.6648, train F loss: -0.2805, acc 0.9946\n",
            "epoch 484: train D loss: 0.6621, train F loss: -0.2816, acc 0.9952\n",
            "epoch 485: train D loss: 0.6630, train F loss: -0.2817, acc 0.9952\n",
            "epoch 486: train D loss: 0.6620, train F loss: -0.2724, acc 0.9928\n",
            "epoch 487: train D loss: 0.6576, train F loss: -0.2812, acc 0.9954\n",
            "epoch 488: train D loss: 0.6655, train F loss: -0.2703, acc 0.9924\n",
            "epoch 489: train D loss: 0.6630, train F loss: -0.2811, acc 0.9944\n",
            "epoch 490: train D loss: 0.6649, train F loss: -0.2833, acc 0.9944\n",
            "epoch 491: train D loss: 0.6596, train F loss: -0.2797, acc 0.9942\n",
            "epoch 492: train D loss: 0.6671, train F loss: -0.2779, acc 0.9934\n",
            "epoch 493: train D loss: 0.6603, train F loss: -0.2807, acc 0.9944\n",
            "epoch 494: train D loss: 0.6701, train F loss: -0.2846, acc 0.9944\n",
            "epoch 495: train D loss: 0.6650, train F loss: -0.2855, acc 0.9950\n",
            "epoch 496: train D loss: 0.6655, train F loss: -0.2893, acc 0.9948\n",
            "epoch 497: train D loss: 0.6657, train F loss: -0.2918, acc 0.9960\n",
            "epoch 498: train D loss: 0.6655, train F loss: -0.2945, acc 0.9974\n",
            "epoch 499: train D loss: 0.6748, train F loss: -0.2879, acc 0.9936\n",
            "epoch 500: train D loss: 0.6658, train F loss: -0.2872, acc 0.9942\n",
            "epoch 501: train D loss: 0.6679, train F loss: -0.2838, acc 0.9924\n",
            "epoch 502: train D loss: 0.6667, train F loss: -0.2839, acc 0.9938\n",
            "epoch 503: train D loss: 0.6654, train F loss: -0.2890, acc 0.9936\n",
            "epoch 504: train D loss: 0.6699, train F loss: -0.2959, acc 0.9954\n",
            "epoch 505: train D loss: 0.6675, train F loss: -0.2893, acc 0.9938\n",
            "epoch 506: train D loss: 0.6724, train F loss: -0.2882, acc 0.9938\n",
            "epoch 507: train D loss: 0.6640, train F loss: -0.2743, acc 0.9916\n",
            "epoch 508: train D loss: 0.6635, train F loss: -0.2885, acc 0.9942\n",
            "epoch 509: train D loss: 0.6639, train F loss: -0.2962, acc 0.9958\n",
            "epoch 510: train D loss: 0.6642, train F loss: -0.2971, acc 0.9958\n",
            "epoch 511: train D loss: 0.6636, train F loss: -0.2949, acc 0.9958\n",
            "epoch 512: train D loss: 0.6724, train F loss: -0.3003, acc 0.9954\n",
            "epoch 513: train D loss: 0.6644, train F loss: -0.2916, acc 0.9938\n",
            "epoch 514: train D loss: 0.6732, train F loss: -0.2738, acc 0.9874\n",
            "epoch 515: train D loss: 0.6641, train F loss: -0.2861, acc 0.9918\n",
            "epoch 516: train D loss: 0.6626, train F loss: -0.2926, acc 0.9934\n",
            "epoch 517: train D loss: 0.6657, train F loss: -0.2966, acc 0.9936\n",
            "epoch 518: train D loss: 0.6668, train F loss: -0.2927, acc 0.9932\n",
            "epoch 519: train D loss: 0.6601, train F loss: -0.2983, acc 0.9950\n",
            "epoch 520: train D loss: 0.6669, train F loss: -0.2682, acc 0.9906\n",
            "epoch 521: train D loss: 0.6647, train F loss: -0.2926, acc 0.9940\n",
            "epoch 522: train D loss: 0.6673, train F loss: -0.2910, acc 0.9920\n",
            "epoch 523: train D loss: 0.6676, train F loss: -0.3061, acc 0.9956\n",
            "epoch 524: train D loss: 0.6659, train F loss: -0.3022, acc 0.9954\n",
            "epoch 525: train D loss: 0.6708, train F loss: -0.2919, acc 0.9928\n",
            "epoch 526: train D loss: 0.6677, train F loss: -0.3032, acc 0.9948\n",
            "epoch 527: train D loss: 0.6661, train F loss: -0.2937, acc 0.9930\n",
            "epoch 528: train D loss: 0.6635, train F loss: -0.2999, acc 0.9938\n",
            "epoch 529: train D loss: 0.6672, train F loss: -0.3023, acc 0.9938\n",
            "epoch 530: train D loss: 0.6652, train F loss: -0.3059, acc 0.9954\n",
            "epoch 531: train D loss: 0.6718, train F loss: -0.3069, acc 0.9942\n",
            "epoch 532: train D loss: 0.6641, train F loss: -0.3043, acc 0.9950\n",
            "epoch 533: train D loss: 0.6652, train F loss: -0.2732, acc 0.9900\n",
            "epoch 534: train D loss: 0.6607, train F loss: -0.3050, acc 0.9960\n",
            "epoch 535: train D loss: 0.6621, train F loss: -0.3063, acc 0.9964\n",
            "epoch 536: train D loss: 0.6679, train F loss: -0.3038, acc 0.9938\n",
            "epoch 537: train D loss: 0.6680, train F loss: -0.3109, acc 0.9954\n",
            "epoch 538: train D loss: 0.6679, train F loss: -0.3110, acc 0.9958\n",
            "epoch 539: train D loss: 0.6653, train F loss: -0.3073, acc 0.9950\n",
            "epoch 540: train D loss: 0.6710, train F loss: -0.3107, acc 0.9938\n",
            "epoch 541: train D loss: 0.6654, train F loss: -0.3068, acc 0.9934\n",
            "epoch 542: train D loss: 0.6686, train F loss: -0.3116, acc 0.9954\n",
            "epoch 543: train D loss: 0.6704, train F loss: -0.3126, acc 0.9944\n",
            "epoch 544: train D loss: 0.6684, train F loss: -0.3176, acc 0.9964\n",
            "epoch 545: train D loss: 0.6650, train F loss: -0.3064, acc 0.9936\n",
            "epoch 546: train D loss: 0.6695, train F loss: -0.3044, acc 0.9936\n",
            "epoch 547: train D loss: 0.6647, train F loss: -0.2894, acc 0.9886\n",
            "epoch 548: train D loss: 0.6608, train F loss: -0.3097, acc 0.9948\n",
            "epoch 549: train D loss: 0.6718, train F loss: -0.3157, acc 0.9948\n",
            "epoch 550: train D loss: 0.6665, train F loss: -0.3148, acc 0.9950\n",
            "epoch 551: train D loss: 0.6674, train F loss: -0.3070, acc 0.9920\n",
            "epoch 552: train D loss: 0.6701, train F loss: -0.3154, acc 0.9944\n",
            "epoch 553: train D loss: 0.6643, train F loss: -0.3152, acc 0.9960\n",
            "epoch 554: train D loss: 0.6648, train F loss: -0.3124, acc 0.9942\n",
            "epoch 555: train D loss: 0.6625, train F loss: -0.3096, acc 0.9934\n",
            "epoch 556: train D loss: 0.6678, train F loss: -0.3133, acc 0.9924\n",
            "epoch 557: train D loss: 0.6657, train F loss: -0.3170, acc 0.9944\n",
            "epoch 558: train D loss: 0.6660, train F loss: -0.3196, acc 0.9950\n",
            "epoch 559: train D loss: 0.6685, train F loss: -0.3147, acc 0.9944\n",
            "epoch 560: train D loss: 0.6696, train F loss: -0.2864, acc 0.9890\n",
            "epoch 561: train D loss: 0.6662, train F loss: -0.3059, acc 0.9932\n",
            "epoch 562: train D loss: 0.6678, train F loss: -0.3199, acc 0.9948\n",
            "epoch 563: train D loss: 0.6651, train F loss: -0.3216, acc 0.9946\n",
            "epoch 564: train D loss: 0.6640, train F loss: -0.3102, acc 0.9932\n",
            "epoch 565: train D loss: 0.6683, train F loss: -0.3238, acc 0.9940\n",
            "epoch 566: train D loss: 0.6707, train F loss: -0.3213, acc 0.9932\n",
            "epoch 567: train D loss: 0.6639, train F loss: -0.3200, acc 0.9940\n",
            "epoch 568: train D loss: 0.6689, train F loss: -0.3044, acc 0.9922\n",
            "epoch 569: train D loss: 0.6750, train F loss: -0.2983, acc 0.9882\n",
            "epoch 570: train D loss: 0.6664, train F loss: -0.3205, acc 0.9942\n",
            "epoch 571: train D loss: 0.6688, train F loss: -0.3193, acc 0.9932\n",
            "epoch 572: train D loss: 0.6700, train F loss: -0.2823, acc 0.9888\n",
            "epoch 573: train D loss: 0.6646, train F loss: -0.3219, acc 0.9948\n",
            "epoch 574: train D loss: 0.6653, train F loss: -0.3275, acc 0.9956\n",
            "epoch 575: train D loss: 0.6716, train F loss: -0.3327, acc 0.9948\n",
            "epoch 576: train D loss: 0.6625, train F loss: -0.3269, acc 0.9952\n",
            "epoch 577: train D loss: 0.6639, train F loss: -0.3261, acc 0.9954\n",
            "epoch 578: train D loss: 0.6624, train F loss: -0.3291, acc 0.9956\n",
            "epoch 579: train D loss: 0.6688, train F loss: -0.3276, acc 0.9950\n",
            "epoch 580: train D loss: 0.6704, train F loss: -0.3301, acc 0.9948\n",
            "epoch 581: train D loss: 0.6709, train F loss: -0.3275, acc 0.9944\n",
            "epoch 582: train D loss: 0.6673, train F loss: -0.3295, acc 0.9942\n",
            "epoch 583: train D loss: 0.6712, train F loss: -0.3330, acc 0.9948\n",
            "epoch 584: train D loss: 0.6737, train F loss: -0.3386, acc 0.9952\n",
            "epoch 585: train D loss: 0.6675, train F loss: -0.3380, acc 0.9966\n",
            "epoch 586: train D loss: 0.6682, train F loss: -0.3237, acc 0.9924\n",
            "epoch 587: train D loss: 0.6716, train F loss: -0.3072, acc 0.9894\n",
            "epoch 588: train D loss: 0.6679, train F loss: -0.3298, acc 0.9934\n",
            "epoch 589: train D loss: 0.6671, train F loss: -0.3359, acc 0.9954\n",
            "epoch 590: train D loss: 0.6664, train F loss: -0.3260, acc 0.9926\n",
            "epoch 591: train D loss: 0.6717, train F loss: -0.3362, acc 0.9938\n",
            "epoch 592: train D loss: 0.6682, train F loss: -0.3306, acc 0.9942\n",
            "epoch 593: train D loss: 0.6657, train F loss: -0.3353, acc 0.9954\n",
            "epoch 594: train D loss: 0.6704, train F loss: -0.3404, acc 0.9962\n",
            "epoch 595: train D loss: 0.6689, train F loss: -0.3430, acc 0.9960\n",
            "epoch 596: train D loss: 0.6704, train F loss: -0.3283, acc 0.9938\n",
            "epoch 597: train D loss: 0.6724, train F loss: -0.3313, acc 0.9924\n",
            "epoch 598: train D loss: 0.6765, train F loss: -0.3379, acc 0.9930\n",
            "epoch 599: train D loss: 0.6704, train F loss: -0.3403, acc 0.9944\n",
            "epoch 600: train D loss: 0.6720, train F loss: -0.3344, acc 0.9938\n",
            "epoch 601: train D loss: 0.6693, train F loss: -0.3380, acc 0.9936\n",
            "epoch 602: train D loss: 0.6717, train F loss: -0.3464, acc 0.9956\n",
            "epoch 603: train D loss: 0.6672, train F loss: -0.3469, acc 0.9964\n",
            "epoch 604: train D loss: 0.6676, train F loss: -0.3385, acc 0.9940\n",
            "epoch 605: train D loss: 0.6733, train F loss: -0.3467, acc 0.9950\n",
            "epoch 606: train D loss: 0.6716, train F loss: -0.3383, acc 0.9930\n",
            "epoch 607: train D loss: 0.6746, train F loss: -0.3352, acc 0.9914\n",
            "epoch 608: train D loss: 0.6717, train F loss: -0.3452, acc 0.9940\n",
            "epoch 609: train D loss: 0.6717, train F loss: -0.3400, acc 0.9940\n",
            "epoch 610: train D loss: 0.6712, train F loss: -0.3303, acc 0.9920\n",
            "epoch 611: train D loss: 0.6679, train F loss: -0.3441, acc 0.9942\n",
            "epoch 612: train D loss: 0.6723, train F loss: -0.3430, acc 0.9934\n",
            "epoch 613: train D loss: 0.6693, train F loss: -0.3460, acc 0.9950\n",
            "epoch 614: train D loss: 0.6720, train F loss: -0.3454, acc 0.9944\n",
            "epoch 615: train D loss: 0.6755, train F loss: -0.3497, acc 0.9938\n",
            "epoch 616: train D loss: 0.6755, train F loss: -0.3452, acc 0.9938\n",
            "epoch 617: train D loss: 0.6700, train F loss: -0.3477, acc 0.9936\n",
            "epoch 618: train D loss: 0.6698, train F loss: -0.3524, acc 0.9956\n",
            "epoch 619: train D loss: 0.6710, train F loss: -0.3482, acc 0.9952\n",
            "epoch 620: train D loss: 0.6728, train F loss: -0.3546, acc 0.9952\n",
            "epoch 621: train D loss: 0.6755, train F loss: -0.3542, acc 0.9948\n",
            "epoch 622: train D loss: 0.6714, train F loss: -0.3223, acc 0.9894\n",
            "epoch 623: train D loss: 0.6745, train F loss: -0.3448, acc 0.9920\n",
            "epoch 624: train D loss: 0.6733, train F loss: -0.3572, acc 0.9958\n",
            "epoch 625: train D loss: 0.6741, train F loss: -0.3527, acc 0.9948\n",
            "epoch 626: train D loss: 0.6710, train F loss: -0.3508, acc 0.9938\n",
            "epoch 627: train D loss: 0.6662, train F loss: -0.3538, acc 0.9960\n",
            "epoch 628: train D loss: 0.6705, train F loss: -0.3565, acc 0.9948\n",
            "epoch 629: train D loss: 0.6727, train F loss: -0.3527, acc 0.9944\n",
            "epoch 630: train D loss: 0.6758, train F loss: -0.3617, acc 0.9956\n",
            "epoch 631: train D loss: 0.6749, train F loss: -0.3016, acc 0.9822\n",
            "epoch 632: train D loss: 0.6723, train F loss: -0.3098, acc 0.9800\n",
            "epoch 633: train D loss: 0.6635, train F loss: -0.3458, acc 0.9930\n",
            "epoch 634: train D loss: 0.6699, train F loss: -0.3493, acc 0.9918\n",
            "epoch 635: train D loss: 0.6708, train F loss: -0.3563, acc 0.9944\n",
            "epoch 636: train D loss: 0.6717, train F loss: -0.3607, acc 0.9952\n",
            "epoch 637: train D loss: 0.6663, train F loss: -0.3582, acc 0.9952\n",
            "epoch 638: train D loss: 0.6717, train F loss: -0.3626, acc 0.9950\n",
            "epoch 639: train D loss: 0.6710, train F loss: -0.3601, acc 0.9952\n",
            "epoch 640: train D loss: 0.6678, train F loss: -0.3600, acc 0.9952\n",
            "epoch 641: train D loss: 0.6723, train F loss: -0.3635, acc 0.9942\n",
            "epoch 642: train D loss: 0.6727, train F loss: -0.3587, acc 0.9934\n",
            "epoch 643: train D loss: 0.6714, train F loss: -0.3638, acc 0.9956\n",
            "epoch 644: train D loss: 0.6697, train F loss: -0.3576, acc 0.9952\n",
            "epoch 645: train D loss: 0.6752, train F loss: -0.3649, acc 0.9950\n",
            "epoch 646: train D loss: 0.6686, train F loss: -0.3557, acc 0.9952\n",
            "epoch 647: train D loss: 0.6704, train F loss: -0.3629, acc 0.9928\n",
            "epoch 648: train D loss: 0.6727, train F loss: -0.3638, acc 0.9948\n",
            "epoch 649: train D loss: 0.6725, train F loss: -0.3644, acc 0.9940\n",
            "epoch 650: train D loss: 0.6729, train F loss: -0.3559, acc 0.9940\n",
            "epoch 651: train D loss: 0.6716, train F loss: -0.3581, acc 0.9938\n",
            "epoch 652: train D loss: 0.6631, train F loss: -0.3545, acc 0.9940\n",
            "epoch 653: train D loss: 0.6664, train F loss: -0.3558, acc 0.9920\n",
            "epoch 654: train D loss: 0.6688, train F loss: -0.3634, acc 0.9948\n",
            "epoch 655: train D loss: 0.6721, train F loss: -0.3667, acc 0.9948\n",
            "epoch 656: train D loss: 0.6705, train F loss: -0.3650, acc 0.9944\n",
            "epoch 657: train D loss: 0.6701, train F loss: -0.3627, acc 0.9926\n",
            "epoch 658: train D loss: 0.6666, train F loss: -0.3647, acc 0.9964\n",
            "epoch 659: train D loss: 0.6717, train F loss: -0.3714, acc 0.9960\n",
            "epoch 660: train D loss: 0.6710, train F loss: -0.3678, acc 0.9936\n",
            "epoch 661: train D loss: 0.6784, train F loss: -0.3287, acc 0.9862\n",
            "epoch 662: train D loss: 0.6685, train F loss: -0.3239, acc 0.9858\n",
            "epoch 663: train D loss: 0.6742, train F loss: -0.3473, acc 0.9872\n",
            "epoch 664: train D loss: 0.6724, train F loss: -0.3414, acc 0.9866\n",
            "epoch 665: train D loss: 0.6654, train F loss: -0.3625, acc 0.9928\n",
            "epoch 666: train D loss: 0.6627, train F loss: -0.3680, acc 0.9958\n",
            "epoch 667: train D loss: 0.6707, train F loss: -0.3709, acc 0.9954\n",
            "epoch 668: train D loss: 0.6663, train F loss: -0.3596, acc 0.9926\n",
            "epoch 669: train D loss: 0.6631, train F loss: -0.3678, acc 0.9948\n",
            "epoch 670: train D loss: 0.6652, train F loss: -0.3722, acc 0.9950\n",
            "epoch 671: train D loss: 0.6656, train F loss: -0.3743, acc 0.9956\n",
            "epoch 672: train D loss: 0.6672, train F loss: -0.3637, acc 0.9936\n",
            "epoch 673: train D loss: 0.6719, train F loss: -0.3790, acc 0.9956\n",
            "epoch 674: train D loss: 0.6695, train F loss: -0.3774, acc 0.9954\n",
            "epoch 675: train D loss: 0.6714, train F loss: -0.3779, acc 0.9944\n",
            "epoch 676: train D loss: 0.6715, train F loss: -0.3706, acc 0.9944\n",
            "epoch 677: train D loss: 0.6712, train F loss: -0.3772, acc 0.9944\n",
            "epoch 678: train D loss: 0.6691, train F loss: -0.3759, acc 0.9946\n",
            "epoch 679: train D loss: 0.6679, train F loss: -0.3787, acc 0.9958\n",
            "epoch 680: train D loss: 0.6753, train F loss: -0.3683, acc 0.9912\n",
            "epoch 681: train D loss: 0.6732, train F loss: -0.3731, acc 0.9922\n",
            "epoch 682: train D loss: 0.6664, train F loss: -0.3715, acc 0.9950\n",
            "epoch 683: train D loss: 0.6778, train F loss: -0.3728, acc 0.9918\n",
            "epoch 684: train D loss: 0.6699, train F loss: -0.3825, acc 0.9960\n",
            "epoch 685: train D loss: 0.6703, train F loss: -0.3814, acc 0.9952\n",
            "epoch 686: train D loss: 0.6734, train F loss: -0.3858, acc 0.9958\n",
            "epoch 687: train D loss: 0.6729, train F loss: -0.3797, acc 0.9948\n",
            "epoch 688: train D loss: 0.6739, train F loss: -0.3762, acc 0.9942\n",
            "epoch 689: train D loss: 0.6735, train F loss: -0.3796, acc 0.9938\n",
            "epoch 690: train D loss: 0.6637, train F loss: -0.3756, acc 0.9944\n",
            "epoch 691: train D loss: 0.6700, train F loss: -0.3798, acc 0.9946\n",
            "epoch 692: train D loss: 0.6750, train F loss: -0.3824, acc 0.9942\n",
            "epoch 693: train D loss: 0.6771, train F loss: -0.3761, acc 0.9930\n",
            "epoch 694: train D loss: 0.6768, train F loss: -0.3807, acc 0.9924\n",
            "epoch 695: train D loss: 0.6680, train F loss: -0.3562, acc 0.9898\n",
            "epoch 696: train D loss: 0.6643, train F loss: -0.3779, acc 0.9938\n",
            "epoch 697: train D loss: 0.6750, train F loss: -0.3874, acc 0.9944\n",
            "epoch 698: train D loss: 0.6759, train F loss: -0.3863, acc 0.9950\n",
            "epoch 699: train D loss: 0.6738, train F loss: -0.3589, acc 0.9902\n",
            "epoch 700: train D loss: 0.6681, train F loss: -0.3820, acc 0.9938\n",
            "epoch 701: train D loss: 0.6737, train F loss: -0.3813, acc 0.9942\n",
            "epoch 702: train D loss: 0.6773, train F loss: -0.3773, acc 0.9908\n",
            "epoch 703: train D loss: 0.6671, train F loss: -0.3791, acc 0.9928\n",
            "epoch 704: train D loss: 0.6652, train F loss: -0.3623, acc 0.9886\n",
            "epoch 705: train D loss: 0.6690, train F loss: -0.3906, acc 0.9958\n",
            "epoch 706: train D loss: 0.6702, train F loss: -0.3839, acc 0.9940\n",
            "epoch 707: train D loss: 0.6711, train F loss: -0.3830, acc 0.9934\n",
            "epoch 708: train D loss: 0.6723, train F loss: -0.3916, acc 0.9952\n",
            "epoch 709: train D loss: 0.6726, train F loss: -0.3946, acc 0.9960\n",
            "epoch 710: train D loss: 0.6712, train F loss: -0.3826, acc 0.9940\n",
            "epoch 711: train D loss: 0.6650, train F loss: -0.3765, acc 0.9928\n",
            "epoch 712: train D loss: 0.6675, train F loss: -0.3909, acc 0.9956\n",
            "epoch 713: train D loss: 0.6664, train F loss: -0.3860, acc 0.9938\n",
            "epoch 714: train D loss: 0.6678, train F loss: -0.3903, acc 0.9954\n",
            "epoch 715: train D loss: 0.6723, train F loss: -0.3862, acc 0.9938\n",
            "epoch 716: train D loss: 0.6737, train F loss: -0.3884, acc 0.9940\n",
            "epoch 717: train D loss: 0.6718, train F loss: -0.3927, acc 0.9938\n",
            "epoch 718: train D loss: 0.6740, train F loss: -0.3848, acc 0.9924\n",
            "epoch 719: train D loss: 0.6715, train F loss: -0.3920, acc 0.9932\n",
            "epoch 720: train D loss: 0.6719, train F loss: -0.3879, acc 0.9932\n",
            "epoch 721: train D loss: 0.6724, train F loss: -0.3965, acc 0.9948\n",
            "epoch 722: train D loss: 0.6768, train F loss: -0.4031, acc 0.9958\n",
            "epoch 723: train D loss: 0.6738, train F loss: -0.3934, acc 0.9932\n",
            "epoch 724: train D loss: 0.6726, train F loss: -0.3842, acc 0.9908\n",
            "epoch 725: train D loss: 0.6738, train F loss: -0.3153, acc 0.9790\n",
            "epoch 726: train D loss: 0.6663, train F loss: -0.3650, acc 0.9858\n",
            "epoch 727: train D loss: 0.6594, train F loss: -0.3848, acc 0.9936\n",
            "epoch 728: train D loss: 0.6682, train F loss: -0.3922, acc 0.9940\n",
            "epoch 729: train D loss: 0.6736, train F loss: -0.4011, acc 0.9952\n",
            "epoch 730: train D loss: 0.6782, train F loss: -0.3492, acc 0.9814\n",
            "epoch 731: train D loss: 0.6698, train F loss: -0.3899, acc 0.9934\n",
            "epoch 732: train D loss: 0.6673, train F loss: -0.3967, acc 0.9946\n",
            "epoch 733: train D loss: 0.6669, train F loss: -0.3949, acc 0.9942\n",
            "epoch 734: train D loss: 0.6724, train F loss: -0.4021, acc 0.9950\n",
            "epoch 735: train D loss: 0.6673, train F loss: -0.3911, acc 0.9924\n",
            "epoch 736: train D loss: 0.6657, train F loss: -0.3954, acc 0.9940\n",
            "epoch 737: train D loss: 0.6629, train F loss: -0.3995, acc 0.9960\n",
            "epoch 738: train D loss: 0.6668, train F loss: -0.4049, acc 0.9964\n",
            "epoch 739: train D loss: 0.6717, train F loss: -0.3982, acc 0.9934\n",
            "epoch 740: train D loss: 0.6737, train F loss: -0.4014, acc 0.9948\n",
            "epoch 741: train D loss: 0.6717, train F loss: -0.4080, acc 0.9960\n",
            "epoch 742: train D loss: 0.6723, train F loss: -0.4079, acc 0.9956\n",
            "epoch 743: train D loss: 0.6722, train F loss: -0.4039, acc 0.9942\n",
            "epoch 744: train D loss: 0.6756, train F loss: -0.4043, acc 0.9944\n",
            "epoch 745: train D loss: 0.6718, train F loss: -0.4017, acc 0.9940\n",
            "epoch 746: train D loss: 0.6708, train F loss: -0.4061, acc 0.9950\n",
            "epoch 747: train D loss: 0.6698, train F loss: -0.4045, acc 0.9954\n",
            "epoch 748: train D loss: 0.6675, train F loss: -0.3984, acc 0.9928\n",
            "epoch 749: train D loss: 0.6776, train F loss: -0.4025, acc 0.9920\n",
            "epoch 750: train D loss: 0.6750, train F loss: -0.4108, acc 0.9944\n",
            "epoch 751: train D loss: 0.6708, train F loss: -0.4133, acc 0.9962\n",
            "epoch 752: train D loss: 0.6754, train F loss: -0.4034, acc 0.9928\n",
            "epoch 753: train D loss: 0.6723, train F loss: -0.4133, acc 0.9960\n",
            "epoch 754: train D loss: 0.6753, train F loss: -0.4053, acc 0.9930\n",
            "epoch 755: train D loss: 0.6757, train F loss: -0.3935, acc 0.9932\n",
            "epoch 756: train D loss: 0.6744, train F loss: -0.3673, acc 0.9834\n",
            "epoch 757: train D loss: 0.6697, train F loss: -0.3942, acc 0.9910\n",
            "epoch 758: train D loss: 0.6672, train F loss: -0.4031, acc 0.9936\n",
            "epoch 759: train D loss: 0.6683, train F loss: -0.4086, acc 0.9948\n",
            "epoch 760: train D loss: 0.6675, train F loss: -0.4090, acc 0.9946\n",
            "epoch 761: train D loss: 0.6704, train F loss: -0.4097, acc 0.9946\n",
            "epoch 762: train D loss: 0.6731, train F loss: -0.4118, acc 0.9944\n",
            "epoch 763: train D loss: 0.6717, train F loss: -0.4130, acc 0.9944\n",
            "epoch 764: train D loss: 0.6743, train F loss: -0.4160, acc 0.9956\n",
            "epoch 765: train D loss: 0.6720, train F loss: -0.4165, acc 0.9950\n",
            "epoch 766: train D loss: 0.6754, train F loss: -0.4214, acc 0.9954\n",
            "epoch 767: train D loss: 0.6764, train F loss: -0.3863, acc 0.9890\n",
            "epoch 768: train D loss: 0.6785, train F loss: -0.4137, acc 0.9926\n",
            "epoch 769: train D loss: 0.6810, train F loss: -0.4185, acc 0.9932\n",
            "epoch 770: train D loss: 0.6739, train F loss: -0.4214, acc 0.9954\n",
            "epoch 771: train D loss: 0.6726, train F loss: -0.4094, acc 0.9938\n",
            "epoch 772: train D loss: 0.6741, train F loss: -0.4204, acc 0.9956\n",
            "epoch 773: train D loss: 0.6742, train F loss: -0.4151, acc 0.9942\n",
            "epoch 774: train D loss: 0.6752, train F loss: -0.4194, acc 0.9954\n",
            "epoch 775: train D loss: 0.6689, train F loss: -0.4153, acc 0.9948\n",
            "epoch 776: train D loss: 0.6725, train F loss: -0.4138, acc 0.9932\n",
            "epoch 777: train D loss: 0.6784, train F loss: -0.4160, acc 0.9924\n",
            "epoch 778: train D loss: 0.6763, train F loss: -0.4218, acc 0.9944\n",
            "epoch 779: train D loss: 0.6778, train F loss: -0.4218, acc 0.9942\n",
            "epoch 780: train D loss: 0.6758, train F loss: -0.4114, acc 0.9924\n",
            "epoch 781: train D loss: 0.6735, train F loss: -0.4179, acc 0.9938\n",
            "epoch 782: train D loss: 0.6749, train F loss: -0.4129, acc 0.9928\n",
            "epoch 783: train D loss: 0.6771, train F loss: -0.4199, acc 0.9934\n",
            "epoch 784: train D loss: 0.6737, train F loss: -0.4166, acc 0.9930\n",
            "epoch 785: train D loss: 0.6748, train F loss: -0.4218, acc 0.9950\n",
            "epoch 786: train D loss: 0.6756, train F loss: -0.4215, acc 0.9942\n",
            "epoch 787: train D loss: 0.6722, train F loss: -0.4154, acc 0.9936\n",
            "epoch 788: train D loss: 0.6734, train F loss: -0.3985, acc 0.9898\n",
            "epoch 789: train D loss: 0.6715, train F loss: -0.4179, acc 0.9938\n",
            "epoch 790: train D loss: 0.6737, train F loss: -0.4149, acc 0.9916\n",
            "epoch 791: train D loss: 0.6753, train F loss: -0.4231, acc 0.9936\n",
            "epoch 792: train D loss: 0.6760, train F loss: -0.4240, acc 0.9944\n",
            "epoch 793: train D loss: 0.6775, train F loss: -0.4284, acc 0.9954\n",
            "epoch 794: train D loss: 0.6733, train F loss: -0.4256, acc 0.9944\n",
            "epoch 795: train D loss: 0.6739, train F loss: -0.4249, acc 0.9950\n",
            "epoch 796: train D loss: 0.6717, train F loss: -0.4241, acc 0.9942\n",
            "epoch 797: train D loss: 0.6761, train F loss: -0.4313, acc 0.9962\n",
            "epoch 798: train D loss: 0.6767, train F loss: -0.4285, acc 0.9944\n",
            "epoch 799: train D loss: 0.6775, train F loss: -0.4271, acc 0.9940\n",
            "epoch 800: train D loss: 0.6781, train F loss: -0.4117, acc 0.9914\n",
            "epoch 801: train D loss: 0.6751, train F loss: -0.4345, acc 0.9960\n",
            "epoch 802: train D loss: 0.6726, train F loss: -0.4235, acc 0.9938\n",
            "epoch 803: train D loss: 0.6743, train F loss: -0.4285, acc 0.9960\n",
            "epoch 804: train D loss: 0.6716, train F loss: -0.4259, acc 0.9942\n",
            "epoch 805: train D loss: 0.6709, train F loss: -0.4229, acc 0.9932\n",
            "epoch 806: train D loss: 0.6732, train F loss: -0.4297, acc 0.9946\n",
            "epoch 807: train D loss: 0.6721, train F loss: -0.4222, acc 0.9928\n",
            "epoch 808: train D loss: 0.6742, train F loss: -0.4150, acc 0.9906\n",
            "epoch 809: train D loss: 0.6741, train F loss: -0.4260, acc 0.9932\n",
            "epoch 810: train D loss: 0.6710, train F loss: -0.4284, acc 0.9938\n",
            "epoch 811: train D loss: 0.6750, train F loss: -0.4289, acc 0.9930\n",
            "epoch 812: train D loss: 0.6680, train F loss: -0.4277, acc 0.9946\n",
            "epoch 813: train D loss: 0.6820, train F loss: -0.3736, acc 0.9782\n",
            "epoch 814: train D loss: 0.6761, train F loss: -0.4193, acc 0.9906\n",
            "epoch 815: train D loss: 0.6741, train F loss: -0.4372, acc 0.9960\n",
            "epoch 816: train D loss: 0.6693, train F loss: -0.4291, acc 0.9944\n",
            "epoch 817: train D loss: 0.6749, train F loss: -0.4327, acc 0.9948\n",
            "epoch 818: train D loss: 0.6747, train F loss: -0.4335, acc 0.9942\n",
            "epoch 819: train D loss: 0.6750, train F loss: -0.4372, acc 0.9954\n",
            "epoch 820: train D loss: 0.6718, train F loss: -0.4326, acc 0.9942\n",
            "epoch 821: train D loss: 0.6737, train F loss: -0.4273, acc 0.9936\n",
            "epoch 822: train D loss: 0.6781, train F loss: -0.4369, acc 0.9928\n",
            "epoch 823: train D loss: 0.6747, train F loss: -0.4279, acc 0.9934\n",
            "epoch 824: train D loss: 0.6738, train F loss: -0.4389, acc 0.9942\n",
            "epoch 825: train D loss: 0.6740, train F loss: -0.4328, acc 0.9938\n",
            "epoch 826: train D loss: 0.6719, train F loss: -0.4385, acc 0.9948\n",
            "epoch 827: train D loss: 0.6756, train F loss: -0.4435, acc 0.9960\n",
            "epoch 828: train D loss: 0.6711, train F loss: -0.4345, acc 0.9948\n",
            "epoch 829: train D loss: 0.6797, train F loss: -0.4414, acc 0.9940\n",
            "epoch 830: train D loss: 0.6721, train F loss: -0.4359, acc 0.9946\n",
            "epoch 831: train D loss: 0.6757, train F loss: -0.4406, acc 0.9948\n",
            "epoch 832: train D loss: 0.6777, train F loss: -0.4419, acc 0.9950\n",
            "epoch 833: train D loss: 0.6716, train F loss: -0.4384, acc 0.9944\n",
            "epoch 834: train D loss: 0.6748, train F loss: -0.4328, acc 0.9926\n",
            "epoch 835: train D loss: 0.6774, train F loss: -0.4380, acc 0.9928\n",
            "epoch 836: train D loss: 0.6750, train F loss: -0.4414, acc 0.9944\n",
            "epoch 837: train D loss: 0.6727, train F loss: -0.4370, acc 0.9928\n",
            "epoch 838: train D loss: 0.6752, train F loss: -0.4307, acc 0.9914\n",
            "epoch 839: train D loss: 0.6738, train F loss: -0.4358, acc 0.9942\n",
            "epoch 840: train D loss: 0.6753, train F loss: -0.4483, acc 0.9962\n",
            "epoch 841: train D loss: 0.6737, train F loss: -0.4448, acc 0.9940\n",
            "epoch 842: train D loss: 0.6780, train F loss: -0.4407, acc 0.9928\n",
            "epoch 843: train D loss: 0.6742, train F loss: -0.4436, acc 0.9948\n",
            "epoch 844: train D loss: 0.6750, train F loss: -0.4463, acc 0.9940\n",
            "epoch 845: train D loss: 0.6788, train F loss: -0.3907, acc 0.9808\n",
            "epoch 846: train D loss: 0.6712, train F loss: -0.4209, acc 0.9892\n",
            "epoch 847: train D loss: 0.6713, train F loss: -0.4292, acc 0.9924\n",
            "epoch 848: train D loss: 0.6743, train F loss: -0.4450, acc 0.9940\n",
            "epoch 849: train D loss: 0.6772, train F loss: -0.4416, acc 0.9932\n",
            "epoch 850: train D loss: 0.6750, train F loss: -0.4479, acc 0.9946\n",
            "epoch 851: train D loss: 0.6742, train F loss: -0.4426, acc 0.9934\n",
            "epoch 852: train D loss: 0.6707, train F loss: -0.4442, acc 0.9948\n",
            "epoch 853: train D loss: 0.6740, train F loss: -0.4319, acc 0.9918\n",
            "epoch 854: train D loss: 0.6768, train F loss: -0.4464, acc 0.9944\n",
            "epoch 855: train D loss: 0.6743, train F loss: -0.4542, acc 0.9962\n",
            "epoch 856: train D loss: 0.6698, train F loss: -0.4441, acc 0.9950\n",
            "epoch 857: train D loss: 0.6732, train F loss: -0.4486, acc 0.9954\n",
            "epoch 858: train D loss: 0.6717, train F loss: -0.4499, acc 0.9954\n",
            "epoch 859: train D loss: 0.6698, train F loss: -0.4465, acc 0.9944\n",
            "epoch 860: train D loss: 0.6758, train F loss: -0.4180, acc 0.9872\n",
            "epoch 861: train D loss: 0.6722, train F loss: -0.4458, acc 0.9930\n",
            "epoch 862: train D loss: 0.6738, train F loss: -0.4435, acc 0.9930\n",
            "epoch 863: train D loss: 0.6773, train F loss: -0.4425, acc 0.9920\n",
            "epoch 864: train D loss: 0.6715, train F loss: -0.4533, acc 0.9950\n",
            "epoch 865: train D loss: 0.6704, train F loss: -0.4467, acc 0.9948\n",
            "epoch 866: train D loss: 0.6755, train F loss: -0.4447, acc 0.9928\n",
            "epoch 867: train D loss: 0.6769, train F loss: -0.4483, acc 0.9936\n",
            "epoch 868: train D loss: 0.6698, train F loss: -0.4522, acc 0.9950\n",
            "epoch 869: train D loss: 0.6733, train F loss: -0.4526, acc 0.9952\n",
            "epoch 870: train D loss: 0.6715, train F loss: -0.4531, acc 0.9948\n",
            "epoch 871: train D loss: 0.6745, train F loss: -0.4531, acc 0.9942\n",
            "epoch 872: train D loss: 0.6732, train F loss: -0.4576, acc 0.9954\n",
            "epoch 873: train D loss: 0.6756, train F loss: -0.4525, acc 0.9936\n",
            "epoch 874: train D loss: 0.6769, train F loss: -0.4550, acc 0.9946\n",
            "epoch 875: train D loss: 0.6702, train F loss: -0.4494, acc 0.9944\n",
            "epoch 876: train D loss: 0.6777, train F loss: -0.4573, acc 0.9950\n",
            "epoch 877: train D loss: 0.6747, train F loss: -0.4518, acc 0.9938\n",
            "epoch 878: train D loss: 0.6808, train F loss: -0.4292, acc 0.9862\n",
            "epoch 879: train D loss: 0.6730, train F loss: -0.4401, acc 0.9904\n",
            "epoch 880: train D loss: 0.6708, train F loss: -0.4541, acc 0.9946\n",
            "epoch 881: train D loss: 0.6710, train F loss: -0.4566, acc 0.9950\n",
            "epoch 882: train D loss: 0.6745, train F loss: -0.4609, acc 0.9960\n",
            "epoch 883: train D loss: 0.6777, train F loss: -0.4574, acc 0.9946\n",
            "epoch 884: train D loss: 0.6751, train F loss: -0.4630, acc 0.9952\n",
            "epoch 885: train D loss: 0.6791, train F loss: -0.4546, acc 0.9930\n",
            "epoch 886: train D loss: 0.6766, train F loss: -0.4469, acc 0.9928\n",
            "epoch 887: train D loss: 0.6729, train F loss: -0.4537, acc 0.9940\n",
            "epoch 888: train D loss: 0.6770, train F loss: -0.4475, acc 0.9928\n",
            "epoch 889: train D loss: 0.6755, train F loss: -0.4597, acc 0.9948\n",
            "epoch 890: train D loss: 0.6756, train F loss: -0.4633, acc 0.9958\n",
            "epoch 891: train D loss: 0.6723, train F loss: -0.4593, acc 0.9942\n",
            "epoch 892: train D loss: 0.6747, train F loss: -0.4592, acc 0.9934\n",
            "epoch 893: train D loss: 0.6764, train F loss: -0.4659, acc 0.9954\n",
            "epoch 894: train D loss: 0.6766, train F loss: -0.4691, acc 0.9964\n",
            "epoch 895: train D loss: 0.6769, train F loss: -0.4556, acc 0.9920\n",
            "epoch 896: train D loss: 0.6756, train F loss: -0.4403, acc 0.9900\n",
            "epoch 897: train D loss: 0.6753, train F loss: -0.4577, acc 0.9924\n",
            "epoch 898: train D loss: 0.6729, train F loss: -0.4642, acc 0.9946\n",
            "epoch 899: train D loss: 0.6774, train F loss: -0.4685, acc 0.9956\n",
            "epoch 900: train D loss: 0.6770, train F loss: -0.4564, acc 0.9932\n",
            "epoch 901: train D loss: 0.6771, train F loss: -0.4645, acc 0.9938\n",
            "epoch 902: train D loss: 0.6732, train F loss: -0.4647, acc 0.9940\n",
            "epoch 903: train D loss: 0.6732, train F loss: -0.4576, acc 0.9922\n",
            "epoch 904: train D loss: 0.6719, train F loss: -0.4652, acc 0.9944\n",
            "epoch 905: train D loss: 0.6772, train F loss: -0.4726, acc 0.9966\n",
            "epoch 906: train D loss: 0.6775, train F loss: -0.4699, acc 0.9946\n",
            "epoch 907: train D loss: 0.6789, train F loss: -0.4563, acc 0.9914\n",
            "epoch 908: train D loss: 0.6780, train F loss: -0.4515, acc 0.9902\n",
            "epoch 909: train D loss: 0.6725, train F loss: -0.4638, acc 0.9946\n",
            "epoch 910: train D loss: 0.6745, train F loss: -0.4608, acc 0.9938\n",
            "epoch 911: train D loss: 0.6753, train F loss: -0.4663, acc 0.9950\n",
            "epoch 912: train D loss: 0.6755, train F loss: -0.4619, acc 0.9940\n",
            "epoch 913: train D loss: 0.6771, train F loss: -0.4726, acc 0.9962\n",
            "epoch 914: train D loss: 0.6779, train F loss: -0.4749, acc 0.9952\n",
            "epoch 915: train D loss: 0.6735, train F loss: -0.4644, acc 0.9936\n",
            "epoch 916: train D loss: 0.6746, train F loss: -0.4547, acc 0.9922\n",
            "epoch 917: train D loss: 0.6770, train F loss: -0.4675, acc 0.9934\n",
            "epoch 918: train D loss: 0.6788, train F loss: -0.4755, acc 0.9954\n",
            "epoch 919: train D loss: 0.6780, train F loss: -0.4721, acc 0.9950\n",
            "epoch 920: train D loss: 0.6769, train F loss: -0.4708, acc 0.9944\n",
            "epoch 921: train D loss: 0.6776, train F loss: -0.4764, acc 0.9956\n",
            "epoch 922: train D loss: 0.6785, train F loss: -0.4799, acc 0.9964\n",
            "epoch 923: train D loss: 0.6747, train F loss: -0.4681, acc 0.9954\n",
            "epoch 924: train D loss: 0.6762, train F loss: -0.4697, acc 0.9942\n",
            "epoch 925: train D loss: 0.6784, train F loss: -0.4709, acc 0.9936\n",
            "epoch 926: train D loss: 0.6785, train F loss: -0.4756, acc 0.9950\n",
            "epoch 927: train D loss: 0.6749, train F loss: -0.4776, acc 0.9960\n",
            "epoch 928: train D loss: 0.6762, train F loss: -0.4698, acc 0.9930\n",
            "epoch 929: train D loss: 0.6777, train F loss: -0.4750, acc 0.9946\n",
            "epoch 930: train D loss: 0.6820, train F loss: -0.4742, acc 0.9932\n",
            "epoch 931: train D loss: 0.6775, train F loss: -0.4757, acc 0.9952\n",
            "epoch 932: train D loss: 0.6780, train F loss: -0.4686, acc 0.9936\n",
            "epoch 933: train D loss: 0.6754, train F loss: -0.4725, acc 0.9940\n",
            "epoch 934: train D loss: 0.6785, train F loss: -0.4639, acc 0.9928\n",
            "epoch 935: train D loss: 0.6780, train F loss: -0.4764, acc 0.9938\n",
            "epoch 936: train D loss: 0.6769, train F loss: -0.4735, acc 0.9940\n",
            "epoch 937: train D loss: 0.6748, train F loss: -0.4743, acc 0.9946\n",
            "epoch 938: train D loss: 0.6794, train F loss: -0.4825, acc 0.9954\n",
            "epoch 939: train D loss: 0.6785, train F loss: -0.4755, acc 0.9942\n",
            "epoch 940: train D loss: 0.6752, train F loss: -0.4754, acc 0.9948\n",
            "epoch 941: train D loss: 0.6807, train F loss: -0.4734, acc 0.9912\n",
            "epoch 942: train D loss: 0.6717, train F loss: -0.4745, acc 0.9940\n",
            "epoch 943: train D loss: 0.6833, train F loss: -0.4865, acc 0.9950\n",
            "epoch 944: train D loss: 0.6795, train F loss: -0.4796, acc 0.9934\n",
            "epoch 945: train D loss: 0.6773, train F loss: -0.4743, acc 0.9928\n",
            "epoch 946: train D loss: 0.6812, train F loss: -0.4709, acc 0.9926\n",
            "epoch 947: train D loss: 0.6792, train F loss: -0.4842, acc 0.9950\n",
            "epoch 948: train D loss: 0.6793, train F loss: -0.4770, acc 0.9950\n",
            "epoch 949: train D loss: 0.6797, train F loss: -0.4719, acc 0.9920\n",
            "epoch 950: train D loss: 0.6804, train F loss: -0.4836, acc 0.9944\n",
            "epoch 951: train D loss: 0.6777, train F loss: -0.4819, acc 0.9940\n",
            "epoch 952: train D loss: 0.6797, train F loss: -0.4853, acc 0.9948\n",
            "epoch 953: train D loss: 0.6737, train F loss: -0.4738, acc 0.9942\n",
            "epoch 954: train D loss: 0.6799, train F loss: -0.4787, acc 0.9922\n",
            "epoch 955: train D loss: 0.6760, train F loss: -0.4756, acc 0.9920\n",
            "epoch 956: train D loss: 0.6780, train F loss: -0.4893, acc 0.9962\n",
            "epoch 957: train D loss: 0.6796, train F loss: -0.4868, acc 0.9952\n",
            "epoch 958: train D loss: 0.6774, train F loss: -0.4803, acc 0.9932\n",
            "epoch 959: train D loss: 0.6815, train F loss: -0.4853, acc 0.9936\n",
            "epoch 960: train D loss: 0.6804, train F loss: -0.4526, acc 0.9856\n",
            "epoch 961: train D loss: 0.6773, train F loss: -0.4797, acc 0.9920\n",
            "epoch 962: train D loss: 0.6758, train F loss: -0.4778, acc 0.9940\n",
            "epoch 963: train D loss: 0.6765, train F loss: -0.4766, acc 0.9922\n",
            "epoch 964: train D loss: 0.6684, train F loss: -0.4755, acc 0.9930\n",
            "epoch 965: train D loss: 0.6763, train F loss: -0.4891, acc 0.9950\n",
            "epoch 966: train D loss: 0.6766, train F loss: -0.4804, acc 0.9946\n",
            "epoch 967: train D loss: 0.6765, train F loss: -0.4842, acc 0.9944\n",
            "epoch 968: train D loss: 0.6780, train F loss: -0.4725, acc 0.9898\n",
            "epoch 969: train D loss: 0.6726, train F loss: -0.4858, acc 0.9952\n",
            "epoch 970: train D loss: 0.6754, train F loss: -0.4823, acc 0.9930\n",
            "epoch 971: train D loss: 0.6760, train F loss: -0.4822, acc 0.9936\n",
            "epoch 972: train D loss: 0.6745, train F loss: -0.4861, acc 0.9948\n",
            "epoch 973: train D loss: 0.6774, train F loss: -0.4689, acc 0.9910\n",
            "epoch 974: train D loss: 0.6754, train F loss: -0.4863, acc 0.9948\n",
            "epoch 975: train D loss: 0.6765, train F loss: -0.4880, acc 0.9948\n",
            "epoch 976: train D loss: 0.6748, train F loss: -0.4894, acc 0.9948\n",
            "epoch 977: train D loss: 0.6774, train F loss: -0.4877, acc 0.9938\n",
            "epoch 978: train D loss: 0.6795, train F loss: -0.4928, acc 0.9948\n",
            "epoch 979: train D loss: 0.6741, train F loss: -0.4838, acc 0.9938\n",
            "epoch 980: train D loss: 0.6766, train F loss: -0.4731, acc 0.9912\n",
            "epoch 981: train D loss: 0.6755, train F loss: -0.4835, acc 0.9930\n",
            "epoch 982: train D loss: 0.6768, train F loss: -0.4862, acc 0.9934\n",
            "epoch 983: train D loss: 0.6806, train F loss: -0.4724, acc 0.9892\n",
            "epoch 984: train D loss: 0.6765, train F loss: -0.4916, acc 0.9946\n",
            "epoch 985: train D loss: 0.6737, train F loss: -0.4894, acc 0.9942\n",
            "epoch 986: train D loss: 0.6715, train F loss: -0.4858, acc 0.9948\n",
            "epoch 987: train D loss: 0.6774, train F loss: -0.4907, acc 0.9946\n",
            "epoch 988: train D loss: 0.6807, train F loss: -0.4506, acc 0.9862\n",
            "epoch 989: train D loss: 0.6749, train F loss: -0.4787, acc 0.9916\n",
            "epoch 990: train D loss: 0.6749, train F loss: -0.4814, acc 0.9920\n",
            "epoch 991: train D loss: 0.6766, train F loss: -0.4931, acc 0.9960\n",
            "epoch 992: train D loss: 0.6771, train F loss: -0.4951, acc 0.9944\n",
            "epoch 993: train D loss: 0.6784, train F loss: -0.4900, acc 0.9944\n",
            "epoch 994: train D loss: 0.6810, train F loss: -0.5004, acc 0.9948\n",
            "epoch 995: train D loss: 0.6756, train F loss: -0.4900, acc 0.9940\n",
            "epoch 996: train D loss: 0.6735, train F loss: -0.4738, acc 0.9902\n",
            "epoch 997: train D loss: 0.6750, train F loss: -0.4841, acc 0.9930\n",
            "epoch 998: train D loss: 0.6764, train F loss: -0.4966, acc 0.9958\n",
            "epoch 999: train D loss: 0.6726, train F loss: -0.4952, acc 0.9958\n",
            "epoch 1000: train D loss: 0.6736, train F loss: -0.4892, acc 0.9942\n",
            "epoch 1001: train D loss: 0.6766, train F loss: -0.4988, acc 0.9952\n",
            "epoch 1002: train D loss: 0.6756, train F loss: -0.4986, acc 0.9950\n",
            "epoch 1003: train D loss: 0.6756, train F loss: -0.4945, acc 0.9940\n",
            "epoch 1004: train D loss: 0.6755, train F loss: -0.4972, acc 0.9942\n",
            "epoch 1005: train D loss: 0.6821, train F loss: -0.5010, acc 0.9948\n",
            "epoch 1006: train D loss: 0.6770, train F loss: -0.4949, acc 0.9938\n",
            "epoch 1007: train D loss: 0.6799, train F loss: -0.4849, acc 0.9938\n",
            "epoch 1008: train D loss: 0.6756, train F loss: -0.4897, acc 0.9904\n",
            "epoch 1009: train D loss: 0.6769, train F loss: -0.4939, acc 0.9938\n",
            "epoch 1010: train D loss: 0.6773, train F loss: -0.4954, acc 0.9936\n",
            "epoch 1011: train D loss: 0.6780, train F loss: -0.4936, acc 0.9938\n",
            "epoch 1012: train D loss: 0.6761, train F loss: -0.4963, acc 0.9946\n",
            "epoch 1013: train D loss: 0.6790, train F loss: -0.5039, acc 0.9952\n",
            "epoch 1014: train D loss: 0.6731, train F loss: -0.4970, acc 0.9946\n",
            "epoch 1015: train D loss: 0.6791, train F loss: -0.5021, acc 0.9952\n",
            "epoch 1016: train D loss: 0.6798, train F loss: -0.5020, acc 0.9938\n",
            "epoch 1017: train D loss: 0.6767, train F loss: -0.4973, acc 0.9938\n",
            "epoch 1018: train D loss: 0.6725, train F loss: -0.4962, acc 0.9936\n",
            "epoch 1019: train D loss: 0.6738, train F loss: -0.4951, acc 0.9928\n",
            "epoch 1020: train D loss: 0.6745, train F loss: -0.4960, acc 0.9940\n",
            "epoch 1021: train D loss: 0.6765, train F loss: -0.5003, acc 0.9956\n",
            "epoch 1022: train D loss: 0.6779, train F loss: -0.5023, acc 0.9944\n",
            "epoch 1023: train D loss: 0.6750, train F loss: -0.5013, acc 0.9942\n",
            "epoch 1024: train D loss: 0.6757, train F loss: -0.5012, acc 0.9948\n",
            "epoch 1025: train D loss: 0.6821, train F loss: -0.5038, acc 0.9944\n",
            "epoch 1026: train D loss: 0.6830, train F loss: -0.5083, acc 0.9952\n",
            "epoch 1027: train D loss: 0.6781, train F loss: -0.4969, acc 0.9918\n",
            "epoch 1028: train D loss: 0.6806, train F loss: -0.5088, acc 0.9952\n",
            "epoch 1029: train D loss: 0.6772, train F loss: -0.4931, acc 0.9918\n",
            "epoch 1030: train D loss: 0.6784, train F loss: -0.4881, acc 0.9910\n",
            "epoch 1031: train D loss: 0.6764, train F loss: -0.5048, acc 0.9952\n",
            "epoch 1032: train D loss: 0.6807, train F loss: -0.5096, acc 0.9960\n",
            "epoch 1033: train D loss: 0.6762, train F loss: -0.4785, acc 0.9878\n",
            "epoch 1034: train D loss: 0.6779, train F loss: -0.5006, acc 0.9920\n",
            "epoch 1035: train D loss: 0.6756, train F loss: -0.5011, acc 0.9936\n",
            "epoch 1036: train D loss: 0.6793, train F loss: -0.4791, acc 0.9890\n",
            "epoch 1037: train D loss: 0.6783, train F loss: -0.5051, acc 0.9946\n",
            "epoch 1038: train D loss: 0.6787, train F loss: -0.5129, acc 0.9958\n",
            "epoch 1039: train D loss: 0.6764, train F loss: -0.4264, acc 0.9866\n",
            "epoch 1040: train D loss: 0.6811, train F loss: -0.4361, acc 0.9718\n",
            "epoch 1041: train D loss: 0.6720, train F loss: -0.4925, acc 0.9922\n",
            "epoch 1042: train D loss: 0.6751, train F loss: -0.5014, acc 0.9938\n",
            "epoch 1043: train D loss: 0.6728, train F loss: -0.5052, acc 0.9946\n",
            "epoch 1044: train D loss: 0.6744, train F loss: -0.5062, acc 0.9942\n",
            "epoch 1045: train D loss: 0.6743, train F loss: -0.5016, acc 0.9940\n",
            "epoch 1046: train D loss: 0.6748, train F loss: -0.5125, acc 0.9964\n",
            "epoch 1047: train D loss: 0.6789, train F loss: -0.5101, acc 0.9954\n",
            "epoch 1048: train D loss: 0.6692, train F loss: -0.5053, acc 0.9948\n",
            "epoch 1049: train D loss: 0.6753, train F loss: -0.5119, acc 0.9958\n",
            "epoch 1050: train D loss: 0.6770, train F loss: -0.5120, acc 0.9956\n",
            "epoch 1051: train D loss: 0.6754, train F loss: -0.5050, acc 0.9936\n",
            "epoch 1052: train D loss: 0.6764, train F loss: -0.5047, acc 0.9936\n",
            "epoch 1053: train D loss: 0.6759, train F loss: -0.5104, acc 0.9946\n",
            "epoch 1054: train D loss: 0.6797, train F loss: -0.5160, acc 0.9954\n",
            "epoch 1055: train D loss: 0.6780, train F loss: -0.5139, acc 0.9950\n",
            "epoch 1056: train D loss: 0.6821, train F loss: -0.5101, acc 0.9932\n",
            "epoch 1057: train D loss: 0.6772, train F loss: -0.5130, acc 0.9940\n",
            "epoch 1058: train D loss: 0.6759, train F loss: -0.5118, acc 0.9950\n",
            "epoch 1059: train D loss: 0.6727, train F loss: -0.5084, acc 0.9948\n",
            "epoch 1060: train D loss: 0.6801, train F loss: -0.5149, acc 0.9944\n",
            "epoch 1061: train D loss: 0.6718, train F loss: -0.5094, acc 0.9952\n",
            "epoch 1062: train D loss: 0.6762, train F loss: -0.5126, acc 0.9948\n",
            "epoch 1063: train D loss: 0.6790, train F loss: -0.5180, acc 0.9958\n",
            "epoch 1064: train D loss: 0.6814, train F loss: -0.5208, acc 0.9956\n",
            "epoch 1065: train D loss: 0.6806, train F loss: -0.5172, acc 0.9956\n",
            "epoch 1066: train D loss: 0.6811, train F loss: -0.5159, acc 0.9946\n",
            "epoch 1067: train D loss: 0.6817, train F loss: -0.5193, acc 0.9946\n",
            "epoch 1068: train D loss: 0.6799, train F loss: -0.5076, acc 0.9936\n",
            "epoch 1069: train D loss: 0.6819, train F loss: -0.5207, acc 0.9946\n",
            "epoch 1070: train D loss: 0.6813, train F loss: -0.5141, acc 0.9940\n",
            "epoch 1071: train D loss: 0.6782, train F loss: -0.5026, acc 0.9914\n",
            "epoch 1072: train D loss: 0.6797, train F loss: -0.5096, acc 0.9932\n",
            "epoch 1073: train D loss: 0.6792, train F loss: -0.5171, acc 0.9954\n",
            "epoch 1074: train D loss: 0.6804, train F loss: -0.5193, acc 0.9942\n",
            "epoch 1075: train D loss: 0.6766, train F loss: -0.5146, acc 0.9950\n",
            "epoch 1076: train D loss: 0.6795, train F loss: -0.5083, acc 0.9922\n",
            "epoch 1077: train D loss: 0.6782, train F loss: -0.5154, acc 0.9936\n",
            "epoch 1078: train D loss: 0.6750, train F loss: -0.5137, acc 0.9942\n",
            "epoch 1079: train D loss: 0.6751, train F loss: -0.5144, acc 0.9946\n",
            "epoch 1080: train D loss: 0.6809, train F loss: -0.5181, acc 0.9936\n",
            "epoch 1081: train D loss: 0.6762, train F loss: -0.5157, acc 0.9942\n",
            "epoch 1082: train D loss: 0.6806, train F loss: -0.5162, acc 0.9944\n",
            "epoch 1083: train D loss: 0.6784, train F loss: -0.5196, acc 0.9948\n",
            "epoch 1084: train D loss: 0.6783, train F loss: -0.5184, acc 0.9948\n",
            "epoch 1085: train D loss: 0.6804, train F loss: -0.5155, acc 0.9920\n",
            "epoch 1086: train D loss: 0.6769, train F loss: -0.5149, acc 0.9934\n",
            "epoch 1087: train D loss: 0.6781, train F loss: -0.5217, acc 0.9948\n",
            "epoch 1088: train D loss: 0.6773, train F loss: -0.5222, acc 0.9950\n",
            "epoch 1089: train D loss: 0.6806, train F loss: -0.5224, acc 0.9958\n",
            "epoch 1090: train D loss: 0.6788, train F loss: -0.5151, acc 0.9930\n",
            "epoch 1091: train D loss: 0.6810, train F loss: -0.5136, acc 0.9922\n",
            "epoch 1092: train D loss: 0.6738, train F loss: -0.5142, acc 0.9930\n",
            "epoch 1093: train D loss: 0.6764, train F loss: -0.5183, acc 0.9938\n",
            "epoch 1094: train D loss: 0.6835, train F loss: -0.5204, acc 0.9928\n",
            "epoch 1095: train D loss: 0.6774, train F loss: -0.5218, acc 0.9940\n",
            "epoch 1096: train D loss: 0.6785, train F loss: -0.5192, acc 0.9944\n",
            "epoch 1097: train D loss: 0.6800, train F loss: -0.5258, acc 0.9952\n",
            "epoch 1098: train D loss: 0.6795, train F loss: -0.5217, acc 0.9944\n",
            "epoch 1099: train D loss: 0.6797, train F loss: -0.5245, acc 0.9946\n",
            "epoch 1100: train D loss: 0.6799, train F loss: -0.5242, acc 0.9938\n",
            "epoch 1101: train D loss: 0.6827, train F loss: -0.5232, acc 0.9940\n",
            "epoch 1102: train D loss: 0.6793, train F loss: -0.5204, acc 0.9932\n",
            "epoch 1103: train D loss: 0.6774, train F loss: -0.5082, acc 0.9898\n",
            "epoch 1104: train D loss: 0.6810, train F loss: -0.5131, acc 0.9910\n",
            "epoch 1105: train D loss: 0.6783, train F loss: -0.5225, acc 0.9934\n",
            "epoch 1106: train D loss: 0.6778, train F loss: -0.5263, acc 0.9956\n",
            "epoch 1107: train D loss: 0.6793, train F loss: -0.5309, acc 0.9962\n",
            "epoch 1108: train D loss: 0.6776, train F loss: -0.5230, acc 0.9948\n",
            "epoch 1109: train D loss: 0.6823, train F loss: -0.5318, acc 0.9948\n",
            "epoch 1110: train D loss: 0.6755, train F loss: -0.5229, acc 0.9952\n",
            "epoch 1111: train D loss: 0.6807, train F loss: -0.5283, acc 0.9952\n",
            "epoch 1112: train D loss: 0.6792, train F loss: -0.5241, acc 0.9940\n",
            "epoch 1113: train D loss: 0.6762, train F loss: -0.5195, acc 0.9928\n",
            "epoch 1114: train D loss: 0.6762, train F loss: -0.5246, acc 0.9948\n",
            "epoch 1115: train D loss: 0.6798, train F loss: -0.5248, acc 0.9940\n",
            "epoch 1116: train D loss: 0.6788, train F loss: -0.5269, acc 0.9948\n",
            "epoch 1117: train D loss: 0.6821, train F loss: -0.5367, acc 0.9966\n",
            "epoch 1118: train D loss: 0.6784, train F loss: -0.5252, acc 0.9948\n",
            "epoch 1119: train D loss: 0.6794, train F loss: -0.5295, acc 0.9964\n",
            "epoch 1120: train D loss: 0.6801, train F loss: -0.5298, acc 0.9940\n",
            "epoch 1121: train D loss: 0.6786, train F loss: -0.5175, acc 0.9912\n",
            "epoch 1122: train D loss: 0.6825, train F loss: -0.5280, acc 0.9958\n",
            "epoch 1123: train D loss: 0.6734, train F loss: -0.5221, acc 0.9946\n",
            "epoch 1124: train D loss: 0.6808, train F loss: -0.5335, acc 0.9954\n",
            "epoch 1125: train D loss: 0.6845, train F loss: -0.5237, acc 0.9924\n",
            "epoch 1126: train D loss: 0.6785, train F loss: -0.5245, acc 0.9930\n",
            "epoch 1127: train D loss: 0.6774, train F loss: -0.5239, acc 0.9920\n",
            "epoch 1128: train D loss: 0.6802, train F loss: -0.5339, acc 0.9948\n",
            "epoch 1129: train D loss: 0.6760, train F loss: -0.5280, acc 0.9940\n",
            "epoch 1130: train D loss: 0.6812, train F loss: -0.5337, acc 0.9946\n",
            "epoch 1131: train D loss: 0.6792, train F loss: -0.5294, acc 0.9948\n",
            "epoch 1132: train D loss: 0.6802, train F loss: -0.5363, acc 0.9956\n",
            "epoch 1133: train D loss: 0.6778, train F loss: -0.5250, acc 0.9938\n",
            "epoch 1134: train D loss: 0.6810, train F loss: -0.5304, acc 0.9936\n",
            "epoch 1135: train D loss: 0.6776, train F loss: -0.5320, acc 0.9950\n",
            "epoch 1136: train D loss: 0.6791, train F loss: -0.5289, acc 0.9926\n",
            "epoch 1137: train D loss: 0.6794, train F loss: -0.5362, acc 0.9954\n",
            "epoch 1138: train D loss: 0.6785, train F loss: -0.5373, acc 0.9966\n",
            "epoch 1139: train D loss: 0.6777, train F loss: -0.5317, acc 0.9944\n",
            "epoch 1140: train D loss: 0.6825, train F loss: -0.5301, acc 0.9926\n",
            "epoch 1141: train D loss: 0.6782, train F loss: -0.5267, acc 0.9930\n",
            "epoch 1142: train D loss: 0.6787, train F loss: -0.5338, acc 0.9944\n",
            "epoch 1143: train D loss: 0.6779, train F loss: -0.5361, acc 0.9944\n",
            "epoch 1144: train D loss: 0.6792, train F loss: -0.5258, acc 0.9946\n",
            "epoch 1145: train D loss: 0.6753, train F loss: -0.5292, acc 0.9928\n",
            "epoch 1146: train D loss: 0.6772, train F loss: -0.5363, acc 0.9960\n",
            "epoch 1147: train D loss: 0.6786, train F loss: -0.5112, acc 0.9920\n",
            "epoch 1148: train D loss: 0.6730, train F loss: -0.5226, acc 0.9934\n",
            "epoch 1149: train D loss: 0.6749, train F loss: -0.5208, acc 0.9912\n",
            "epoch 1150: train D loss: 0.6790, train F loss: -0.5321, acc 0.9938\n",
            "epoch 1151: train D loss: 0.6748, train F loss: -0.5384, acc 0.9966\n",
            "epoch 1152: train D loss: 0.6764, train F loss: -0.5300, acc 0.9930\n",
            "epoch 1153: train D loss: 0.6803, train F loss: -0.5360, acc 0.9944\n",
            "epoch 1154: train D loss: 0.6805, train F loss: -0.5361, acc 0.9942\n",
            "epoch 1155: train D loss: 0.6796, train F loss: -0.5389, acc 0.9950\n",
            "epoch 1156: train D loss: 0.6767, train F loss: -0.5352, acc 0.9950\n",
            "epoch 1157: train D loss: 0.6779, train F loss: -0.5384, acc 0.9944\n",
            "epoch 1158: train D loss: 0.6756, train F loss: -0.5331, acc 0.9944\n",
            "epoch 1159: train D loss: 0.6750, train F loss: -0.5282, acc 0.9946\n",
            "epoch 1160: train D loss: 0.6830, train F loss: -0.5388, acc 0.9940\n",
            "epoch 1161: train D loss: 0.6805, train F loss: -0.5351, acc 0.9942\n",
            "epoch 1162: train D loss: 0.6754, train F loss: -0.5364, acc 0.9948\n",
            "epoch 1163: train D loss: 0.6790, train F loss: -0.5250, acc 0.9916\n",
            "epoch 1164: train D loss: 0.6847, train F loss: -0.5467, acc 0.9956\n",
            "epoch 1165: train D loss: 0.6742, train F loss: -0.5308, acc 0.9944\n",
            "epoch 1166: train D loss: 0.6830, train F loss: -0.5440, acc 0.9956\n",
            "epoch 1167: train D loss: 0.6785, train F loss: -0.5408, acc 0.9944\n",
            "epoch 1168: train D loss: 0.6770, train F loss: -0.5370, acc 0.9946\n",
            "epoch 1169: train D loss: 0.6789, train F loss: -0.5310, acc 0.9932\n",
            "epoch 1170: train D loss: 0.6811, train F loss: -0.5388, acc 0.9946\n",
            "epoch 1171: train D loss: 0.6821, train F loss: -0.5374, acc 0.9930\n",
            "epoch 1172: train D loss: 0.6788, train F loss: -0.5457, acc 0.9960\n",
            "epoch 1173: train D loss: 0.6808, train F loss: -0.5415, acc 0.9942\n",
            "epoch 1174: train D loss: 0.6851, train F loss: -0.5345, acc 0.9922\n",
            "epoch 1175: train D loss: 0.6784, train F loss: -0.5380, acc 0.9926\n",
            "epoch 1176: train D loss: 0.6780, train F loss: -0.5397, acc 0.9932\n",
            "epoch 1177: train D loss: 0.6778, train F loss: -0.5463, acc 0.9958\n",
            "epoch 1178: train D loss: 0.6837, train F loss: -0.5276, acc 0.9900\n",
            "epoch 1179: train D loss: 0.6757, train F loss: -0.5335, acc 0.9940\n",
            "epoch 1180: train D loss: 0.6819, train F loss: -0.5450, acc 0.9948\n",
            "epoch 1181: train D loss: 0.6758, train F loss: -0.5392, acc 0.9942\n",
            "epoch 1182: train D loss: 0.6767, train F loss: -0.5401, acc 0.9944\n",
            "epoch 1183: train D loss: 0.6820, train F loss: -0.5407, acc 0.9938\n",
            "epoch 1184: train D loss: 0.6838, train F loss: -0.5038, acc 0.9884\n",
            "epoch 1185: train D loss: 0.6787, train F loss: -0.5393, acc 0.9938\n",
            "epoch 1186: train D loss: 0.6774, train F loss: -0.5340, acc 0.9928\n",
            "epoch 1187: train D loss: 0.6804, train F loss: -0.5443, acc 0.9948\n",
            "epoch 1188: train D loss: 0.6797, train F loss: -0.5440, acc 0.9942\n",
            "epoch 1189: train D loss: 0.6803, train F loss: -0.5484, acc 0.9958\n",
            "epoch 1190: train D loss: 0.6801, train F loss: -0.5482, acc 0.9958\n",
            "epoch 1191: train D loss: 0.6802, train F loss: -0.5504, acc 0.9956\n",
            "epoch 1192: train D loss: 0.6805, train F loss: -0.5390, acc 0.9926\n",
            "epoch 1193: train D loss: 0.6811, train F loss: -0.5489, acc 0.9956\n",
            "epoch 1194: train D loss: 0.6787, train F loss: -0.5509, acc 0.9956\n",
            "epoch 1195: train D loss: 0.6794, train F loss: -0.5389, acc 0.9924\n",
            "epoch 1196: train D loss: 0.6773, train F loss: -0.5472, acc 0.9954\n",
            "epoch 1197: train D loss: 0.6810, train F loss: -0.5272, acc 0.9898\n",
            "epoch 1198: train D loss: 0.6790, train F loss: -0.5192, acc 0.9892\n",
            "epoch 1199: train D loss: 0.6794, train F loss: -0.5392, acc 0.9916\n",
            "epoch 1200: train D loss: 0.6732, train F loss: -0.5392, acc 0.9948\n",
            "epoch 1201: train D loss: 0.6793, train F loss: -0.5462, acc 0.9948\n",
            "epoch 1202: train D loss: 0.6795, train F loss: -0.5399, acc 0.9930\n",
            "epoch 1203: train D loss: 0.6782, train F loss: -0.5458, acc 0.9946\n",
            "epoch 1204: train D loss: 0.6764, train F loss: -0.5517, acc 0.9962\n",
            "epoch 1205: train D loss: 0.6780, train F loss: -0.5503, acc 0.9962\n",
            "epoch 1206: train D loss: 0.6788, train F loss: -0.5330, acc 0.9916\n",
            "epoch 1207: train D loss: 0.6793, train F loss: -0.5462, acc 0.9934\n",
            "epoch 1208: train D loss: 0.6772, train F loss: -0.5435, acc 0.9940\n",
            "epoch 1209: train D loss: 0.6797, train F loss: -0.5485, acc 0.9946\n",
            "epoch 1210: train D loss: 0.6849, train F loss: -0.5476, acc 0.9938\n",
            "epoch 1211: train D loss: 0.6799, train F loss: -0.5423, acc 0.9932\n",
            "epoch 1212: train D loss: 0.6804, train F loss: -0.5542, acc 0.9952\n",
            "epoch 1213: train D loss: 0.6760, train F loss: -0.5542, acc 0.9970\n",
            "epoch 1214: train D loss: 0.6792, train F loss: -0.5496, acc 0.9942\n",
            "epoch 1215: train D loss: 0.6786, train F loss: -0.5433, acc 0.9922\n",
            "epoch 1216: train D loss: 0.6797, train F loss: -0.5419, acc 0.9944\n",
            "epoch 1217: train D loss: 0.6828, train F loss: -0.5473, acc 0.9924\n",
            "epoch 1218: train D loss: 0.6769, train F loss: -0.5517, acc 0.9954\n",
            "epoch 1219: train D loss: 0.6817, train F loss: -0.5491, acc 0.9934\n",
            "epoch 1220: train D loss: 0.6800, train F loss: -0.5354, acc 0.9918\n",
            "epoch 1221: train D loss: 0.6789, train F loss: -0.5516, acc 0.9948\n",
            "epoch 1222: train D loss: 0.6776, train F loss: -0.5162, acc 0.9900\n",
            "epoch 1223: train D loss: 0.6794, train F loss: -0.5452, acc 0.9926\n",
            "epoch 1224: train D loss: 0.6787, train F loss: -0.5534, acc 0.9958\n",
            "epoch 1225: train D loss: 0.6834, train F loss: -0.4435, acc 0.9780\n",
            "epoch 1226: train D loss: 0.6769, train F loss: -0.5419, acc 0.9928\n",
            "epoch 1227: train D loss: 0.6758, train F loss: -0.5473, acc 0.9944\n",
            "epoch 1228: train D loss: 0.6751, train F loss: -0.5499, acc 0.9962\n",
            "epoch 1229: train D loss: 0.6794, train F loss: -0.5517, acc 0.9932\n",
            "epoch 1230: train D loss: 0.6814, train F loss: -0.5555, acc 0.9950\n",
            "epoch 1231: train D loss: 0.6775, train F loss: -0.5587, acc 0.9970\n",
            "epoch 1232: train D loss: 0.6758, train F loss: -0.5536, acc 0.9956\n",
            "epoch 1233: train D loss: 0.6825, train F loss: -0.5586, acc 0.9958\n",
            "epoch 1234: train D loss: 0.6802, train F loss: -0.5477, acc 0.9932\n",
            "epoch 1235: train D loss: 0.6800, train F loss: -0.5572, acc 0.9962\n",
            "epoch 1236: train D loss: 0.6756, train F loss: -0.5563, acc 0.9960\n",
            "epoch 1237: train D loss: 0.6790, train F loss: -0.5562, acc 0.9950\n",
            "epoch 1238: train D loss: 0.6767, train F loss: -0.5318, acc 0.9912\n",
            "epoch 1239: train D loss: 0.6780, train F loss: -0.5555, acc 0.9948\n",
            "epoch 1240: train D loss: 0.6845, train F loss: -0.5598, acc 0.9948\n",
            "epoch 1241: train D loss: 0.6785, train F loss: -0.5533, acc 0.9938\n",
            "epoch 1242: train D loss: 0.6797, train F loss: -0.5550, acc 0.9946\n",
            "epoch 1243: train D loss: 0.6760, train F loss: -0.5537, acc 0.9950\n",
            "epoch 1244: train D loss: 0.6789, train F loss: -0.5583, acc 0.9952\n",
            "epoch 1245: train D loss: 0.6795, train F loss: -0.5601, acc 0.9964\n",
            "epoch 1246: train D loss: 0.6808, train F loss: -0.5591, acc 0.9952\n",
            "epoch 1247: train D loss: 0.6803, train F loss: -0.5606, acc 0.9954\n",
            "epoch 1248: train D loss: 0.6817, train F loss: -0.5541, acc 0.9942\n",
            "epoch 1249: train D loss: 0.6811, train F loss: -0.5560, acc 0.9944\n",
            "epoch 1250: train D loss: 0.6779, train F loss: -0.5515, acc 0.9944\n",
            "epoch 1251: train D loss: 0.6798, train F loss: -0.5582, acc 0.9944\n",
            "epoch 1252: train D loss: 0.6801, train F loss: -0.5576, acc 0.9942\n",
            "epoch 1253: train D loss: 0.6818, train F loss: -0.5569, acc 0.9944\n",
            "epoch 1254: train D loss: 0.6807, train F loss: -0.5588, acc 0.9946\n",
            "epoch 1255: train D loss: 0.6825, train F loss: -0.5517, acc 0.9920\n",
            "epoch 1256: train D loss: 0.6770, train F loss: -0.5557, acc 0.9942\n",
            "epoch 1257: train D loss: 0.6857, train F loss: -0.5558, acc 0.9928\n",
            "epoch 1258: train D loss: 0.6798, train F loss: -0.5625, acc 0.9954\n",
            "epoch 1259: train D loss: 0.6832, train F loss: -0.5196, acc 0.9874\n",
            "epoch 1260: train D loss: 0.6804, train F loss: -0.5561, acc 0.9932\n",
            "epoch 1261: train D loss: 0.6743, train F loss: -0.5511, acc 0.9932\n",
            "epoch 1262: train D loss: 0.6757, train F loss: -0.5422, acc 0.9928\n",
            "epoch 1263: train D loss: 0.6776, train F loss: -0.5580, acc 0.9946\n",
            "epoch 1264: train D loss: 0.6781, train F loss: -0.5606, acc 0.9946\n",
            "epoch 1265: train D loss: 0.6804, train F loss: -0.5562, acc 0.9936\n",
            "epoch 1266: train D loss: 0.6748, train F loss: -0.5570, acc 0.9950\n",
            "epoch 1267: train D loss: 0.6788, train F loss: -0.5664, acc 0.9966\n",
            "epoch 1268: train D loss: 0.6791, train F loss: -0.5635, acc 0.9958\n",
            "epoch 1269: train D loss: 0.6834, train F loss: -0.5556, acc 0.9938\n",
            "epoch 1270: train D loss: 0.6773, train F loss: -0.5538, acc 0.9942\n",
            "epoch 1271: train D loss: 0.6749, train F loss: -0.5498, acc 0.9934\n",
            "epoch 1272: train D loss: 0.6809, train F loss: -0.5623, acc 0.9952\n",
            "epoch 1273: train D loss: 0.6807, train F loss: -0.5618, acc 0.9946\n",
            "epoch 1274: train D loss: 0.6738, train F loss: -0.5577, acc 0.9954\n",
            "epoch 1275: train D loss: 0.6832, train F loss: -0.5659, acc 0.9950\n",
            "epoch 1276: train D loss: 0.6796, train F loss: -0.5592, acc 0.9924\n",
            "epoch 1277: train D loss: 0.6770, train F loss: -0.5612, acc 0.9946\n",
            "epoch 1278: train D loss: 0.6859, train F loss: -0.5271, acc 0.9876\n",
            "epoch 1279: train D loss: 0.6773, train F loss: -0.5502, acc 0.9914\n",
            "epoch 1280: train D loss: 0.6768, train F loss: -0.5569, acc 0.9940\n",
            "epoch 1281: train D loss: 0.6806, train F loss: -0.5661, acc 0.9952\n",
            "epoch 1282: train D loss: 0.6821, train F loss: -0.5687, acc 0.9958\n",
            "epoch 1283: train D loss: 0.6810, train F loss: -0.5613, acc 0.9938\n",
            "epoch 1284: train D loss: 0.6799, train F loss: -0.5580, acc 0.9932\n",
            "epoch 1285: train D loss: 0.6779, train F loss: -0.5578, acc 0.9936\n",
            "epoch 1286: train D loss: 0.6806, train F loss: -0.5658, acc 0.9944\n",
            "epoch 1287: train D loss: 0.6796, train F loss: -0.5624, acc 0.9944\n",
            "epoch 1288: train D loss: 0.6792, train F loss: -0.5666, acc 0.9954\n",
            "epoch 1289: train D loss: 0.6793, train F loss: -0.5575, acc 0.9936\n",
            "epoch 1290: train D loss: 0.6815, train F loss: -0.5650, acc 0.9942\n",
            "epoch 1291: train D loss: 0.6772, train F loss: -0.5651, acc 0.9952\n",
            "epoch 1292: train D loss: 0.6751, train F loss: -0.5676, acc 0.9972\n",
            "epoch 1293: train D loss: 0.6842, train F loss: -0.5660, acc 0.9940\n",
            "epoch 1294: train D loss: 0.6778, train F loss: -0.5609, acc 0.9938\n",
            "epoch 1295: train D loss: 0.6791, train F loss: -0.5563, acc 0.9930\n",
            "epoch 1296: train D loss: 0.6780, train F loss: -0.5589, acc 0.9928\n",
            "epoch 1297: train D loss: 0.6844, train F loss: -0.5664, acc 0.9936\n",
            "epoch 1298: train D loss: 0.6812, train F loss: -0.5470, acc 0.9908\n",
            "epoch 1299: train D loss: 0.6753, train F loss: -0.5646, acc 0.9954\n",
            "epoch 1300: train D loss: 0.6798, train F loss: -0.5654, acc 0.9940\n",
            "epoch 1301: train D loss: 0.6817, train F loss: -0.5633, acc 0.9938\n",
            "epoch 1302: train D loss: 0.6833, train F loss: -0.5733, acc 0.9956\n",
            "epoch 1303: train D loss: 0.6830, train F loss: -0.5699, acc 0.9948\n",
            "epoch 1304: train D loss: 0.6797, train F loss: -0.5692, acc 0.9948\n",
            "epoch 1305: train D loss: 0.6790, train F loss: -0.5717, acc 0.9962\n",
            "epoch 1306: train D loss: 0.6807, train F loss: -0.5611, acc 0.9934\n",
            "epoch 1307: train D loss: 0.6776, train F loss: -0.5643, acc 0.9936\n",
            "epoch 1308: train D loss: 0.6843, train F loss: -0.5718, acc 0.9948\n",
            "epoch 1309: train D loss: 0.6765, train F loss: -0.5621, acc 0.9948\n",
            "epoch 1310: train D loss: 0.6841, train F loss: -0.5736, acc 0.9954\n",
            "epoch 1311: train D loss: 0.6797, train F loss: -0.5671, acc 0.9946\n",
            "epoch 1312: train D loss: 0.6829, train F loss: -0.5678, acc 0.9934\n",
            "epoch 1313: train D loss: 0.6787, train F loss: -0.5672, acc 0.9944\n",
            "epoch 1314: train D loss: 0.6787, train F loss: -0.5672, acc 0.9944\n",
            "epoch 1315: train D loss: 0.6783, train F loss: -0.5588, acc 0.9920\n",
            "epoch 1316: train D loss: 0.6783, train F loss: -0.5690, acc 0.9948\n",
            "epoch 1317: train D loss: 0.6786, train F loss: -0.5721, acc 0.9960\n",
            "epoch 1318: train D loss: 0.6840, train F loss: -0.5724, acc 0.9940\n",
            "epoch 1319: train D loss: 0.6763, train F loss: -0.5648, acc 0.9942\n",
            "epoch 1320: train D loss: 0.6860, train F loss: -0.5722, acc 0.9942\n",
            "epoch 1321: train D loss: 0.6810, train F loss: -0.5570, acc 0.9928\n",
            "epoch 1322: train D loss: 0.6823, train F loss: -0.5697, acc 0.9942\n",
            "epoch 1323: train D loss: 0.6780, train F loss: -0.5705, acc 0.9950\n",
            "epoch 1324: train D loss: 0.6779, train F loss: -0.5730, acc 0.9958\n",
            "epoch 1325: train D loss: 0.6847, train F loss: -0.5669, acc 0.9934\n",
            "epoch 1326: train D loss: 0.6831, train F loss: -0.5758, acc 0.9952\n",
            "epoch 1327: train D loss: 0.6800, train F loss: -0.5662, acc 0.9922\n",
            "epoch 1328: train D loss: 0.6765, train F loss: -0.5702, acc 0.9944\n",
            "epoch 1329: train D loss: 0.6834, train F loss: -0.5701, acc 0.9934\n",
            "epoch 1330: train D loss: 0.6830, train F loss: -0.5700, acc 0.9926\n",
            "epoch 1331: train D loss: 0.6799, train F loss: -0.5702, acc 0.9944\n",
            "epoch 1332: train D loss: 0.6765, train F loss: -0.5721, acc 0.9954\n",
            "epoch 1333: train D loss: 0.6860, train F loss: -0.5672, acc 0.9934\n",
            "epoch 1334: train D loss: 0.6805, train F loss: -0.5681, acc 0.9930\n",
            "epoch 1335: train D loss: 0.6792, train F loss: -0.5457, acc 0.9868\n",
            "epoch 1336: train D loss: 0.6726, train F loss: -0.5656, acc 0.9944\n",
            "epoch 1337: train D loss: 0.6758, train F loss: -0.5644, acc 0.9934\n",
            "epoch 1338: train D loss: 0.6799, train F loss: -0.5717, acc 0.9944\n",
            "epoch 1339: train D loss: 0.6800, train F loss: -0.5573, acc 0.9914\n",
            "epoch 1340: train D loss: 0.6777, train F loss: -0.5653, acc 0.9928\n",
            "epoch 1341: train D loss: 0.6785, train F loss: -0.5753, acc 0.9956\n",
            "epoch 1342: train D loss: 0.6774, train F loss: -0.5684, acc 0.9936\n",
            "epoch 1343: train D loss: 0.6819, train F loss: -0.5768, acc 0.9948\n",
            "epoch 1344: train D loss: 0.6815, train F loss: -0.5751, acc 0.9950\n",
            "epoch 1345: train D loss: 0.6816, train F loss: -0.5760, acc 0.9956\n",
            "epoch 1346: train D loss: 0.6808, train F loss: -0.5778, acc 0.9950\n",
            "epoch 1347: train D loss: 0.6769, train F loss: -0.5697, acc 0.9940\n",
            "epoch 1348: train D loss: 0.6823, train F loss: -0.5483, acc 0.9880\n",
            "epoch 1349: train D loss: 0.6820, train F loss: -0.5725, acc 0.9940\n",
            "epoch 1350: train D loss: 0.6819, train F loss: -0.5756, acc 0.9940\n",
            "epoch 1351: train D loss: 0.6800, train F loss: -0.5798, acc 0.9960\n",
            "epoch 1352: train D loss: 0.6815, train F loss: -0.5750, acc 0.9938\n",
            "epoch 1353: train D loss: 0.6748, train F loss: -0.5747, acc 0.9968\n",
            "epoch 1354: train D loss: 0.6797, train F loss: -0.5709, acc 0.9942\n",
            "epoch 1355: train D loss: 0.6804, train F loss: -0.5798, acc 0.9950\n",
            "epoch 1356: train D loss: 0.6821, train F loss: -0.5704, acc 0.9924\n",
            "epoch 1357: train D loss: 0.6775, train F loss: -0.5726, acc 0.9942\n",
            "epoch 1358: train D loss: 0.6813, train F loss: -0.5744, acc 0.9938\n",
            "epoch 1359: train D loss: 0.6758, train F loss: -0.5685, acc 0.9950\n",
            "epoch 1360: train D loss: 0.6821, train F loss: -0.5749, acc 0.9944\n",
            "epoch 1361: train D loss: 0.6813, train F loss: -0.5775, acc 0.9946\n",
            "epoch 1362: train D loss: 0.6784, train F loss: -0.5785, acc 0.9946\n",
            "epoch 1363: train D loss: 0.6828, train F loss: -0.5664, acc 0.9938\n",
            "epoch 1364: train D loss: 0.6792, train F loss: -0.5671, acc 0.9924\n",
            "epoch 1365: train D loss: 0.6783, train F loss: -0.5774, acc 0.9958\n",
            "epoch 1366: train D loss: 0.6838, train F loss: -0.5807, acc 0.9938\n",
            "epoch 1367: train D loss: 0.6832, train F loss: -0.5778, acc 0.9938\n",
            "epoch 1368: train D loss: 0.6818, train F loss: -0.5817, acc 0.9950\n",
            "epoch 1369: train D loss: 0.6792, train F loss: -0.5808, acc 0.9952\n",
            "epoch 1370: train D loss: 0.6796, train F loss: -0.5748, acc 0.9940\n",
            "epoch 1371: train D loss: 0.6821, train F loss: -0.5818, acc 0.9946\n",
            "epoch 1372: train D loss: 0.6843, train F loss: -0.5826, acc 0.9950\n",
            "epoch 1373: train D loss: 0.6826, train F loss: -0.5830, acc 0.9952\n",
            "epoch 1374: train D loss: 0.6828, train F loss: -0.5734, acc 0.9932\n",
            "epoch 1375: train D loss: 0.6811, train F loss: -0.5691, acc 0.9930\n",
            "epoch 1376: train D loss: 0.6810, train F loss: -0.5639, acc 0.9902\n",
            "epoch 1377: train D loss: 0.6847, train F loss: -0.5818, acc 0.9938\n",
            "epoch 1378: train D loss: 0.6818, train F loss: -0.5804, acc 0.9938\n",
            "epoch 1379: train D loss: 0.6800, train F loss: -0.5785, acc 0.9932\n",
            "epoch 1380: train D loss: 0.6826, train F loss: -0.5816, acc 0.9942\n",
            "epoch 1381: train D loss: 0.6813, train F loss: -0.5767, acc 0.9946\n",
            "epoch 1382: train D loss: 0.6831, train F loss: -0.5743, acc 0.9926\n",
            "epoch 1383: train D loss: 0.6804, train F loss: -0.5761, acc 0.9940\n",
            "epoch 1384: train D loss: 0.6825, train F loss: -0.5860, acc 0.9938\n",
            "epoch 1385: train D loss: 0.6825, train F loss: -0.5595, acc 0.9892\n",
            "epoch 1386: train D loss: 0.6791, train F loss: -0.5726, acc 0.9928\n",
            "epoch 1387: train D loss: 0.6792, train F loss: -0.5777, acc 0.9944\n",
            "epoch 1388: train D loss: 0.6757, train F loss: -0.5758, acc 0.9934\n",
            "epoch 1389: train D loss: 0.6787, train F loss: -0.5784, acc 0.9944\n",
            "epoch 1390: train D loss: 0.6836, train F loss: -0.5840, acc 0.9946\n",
            "epoch 1391: train D loss: 0.6803, train F loss: -0.5838, acc 0.9954\n",
            "epoch 1392: train D loss: 0.6816, train F loss: -0.5772, acc 0.9940\n",
            "epoch 1393: train D loss: 0.6799, train F loss: -0.5675, acc 0.9922\n",
            "epoch 1394: train D loss: 0.6819, train F loss: -0.5775, acc 0.9936\n",
            "epoch 1395: train D loss: 0.6833, train F loss: -0.5813, acc 0.9942\n",
            "epoch 1396: train D loss: 0.6834, train F loss: -0.5906, acc 0.9956\n",
            "epoch 1397: train D loss: 0.6824, train F loss: -0.5885, acc 0.9956\n",
            "epoch 1398: train D loss: 0.6816, train F loss: -0.5862, acc 0.9946\n",
            "epoch 1399: train D loss: 0.6812, train F loss: -0.5865, acc 0.9952\n",
            "epoch 1400: train D loss: 0.6791, train F loss: -0.5803, acc 0.9948\n",
            "epoch 1401: train D loss: 0.6812, train F loss: -0.5624, acc 0.9890\n",
            "epoch 1402: train D loss: 0.6763, train F loss: -0.5797, acc 0.9950\n",
            "epoch 1403: train D loss: 0.6813, train F loss: -0.5823, acc 0.9944\n",
            "epoch 1404: train D loss: 0.6784, train F loss: -0.5731, acc 0.9914\n",
            "epoch 1405: train D loss: 0.6793, train F loss: -0.5781, acc 0.9934\n",
            "epoch 1406: train D loss: 0.6802, train F loss: -0.5840, acc 0.9950\n",
            "epoch 1407: train D loss: 0.6789, train F loss: -0.5771, acc 0.9940\n",
            "epoch 1408: train D loss: 0.6798, train F loss: -0.5785, acc 0.9930\n",
            "epoch 1409: train D loss: 0.6796, train F loss: -0.5704, acc 0.9928\n",
            "epoch 1410: train D loss: 0.6833, train F loss: -0.5877, acc 0.9948\n",
            "epoch 1411: train D loss: 0.6833, train F loss: -0.5788, acc 0.9934\n",
            "epoch 1412: train D loss: 0.6752, train F loss: -0.5774, acc 0.9944\n",
            "epoch 1413: train D loss: 0.6795, train F loss: -0.5815, acc 0.9932\n",
            "epoch 1414: train D loss: 0.6818, train F loss: -0.5856, acc 0.9948\n",
            "epoch 1415: train D loss: 0.6824, train F loss: -0.5851, acc 0.9942\n",
            "epoch 1416: train D loss: 0.6809, train F loss: -0.5804, acc 0.9932\n",
            "epoch 1417: train D loss: 0.6805, train F loss: -0.5891, acc 0.9954\n",
            "epoch 1418: train D loss: 0.6782, train F loss: -0.5857, acc 0.9956\n",
            "epoch 1419: train D loss: 0.6815, train F loss: -0.5813, acc 0.9950\n",
            "epoch 1420: train D loss: 0.6865, train F loss: -0.5898, acc 0.9946\n",
            "epoch 1421: train D loss: 0.6748, train F loss: -0.5840, acc 0.9954\n",
            "epoch 1422: train D loss: 0.6777, train F loss: -0.5875, acc 0.9958\n",
            "epoch 1423: train D loss: 0.6832, train F loss: -0.5859, acc 0.9940\n",
            "epoch 1424: train D loss: 0.6829, train F loss: -0.5662, acc 0.9914\n",
            "epoch 1425: train D loss: 0.6831, train F loss: -0.5853, acc 0.9942\n",
            "epoch 1426: train D loss: 0.6816, train F loss: -0.5759, acc 0.9936\n",
            "epoch 1427: train D loss: 0.6803, train F loss: -0.5919, acc 0.9960\n",
            "epoch 1428: train D loss: 0.6815, train F loss: -0.5899, acc 0.9956\n",
            "epoch 1429: train D loss: 0.6794, train F loss: -0.5888, acc 0.9956\n",
            "epoch 1430: train D loss: 0.6798, train F loss: -0.5851, acc 0.9938\n",
            "epoch 1431: train D loss: 0.6805, train F loss: -0.5858, acc 0.9950\n",
            "epoch 1432: train D loss: 0.6857, train F loss: -0.5806, acc 0.9934\n",
            "epoch 1433: train D loss: 0.6836, train F loss: -0.5842, acc 0.9930\n",
            "epoch 1434: train D loss: 0.6808, train F loss: -0.5924, acc 0.9958\n",
            "epoch 1435: train D loss: 0.6797, train F loss: -0.5872, acc 0.9950\n",
            "epoch 1436: train D loss: 0.6824, train F loss: -0.5880, acc 0.9948\n",
            "epoch 1437: train D loss: 0.6809, train F loss: -0.5883, acc 0.9950\n",
            "epoch 1438: train D loss: 0.6823, train F loss: -0.5773, acc 0.9918\n",
            "epoch 1439: train D loss: 0.6859, train F loss: -0.5807, acc 0.9906\n",
            "epoch 1440: train D loss: 0.6802, train F loss: -0.5873, acc 0.9946\n",
            "epoch 1441: train D loss: 0.6844, train F loss: -0.5870, acc 0.9938\n",
            "epoch 1442: train D loss: 0.6823, train F loss: -0.5901, acc 0.9946\n",
            "epoch 1443: train D loss: 0.6812, train F loss: -0.5944, acc 0.9962\n",
            "epoch 1444: train D loss: 0.6834, train F loss: -0.5973, acc 0.9962\n",
            "epoch 1445: train D loss: 0.6813, train F loss: -0.5939, acc 0.9958\n",
            "epoch 1446: train D loss: 0.6802, train F loss: -0.5908, acc 0.9956\n",
            "epoch 1447: train D loss: 0.6798, train F loss: -0.5804, acc 0.9944\n",
            "epoch 1448: train D loss: 0.6820, train F loss: -0.5894, acc 0.9944\n",
            "epoch 1449: train D loss: 0.6786, train F loss: -0.5881, acc 0.9942\n",
            "epoch 1450: train D loss: 0.6772, train F loss: -0.5904, acc 0.9956\n",
            "epoch 1451: train D loss: 0.6834, train F loss: -0.5881, acc 0.9936\n",
            "epoch 1452: train D loss: 0.6842, train F loss: -0.5870, acc 0.9924\n",
            "epoch 1453: train D loss: 0.6775, train F loss: -0.5895, acc 0.9960\n",
            "epoch 1454: train D loss: 0.6842, train F loss: -0.5947, acc 0.9954\n",
            "epoch 1455: train D loss: 0.6820, train F loss: -0.5953, acc 0.9954\n",
            "epoch 1456: train D loss: 0.6854, train F loss: -0.5895, acc 0.9940\n",
            "epoch 1457: train D loss: 0.6790, train F loss: -0.5824, acc 0.9930\n",
            "epoch 1458: train D loss: 0.6811, train F loss: -0.5904, acc 0.9930\n",
            "epoch 1459: train D loss: 0.6821, train F loss: -0.5912, acc 0.9944\n",
            "epoch 1460: train D loss: 0.6830, train F loss: -0.5944, acc 0.9948\n",
            "epoch 1461: train D loss: 0.6857, train F loss: -0.6018, acc 0.9960\n",
            "epoch 1462: train D loss: 0.6838, train F loss: -0.5906, acc 0.9932\n",
            "epoch 1463: train D loss: 0.6829, train F loss: -0.5928, acc 0.9940\n",
            "epoch 1464: train D loss: 0.6815, train F loss: -0.5873, acc 0.9920\n",
            "epoch 1465: train D loss: 0.6817, train F loss: -0.5654, acc 0.9898\n",
            "epoch 1466: train D loss: 0.6792, train F loss: -0.5806, acc 0.9924\n",
            "epoch 1467: train D loss: 0.6794, train F loss: -0.5963, acc 0.9960\n",
            "epoch 1468: train D loss: 0.6822, train F loss: -0.5842, acc 0.9926\n",
            "epoch 1469: train D loss: 0.6815, train F loss: -0.5907, acc 0.9940\n",
            "epoch 1470: train D loss: 0.6866, train F loss: -0.5846, acc 0.9918\n",
            "epoch 1471: train D loss: 0.6795, train F loss: -0.5955, acc 0.9954\n",
            "epoch 1472: train D loss: 0.6788, train F loss: -0.5940, acc 0.9962\n",
            "epoch 1473: train D loss: 0.6821, train F loss: -0.5990, acc 0.9958\n",
            "epoch 1474: train D loss: 0.6785, train F loss: -0.5934, acc 0.9948\n",
            "epoch 1475: train D loss: 0.6824, train F loss: -0.5987, acc 0.9952\n",
            "epoch 1476: train D loss: 0.6772, train F loss: -0.5880, acc 0.9936\n",
            "epoch 1477: train D loss: 0.6790, train F loss: -0.5592, acc 0.9920\n",
            "epoch 1478: train D loss: 0.6815, train F loss: -0.5764, acc 0.9896\n",
            "epoch 1479: train D loss: 0.6793, train F loss: -0.5938, acc 0.9948\n",
            "epoch 1480: train D loss: 0.6794, train F loss: -0.5970, acc 0.9954\n",
            "epoch 1481: train D loss: 0.6801, train F loss: -0.5992, acc 0.9962\n",
            "epoch 1482: train D loss: 0.6807, train F loss: -0.5935, acc 0.9954\n",
            "epoch 1483: train D loss: 0.6822, train F loss: -0.5892, acc 0.9932\n",
            "epoch 1484: train D loss: 0.6800, train F loss: -0.5979, acc 0.9960\n",
            "epoch 1485: train D loss: 0.6762, train F loss: -0.5914, acc 0.9950\n",
            "epoch 1486: train D loss: 0.6862, train F loss: -0.5895, acc 0.9940\n",
            "epoch 1487: train D loss: 0.6829, train F loss: -0.5938, acc 0.9944\n",
            "epoch 1488: train D loss: 0.6819, train F loss: -0.5944, acc 0.9940\n",
            "epoch 1489: train D loss: 0.6807, train F loss: -0.5918, acc 0.9936\n",
            "epoch 1490: train D loss: 0.6762, train F loss: -0.5955, acc 0.9956\n",
            "epoch 1491: train D loss: 0.6817, train F loss: -0.5993, acc 0.9960\n",
            "epoch 1492: train D loss: 0.6873, train F loss: -0.6032, acc 0.9956\n",
            "epoch 1493: train D loss: 0.6807, train F loss: -0.5978, acc 0.9950\n",
            "epoch 1494: train D loss: 0.6800, train F loss: -0.5954, acc 0.9952\n",
            "epoch 1495: train D loss: 0.6832, train F loss: -0.5941, acc 0.9944\n",
            "epoch 1496: train D loss: 0.6822, train F loss: -0.5889, acc 0.9946\n",
            "epoch 1497: train D loss: 0.6824, train F loss: -0.5916, acc 0.9914\n",
            "epoch 1498: train D loss: 0.6833, train F loss: -0.5996, acc 0.9940\n",
            "epoch 1499: train D loss: 0.6789, train F loss: -0.5960, acc 0.9948\n",
            "epoch 1500: train D loss: 0.6786, train F loss: -0.5979, acc 0.9956\n",
            "epoch 1501: train D loss: 0.6822, train F loss: -0.5995, acc 0.9948\n",
            "epoch 1502: train D loss: 0.6826, train F loss: -0.5977, acc 0.9938\n",
            "epoch 1503: train D loss: 0.6822, train F loss: -0.5974, acc 0.9942\n",
            "epoch 1504: train D loss: 0.6795, train F loss: -0.5985, acc 0.9962\n",
            "epoch 1505: train D loss: 0.6808, train F loss: -0.5943, acc 0.9944\n",
            "epoch 1506: train D loss: 0.6843, train F loss: -0.6035, acc 0.9956\n",
            "epoch 1507: train D loss: 0.6821, train F loss: -0.5919, acc 0.9918\n",
            "epoch 1508: train D loss: 0.6797, train F loss: -0.5966, acc 0.9942\n",
            "epoch 1509: train D loss: 0.6808, train F loss: -0.5948, acc 0.9938\n",
            "epoch 1510: train D loss: 0.6850, train F loss: -0.6030, acc 0.9948\n",
            "epoch 1511: train D loss: 0.6786, train F loss: -0.6001, acc 0.9960\n",
            "epoch 1512: train D loss: 0.6845, train F loss: -0.6027, acc 0.9948\n",
            "epoch 1513: train D loss: 0.6837, train F loss: -0.5998, acc 0.9934\n",
            "epoch 1514: train D loss: 0.6854, train F loss: -0.5952, acc 0.9928\n",
            "epoch 1515: train D loss: 0.6809, train F loss: -0.5963, acc 0.9932\n",
            "epoch 1516: train D loss: 0.6850, train F loss: -0.5926, acc 0.9920\n",
            "epoch 1517: train D loss: 0.6812, train F loss: -0.5985, acc 0.9942\n",
            "epoch 1518: train D loss: 0.6816, train F loss: -0.5903, acc 0.9926\n",
            "epoch 1519: train D loss: 0.6790, train F loss: -0.5929, acc 0.9942\n",
            "epoch 1520: train D loss: 0.6834, train F loss: -0.6028, acc 0.9948\n",
            "epoch 1521: train D loss: 0.6770, train F loss: -0.5916, acc 0.9944\n",
            "epoch 1522: train D loss: 0.6842, train F loss: -0.5950, acc 0.9926\n",
            "epoch 1523: train D loss: 0.6822, train F loss: -0.5969, acc 0.9934\n",
            "epoch 1524: train D loss: 0.6814, train F loss: -0.5998, acc 0.9930\n",
            "epoch 1525: train D loss: 0.6808, train F loss: -0.5957, acc 0.9938\n",
            "epoch 1526: train D loss: 0.6800, train F loss: -0.5956, acc 0.9942\n",
            "epoch 1527: train D loss: 0.6805, train F loss: -0.5973, acc 0.9948\n",
            "epoch 1528: train D loss: 0.6792, train F loss: -0.5970, acc 0.9944\n",
            "epoch 1529: train D loss: 0.6831, train F loss: -0.6049, acc 0.9954\n",
            "epoch 1530: train D loss: 0.6810, train F loss: -0.6001, acc 0.9952\n",
            "epoch 1531: train D loss: 0.6780, train F loss: -0.6000, acc 0.9956\n",
            "epoch 1532: train D loss: 0.6781, train F loss: -0.6020, acc 0.9960\n",
            "epoch 1533: train D loss: 0.6800, train F loss: -0.5830, acc 0.9912\n",
            "epoch 1534: train D loss: 0.6808, train F loss: -0.5992, acc 0.9936\n",
            "epoch 1535: train D loss: 0.6844, train F loss: -0.6036, acc 0.9946\n",
            "epoch 1536: train D loss: 0.6832, train F loss: -0.6099, acc 0.9968\n",
            "epoch 1537: train D loss: 0.6828, train F loss: -0.5688, acc 0.9904\n",
            "epoch 1538: train D loss: 0.6782, train F loss: -0.5889, acc 0.9932\n",
            "epoch 1539: train D loss: 0.6806, train F loss: -0.6031, acc 0.9954\n",
            "epoch 1540: train D loss: 0.6764, train F loss: -0.5861, acc 0.9934\n",
            "epoch 1541: train D loss: 0.6785, train F loss: -0.5997, acc 0.9946\n",
            "epoch 1542: train D loss: 0.6826, train F loss: -0.6018, acc 0.9942\n",
            "epoch 1543: train D loss: 0.6804, train F loss: -0.6053, acc 0.9954\n",
            "epoch 1544: train D loss: 0.6783, train F loss: -0.6032, acc 0.9964\n",
            "epoch 1545: train D loss: 0.6837, train F loss: -0.6023, acc 0.9942\n",
            "epoch 1546: train D loss: 0.6788, train F loss: -0.5874, acc 0.9912\n",
            "epoch 1547: train D loss: 0.6797, train F loss: -0.6030, acc 0.9948\n",
            "epoch 1548: train D loss: 0.6800, train F loss: -0.6029, acc 0.9948\n",
            "epoch 1549: train D loss: 0.6822, train F loss: -0.5999, acc 0.9928\n",
            "epoch 1550: train D loss: 0.6766, train F loss: -0.6039, acc 0.9958\n",
            "epoch 1551: train D loss: 0.6796, train F loss: -0.6039, acc 0.9954\n",
            "epoch 1552: train D loss: 0.6773, train F loss: -0.6004, acc 0.9950\n",
            "epoch 1553: train D loss: 0.6807, train F loss: -0.6065, acc 0.9952\n",
            "epoch 1554: train D loss: 0.6806, train F loss: -0.6016, acc 0.9950\n",
            "epoch 1555: train D loss: 0.6791, train F loss: -0.5961, acc 0.9944\n",
            "epoch 1556: train D loss: 0.6789, train F loss: -0.5147, acc 0.9882\n",
            "epoch 1557: train D loss: 0.6814, train F loss: -0.5861, acc 0.9894\n",
            "epoch 1558: train D loss: 0.6787, train F loss: -0.6014, acc 0.9946\n",
            "epoch 1559: train D loss: 0.6801, train F loss: -0.5984, acc 0.9932\n",
            "epoch 1560: train D loss: 0.6799, train F loss: -0.5875, acc 0.9944\n",
            "epoch 1561: train D loss: 0.6823, train F loss: -0.6031, acc 0.9942\n",
            "epoch 1562: train D loss: 0.6820, train F loss: -0.6068, acc 0.9956\n",
            "epoch 1563: train D loss: 0.6798, train F loss: -0.6068, acc 0.9952\n",
            "epoch 1564: train D loss: 0.6819, train F loss: -0.6026, acc 0.9942\n",
            "epoch 1565: train D loss: 0.6825, train F loss: -0.6023, acc 0.9932\n",
            "epoch 1566: train D loss: 0.6797, train F loss: -0.6095, acc 0.9964\n",
            "epoch 1567: train D loss: 0.6813, train F loss: -0.6069, acc 0.9952\n",
            "epoch 1568: train D loss: 0.6805, train F loss: -0.5970, acc 0.9930\n",
            "epoch 1569: train D loss: 0.6808, train F loss: -0.6032, acc 0.9950\n",
            "epoch 1570: train D loss: 0.6779, train F loss: -0.5920, acc 0.9944\n",
            "epoch 1571: train D loss: 0.6811, train F loss: -0.6073, acc 0.9960\n",
            "epoch 1572: train D loss: 0.6851, train F loss: -0.6140, acc 0.9964\n",
            "epoch 1573: train D loss: 0.6840, train F loss: -0.6090, acc 0.9950\n",
            "epoch 1574: train D loss: 0.6788, train F loss: -0.6060, acc 0.9948\n",
            "epoch 1575: train D loss: 0.6841, train F loss: -0.6088, acc 0.9942\n",
            "epoch 1576: train D loss: 0.6812, train F loss: -0.5913, acc 0.9928\n",
            "epoch 1577: train D loss: 0.6798, train F loss: -0.6060, acc 0.9950\n",
            "epoch 1578: train D loss: 0.6828, train F loss: -0.5971, acc 0.9928\n",
            "epoch 1579: train D loss: 0.6787, train F loss: -0.6019, acc 0.9948\n",
            "epoch 1580: train D loss: 0.6768, train F loss: -0.6034, acc 0.9954\n",
            "epoch 1581: train D loss: 0.6831, train F loss: -0.6107, acc 0.9948\n",
            "epoch 1582: train D loss: 0.6816, train F loss: -0.6054, acc 0.9946\n",
            "epoch 1583: train D loss: 0.6828, train F loss: -0.6027, acc 0.9938\n",
            "epoch 1584: train D loss: 0.6808, train F loss: -0.6062, acc 0.9950\n",
            "epoch 1585: train D loss: 0.6789, train F loss: -0.6019, acc 0.9950\n",
            "epoch 1586: train D loss: 0.6838, train F loss: -0.6111, acc 0.9952\n",
            "epoch 1587: train D loss: 0.6785, train F loss: -0.6106, acc 0.9960\n",
            "epoch 1588: train D loss: 0.6796, train F loss: -0.5993, acc 0.9928\n",
            "epoch 1589: train D loss: 0.6784, train F loss: -0.6014, acc 0.9946\n",
            "epoch 1590: train D loss: 0.6808, train F loss: -0.6076, acc 0.9944\n",
            "epoch 1591: train D loss: 0.6827, train F loss: -0.6053, acc 0.9938\n",
            "epoch 1592: train D loss: 0.6820, train F loss: -0.6127, acc 0.9958\n",
            "epoch 1593: train D loss: 0.6812, train F loss: -0.6128, acc 0.9958\n",
            "epoch 1594: train D loss: 0.6847, train F loss: -0.6126, acc 0.9950\n",
            "epoch 1595: train D loss: 0.6842, train F loss: -0.6076, acc 0.9938\n",
            "epoch 1596: train D loss: 0.6834, train F loss: -0.6044, acc 0.9934\n",
            "epoch 1597: train D loss: 0.6819, train F loss: -0.5814, acc 0.9890\n",
            "epoch 1598: train D loss: 0.6791, train F loss: -0.6000, acc 0.9934\n",
            "epoch 1599: train D loss: 0.6773, train F loss: -0.6038, acc 0.9958\n",
            "epoch 1600: train D loss: 0.6777, train F loss: -0.6075, acc 0.9958\n",
            "epoch 1601: train D loss: 0.6816, train F loss: -0.6074, acc 0.9938\n",
            "epoch 1602: train D loss: 0.6809, train F loss: -0.6031, acc 0.9952\n",
            "epoch 1603: train D loss: 0.6803, train F loss: -0.6084, acc 0.9954\n",
            "epoch 1604: train D loss: 0.6826, train F loss: -0.6055, acc 0.9946\n",
            "epoch 1605: train D loss: 0.6846, train F loss: -0.5868, acc 0.9920\n",
            "epoch 1606: train D loss: 0.6811, train F loss: -0.6059, acc 0.9934\n",
            "epoch 1607: train D loss: 0.6838, train F loss: -0.6072, acc 0.9944\n",
            "epoch 1608: train D loss: 0.6809, train F loss: -0.6069, acc 0.9936\n",
            "epoch 1609: train D loss: 0.6838, train F loss: -0.6044, acc 0.9940\n",
            "epoch 1610: train D loss: 0.6833, train F loss: -0.6099, acc 0.9944\n",
            "epoch 1611: train D loss: 0.6810, train F loss: -0.6127, acc 0.9956\n",
            "epoch 1612: train D loss: 0.6834, train F loss: -0.6163, acc 0.9962\n",
            "epoch 1613: train D loss: 0.6813, train F loss: -0.6137, acc 0.9952\n",
            "epoch 1614: train D loss: 0.6828, train F loss: -0.6077, acc 0.9932\n",
            "epoch 1615: train D loss: 0.6818, train F loss: -0.6078, acc 0.9938\n",
            "epoch 1616: train D loss: 0.6815, train F loss: -0.6121, acc 0.9946\n",
            "epoch 1617: train D loss: 0.6820, train F loss: -0.6118, acc 0.9954\n",
            "epoch 1618: train D loss: 0.6760, train F loss: -0.5974, acc 0.9930\n",
            "epoch 1619: train D loss: 0.6827, train F loss: -0.6055, acc 0.9928\n",
            "epoch 1620: train D loss: 0.6799, train F loss: -0.6084, acc 0.9952\n",
            "epoch 1621: train D loss: 0.6837, train F loss: -0.6117, acc 0.9952\n",
            "epoch 1622: train D loss: 0.6797, train F loss: -0.6059, acc 0.9938\n",
            "epoch 1623: train D loss: 0.6833, train F loss: -0.6075, acc 0.9938\n",
            "epoch 1624: train D loss: 0.6853, train F loss: -0.6144, acc 0.9944\n",
            "epoch 1625: train D loss: 0.6826, train F loss: -0.6161, acc 0.9964\n",
            "epoch 1626: train D loss: 0.6815, train F loss: -0.6157, acc 0.9954\n",
            "epoch 1627: train D loss: 0.6835, train F loss: -0.6079, acc 0.9934\n",
            "epoch 1628: train D loss: 0.6820, train F loss: -0.6100, acc 0.9946\n",
            "epoch 1629: train D loss: 0.6786, train F loss: -0.6023, acc 0.9926\n",
            "epoch 1630: train D loss: 0.6824, train F loss: -0.6083, acc 0.9952\n",
            "epoch 1631: train D loss: 0.6814, train F loss: -0.6127, acc 0.9954\n",
            "epoch 1632: train D loss: 0.6827, train F loss: -0.6161, acc 0.9944\n",
            "epoch 1633: train D loss: 0.6806, train F loss: -0.6116, acc 0.9954\n",
            "epoch 1634: train D loss: 0.6836, train F loss: -0.6177, acc 0.9960\n",
            "epoch 1635: train D loss: 0.6835, train F loss: -0.6106, acc 0.9928\n",
            "epoch 1636: train D loss: 0.6857, train F loss: -0.6124, acc 0.9934\n",
            "epoch 1637: train D loss: 0.6868, train F loss: -0.6184, acc 0.9950\n",
            "epoch 1638: train D loss: 0.6814, train F loss: -0.5960, acc 0.9914\n",
            "epoch 1639: train D loss: 0.6778, train F loss: -0.5998, acc 0.9938\n",
            "epoch 1640: train D loss: 0.6835, train F loss: -0.5954, acc 0.9910\n",
            "epoch 1641: train D loss: 0.6799, train F loss: -0.6094, acc 0.9934\n",
            "epoch 1642: train D loss: 0.6792, train F loss: -0.6125, acc 0.9948\n",
            "epoch 1643: train D loss: 0.6817, train F loss: -0.6163, acc 0.9958\n",
            "epoch 1644: train D loss: 0.6821, train F loss: -0.6125, acc 0.9946\n",
            "epoch 1645: train D loss: 0.6796, train F loss: -0.6090, acc 0.9940\n",
            "epoch 1646: train D loss: 0.6836, train F loss: -0.6153, acc 0.9948\n",
            "epoch 1647: train D loss: 0.6834, train F loss: -0.6175, acc 0.9952\n",
            "epoch 1648: train D loss: 0.6804, train F loss: -0.6134, acc 0.9958\n",
            "epoch 1649: train D loss: 0.6831, train F loss: -0.6106, acc 0.9934\n",
            "epoch 1650: train D loss: 0.6839, train F loss: -0.6133, acc 0.9944\n",
            "epoch 1651: train D loss: 0.6849, train F loss: -0.6212, acc 0.9952\n",
            "epoch 1652: train D loss: 0.6808, train F loss: -0.6155, acc 0.9960\n",
            "epoch 1653: train D loss: 0.6868, train F loss: -0.6183, acc 0.9932\n",
            "epoch 1654: train D loss: 0.6815, train F loss: -0.6183, acc 0.9954\n",
            "epoch 1655: train D loss: 0.6823, train F loss: -0.6187, acc 0.9950\n",
            "epoch 1656: train D loss: 0.6839, train F loss: -0.6192, acc 0.9948\n",
            "epoch 1657: train D loss: 0.6857, train F loss: -0.6137, acc 0.9938\n",
            "epoch 1658: train D loss: 0.6811, train F loss: -0.6088, acc 0.9942\n",
            "epoch 1659: train D loss: 0.6818, train F loss: -0.6037, acc 0.9918\n",
            "epoch 1660: train D loss: 0.6820, train F loss: -0.6057, acc 0.9910\n",
            "epoch 1661: train D loss: 0.6826, train F loss: -0.6115, acc 0.9938\n",
            "epoch 1662: train D loss: 0.6838, train F loss: -0.6177, acc 0.9950\n",
            "epoch 1663: train D loss: 0.6799, train F loss: -0.6181, acc 0.9962\n",
            "epoch 1664: train D loss: 0.6833, train F loss: -0.6202, acc 0.9952\n",
            "epoch 1665: train D loss: 0.6818, train F loss: -0.6130, acc 0.9938\n",
            "epoch 1666: train D loss: 0.6823, train F loss: -0.6103, acc 0.9936\n",
            "epoch 1667: train D loss: 0.6808, train F loss: -0.6041, acc 0.9904\n",
            "epoch 1668: train D loss: 0.6827, train F loss: -0.6164, acc 0.9954\n",
            "epoch 1669: train D loss: 0.6806, train F loss: -0.6188, acc 0.9954\n",
            "epoch 1670: train D loss: 0.6856, train F loss: -0.5793, acc 0.9870\n",
            "epoch 1671: train D loss: 0.6818, train F loss: -0.6047, acc 0.9914\n",
            "epoch 1672: train D loss: 0.6814, train F loss: -0.6091, acc 0.9926\n",
            "epoch 1673: train D loss: 0.6788, train F loss: -0.6148, acc 0.9952\n",
            "epoch 1674: train D loss: 0.6803, train F loss: -0.6148, acc 0.9952\n",
            "epoch 1675: train D loss: 0.6792, train F loss: -0.6108, acc 0.9956\n",
            "epoch 1676: train D loss: 0.6794, train F loss: -0.5950, acc 0.9920\n",
            "epoch 1677: train D loss: 0.6776, train F loss: -0.6120, acc 0.9944\n",
            "epoch 1678: train D loss: 0.6788, train F loss: -0.6125, acc 0.9952\n",
            "epoch 1679: train D loss: 0.6822, train F loss: -0.6103, acc 0.9930\n",
            "epoch 1680: train D loss: 0.6821, train F loss: -0.6206, acc 0.9960\n",
            "epoch 1681: train D loss: 0.6821, train F loss: -0.6166, acc 0.9944\n",
            "epoch 1682: train D loss: 0.6818, train F loss: -0.6137, acc 0.9940\n",
            "epoch 1683: train D loss: 0.6846, train F loss: -0.6156, acc 0.9938\n",
            "epoch 1684: train D loss: 0.6817, train F loss: -0.6137, acc 0.9946\n",
            "epoch 1685: train D loss: 0.6835, train F loss: -0.6202, acc 0.9944\n",
            "epoch 1686: train D loss: 0.6856, train F loss: -0.6250, acc 0.9956\n",
            "epoch 1687: train D loss: 0.6842, train F loss: -0.6233, acc 0.9954\n",
            "epoch 1688: train D loss: 0.6847, train F loss: -0.6222, acc 0.9946\n",
            "epoch 1689: train D loss: 0.6821, train F loss: -0.6162, acc 0.9942\n",
            "epoch 1690: train D loss: 0.6835, train F loss: -0.6160, acc 0.9936\n",
            "epoch 1691: train D loss: 0.6829, train F loss: -0.6184, acc 0.9938\n",
            "epoch 1692: train D loss: 0.6820, train F loss: -0.6204, acc 0.9956\n",
            "epoch 1693: train D loss: 0.6844, train F loss: -0.6236, acc 0.9948\n",
            "epoch 1694: train D loss: 0.6847, train F loss: -0.6228, acc 0.9948\n",
            "epoch 1695: train D loss: 0.6834, train F loss: -0.6142, acc 0.9940\n",
            "epoch 1696: train D loss: 0.6808, train F loss: -0.6209, acc 0.9952\n",
            "epoch 1697: train D loss: 0.6830, train F loss: -0.6252, acc 0.9964\n",
            "epoch 1698: train D loss: 0.6861, train F loss: -0.6167, acc 0.9930\n",
            "epoch 1699: train D loss: 0.6808, train F loss: -0.6089, acc 0.9930\n",
            "epoch 1700: train D loss: 0.6835, train F loss: -0.5965, acc 0.9916\n",
            "epoch 1701: train D loss: 0.6811, train F loss: -0.6100, acc 0.9918\n",
            "epoch 1702: train D loss: 0.6831, train F loss: -0.6217, acc 0.9964\n",
            "epoch 1703: train D loss: 0.6827, train F loss: -0.6154, acc 0.9926\n",
            "epoch 1704: train D loss: 0.6797, train F loss: -0.6104, acc 0.9936\n",
            "epoch 1705: train D loss: 0.6889, train F loss: -0.6135, acc 0.9922\n",
            "epoch 1706: train D loss: 0.6829, train F loss: -0.6231, acc 0.9952\n",
            "epoch 1707: train D loss: 0.6814, train F loss: -0.6210, acc 0.9944\n",
            "epoch 1708: train D loss: 0.6809, train F loss: -0.6198, acc 0.9948\n",
            "epoch 1709: train D loss: 0.6796, train F loss: -0.6149, acc 0.9948\n",
            "epoch 1710: train D loss: 0.6814, train F loss: -0.6250, acc 0.9964\n",
            "epoch 1711: train D loss: 0.6825, train F loss: -0.6189, acc 0.9938\n",
            "epoch 1712: train D loss: 0.6854, train F loss: -0.6256, acc 0.9962\n",
            "epoch 1713: train D loss: 0.6821, train F loss: -0.6201, acc 0.9956\n",
            "epoch 1714: train D loss: 0.6837, train F loss: -0.6177, acc 0.9936\n",
            "epoch 1715: train D loss: 0.6857, train F loss: -0.6163, acc 0.9922\n",
            "epoch 1716: train D loss: 0.6846, train F loss: -0.6176, acc 0.9936\n",
            "epoch 1717: train D loss: 0.6803, train F loss: -0.6161, acc 0.9938\n",
            "epoch 1718: train D loss: 0.6814, train F loss: -0.6176, acc 0.9934\n",
            "epoch 1719: train D loss: 0.6850, train F loss: -0.6197, acc 0.9934\n",
            "epoch 1720: train D loss: 0.6816, train F loss: -0.6244, acc 0.9960\n",
            "epoch 1721: train D loss: 0.6816, train F loss: -0.6197, acc 0.9944\n",
            "epoch 1722: train D loss: 0.6801, train F loss: -0.6209, acc 0.9952\n",
            "epoch 1723: train D loss: 0.6857, train F loss: -0.6104, acc 0.9924\n",
            "epoch 1724: train D loss: 0.6852, train F loss: -0.6260, acc 0.9948\n",
            "epoch 1725: train D loss: 0.6778, train F loss: -0.6201, acc 0.9964\n",
            "epoch 1726: train D loss: 0.6826, train F loss: -0.6213, acc 0.9950\n",
            "epoch 1727: train D loss: 0.6802, train F loss: -0.6218, acc 0.9956\n",
            "epoch 1728: train D loss: 0.6837, train F loss: -0.6252, acc 0.9954\n",
            "epoch 1729: train D loss: 0.6828, train F loss: -0.6138, acc 0.9928\n",
            "epoch 1730: train D loss: 0.6791, train F loss: -0.6175, acc 0.9934\n",
            "epoch 1731: train D loss: 0.6834, train F loss: -0.6193, acc 0.9944\n",
            "epoch 1732: train D loss: 0.6832, train F loss: -0.5916, acc 0.9882\n",
            "epoch 1733: train D loss: 0.6797, train F loss: -0.6117, acc 0.9922\n",
            "epoch 1734: train D loss: 0.6815, train F loss: -0.6226, acc 0.9942\n",
            "epoch 1735: train D loss: 0.6811, train F loss: -0.6149, acc 0.9934\n",
            "epoch 1736: train D loss: 0.6869, train F loss: -0.6231, acc 0.9940\n",
            "epoch 1737: train D loss: 0.6824, train F loss: -0.6230, acc 0.9948\n",
            "epoch 1738: train D loss: 0.6831, train F loss: -0.6231, acc 0.9946\n",
            "epoch 1739: train D loss: 0.6805, train F loss: -0.6218, acc 0.9956\n",
            "epoch 1740: train D loss: 0.6816, train F loss: -0.6249, acc 0.9958\n",
            "epoch 1741: train D loss: 0.6866, train F loss: -0.6268, acc 0.9942\n",
            "epoch 1742: train D loss: 0.6801, train F loss: -0.6182, acc 0.9952\n",
            "epoch 1743: train D loss: 0.6829, train F loss: -0.6237, acc 0.9938\n",
            "epoch 1744: train D loss: 0.6824, train F loss: -0.6190, acc 0.9940\n",
            "epoch 1745: train D loss: 0.6803, train F loss: -0.6159, acc 0.9938\n",
            "epoch 1746: train D loss: 0.6837, train F loss: -0.6265, acc 0.9954\n",
            "epoch 1747: train D loss: 0.6815, train F loss: -0.6210, acc 0.9936\n",
            "epoch 1748: train D loss: 0.6809, train F loss: -0.6228, acc 0.9950\n",
            "epoch 1749: train D loss: 0.6875, train F loss: -0.6294, acc 0.9948\n",
            "epoch 1750: train D loss: 0.6847, train F loss: -0.6281, acc 0.9946\n",
            "epoch 1751: train D loss: 0.6834, train F loss: -0.6237, acc 0.9942\n",
            "epoch 1752: train D loss: 0.6821, train F loss: -0.6259, acc 0.9946\n",
            "epoch 1753: train D loss: 0.6820, train F loss: -0.6175, acc 0.9938\n",
            "epoch 1754: train D loss: 0.6843, train F loss: -0.6246, acc 0.9938\n",
            "epoch 1755: train D loss: 0.6841, train F loss: -0.6283, acc 0.9958\n",
            "epoch 1756: train D loss: 0.6811, train F loss: -0.6205, acc 0.9938\n",
            "epoch 1757: train D loss: 0.6837, train F loss: -0.6236, acc 0.9934\n",
            "epoch 1758: train D loss: 0.6824, train F loss: -0.6229, acc 0.9948\n",
            "epoch 1759: train D loss: 0.6832, train F loss: -0.6198, acc 0.9934\n",
            "epoch 1760: train D loss: 0.6852, train F loss: -0.6249, acc 0.9950\n",
            "epoch 1761: train D loss: 0.6828, train F loss: -0.6065, acc 0.9882\n",
            "epoch 1762: train D loss: 0.6792, train F loss: -0.6232, acc 0.9956\n",
            "epoch 1763: train D loss: 0.6817, train F loss: -0.6225, acc 0.9938\n",
            "epoch 1764: train D loss: 0.6845, train F loss: -0.6200, acc 0.9928\n",
            "epoch 1765: train D loss: 0.6837, train F loss: -0.6297, acc 0.9954\n",
            "epoch 1766: train D loss: 0.6815, train F loss: -0.6286, acc 0.9958\n",
            "epoch 1767: train D loss: 0.6812, train F loss: -0.6269, acc 0.9962\n",
            "epoch 1768: train D loss: 0.6820, train F loss: -0.6303, acc 0.9970\n",
            "epoch 1769: train D loss: 0.6824, train F loss: -0.6193, acc 0.9944\n",
            "epoch 1770: train D loss: 0.6810, train F loss: -0.6202, acc 0.9930\n",
            "epoch 1771: train D loss: 0.6840, train F loss: -0.6294, acc 0.9962\n",
            "epoch 1772: train D loss: 0.6822, train F loss: -0.6223, acc 0.9948\n",
            "epoch 1773: train D loss: 0.6837, train F loss: -0.6216, acc 0.9932\n",
            "epoch 1774: train D loss: 0.6812, train F loss: -0.6297, acc 0.9958\n",
            "epoch 1775: train D loss: 0.6856, train F loss: -0.6268, acc 0.9934\n",
            "epoch 1776: train D loss: 0.6842, train F loss: -0.6214, acc 0.9936\n",
            "epoch 1777: train D loss: 0.6847, train F loss: -0.6302, acc 0.9954\n",
            "epoch 1778: train D loss: 0.6838, train F loss: -0.6291, acc 0.9952\n",
            "epoch 1779: train D loss: 0.6836, train F loss: -0.6267, acc 0.9944\n",
            "epoch 1780: train D loss: 0.6820, train F loss: -0.6253, acc 0.9944\n",
            "epoch 1781: train D loss: 0.6859, train F loss: -0.6299, acc 0.9948\n",
            "epoch 1782: train D loss: 0.6852, train F loss: -0.6344, acc 0.9960\n",
            "epoch 1783: train D loss: 0.6836, train F loss: -0.6284, acc 0.9948\n",
            "epoch 1784: train D loss: 0.6844, train F loss: -0.6229, acc 0.9928\n",
            "epoch 1785: train D loss: 0.6818, train F loss: -0.6259, acc 0.9952\n",
            "epoch 1786: train D loss: 0.6842, train F loss: -0.6135, acc 0.9910\n",
            "epoch 1787: train D loss: 0.6821, train F loss: -0.6280, acc 0.9962\n",
            "epoch 1788: train D loss: 0.6844, train F loss: -0.6240, acc 0.9940\n",
            "epoch 1789: train D loss: 0.6831, train F loss: -0.6168, acc 0.9926\n",
            "epoch 1790: train D loss: 0.6795, train F loss: -0.6184, acc 0.9934\n",
            "epoch 1791: train D loss: 0.6836, train F loss: -0.6121, acc 0.9932\n",
            "epoch 1792: train D loss: 0.6840, train F loss: -0.6304, acc 0.9958\n",
            "epoch 1793: train D loss: 0.6824, train F loss: -0.6243, acc 0.9940\n",
            "epoch 1794: train D loss: 0.6870, train F loss: -0.6337, acc 0.9952\n",
            "epoch 1795: train D loss: 0.6821, train F loss: -0.6337, acc 0.9968\n",
            "epoch 1796: train D loss: 0.6840, train F loss: -0.6248, acc 0.9944\n",
            "epoch 1797: train D loss: 0.6842, train F loss: -0.6273, acc 0.9942\n",
            "epoch 1798: train D loss: 0.6852, train F loss: -0.6334, acc 0.9954\n",
            "epoch 1799: train D loss: 0.6823, train F loss: -0.6296, acc 0.9960\n",
            "epoch 1800: train D loss: 0.6800, train F loss: -0.6184, acc 0.9942\n",
            "epoch 1801: train D loss: 0.6835, train F loss: -0.6279, acc 0.9946\n",
            "epoch 1802: train D loss: 0.6821, train F loss: -0.6282, acc 0.9950\n",
            "epoch 1803: train D loss: 0.6818, train F loss: -0.6242, acc 0.9936\n",
            "epoch 1804: train D loss: 0.6818, train F loss: -0.6304, acc 0.9960\n",
            "epoch 1805: train D loss: 0.6807, train F loss: -0.6258, acc 0.9938\n",
            "epoch 1806: train D loss: 0.6885, train F loss: -0.6335, acc 0.9942\n",
            "epoch 1807: train D loss: 0.6837, train F loss: -0.6218, acc 0.9918\n",
            "epoch 1808: train D loss: 0.6827, train F loss: -0.6166, acc 0.9916\n",
            "epoch 1809: train D loss: 0.6856, train F loss: -0.6250, acc 0.9932\n",
            "epoch 1810: train D loss: 0.6847, train F loss: -0.6293, acc 0.9954\n",
            "epoch 1811: train D loss: 0.6813, train F loss: -0.6296, acc 0.9956\n",
            "epoch 1812: train D loss: 0.6842, train F loss: -0.6235, acc 0.9942\n",
            "epoch 1813: train D loss: 0.6841, train F loss: -0.6220, acc 0.9926\n",
            "epoch 1814: train D loss: 0.6827, train F loss: -0.6314, acc 0.9958\n",
            "epoch 1815: train D loss: 0.6825, train F loss: -0.6350, acc 0.9970\n",
            "epoch 1816: train D loss: 0.6835, train F loss: -0.6109, acc 0.9924\n",
            "epoch 1817: train D loss: 0.6831, train F loss: -0.6274, acc 0.9942\n",
            "epoch 1818: train D loss: 0.6817, train F loss: -0.6285, acc 0.9948\n",
            "epoch 1819: train D loss: 0.6846, train F loss: -0.6326, acc 0.9944\n",
            "epoch 1820: train D loss: 0.6799, train F loss: -0.6325, acc 0.9962\n",
            "epoch 1821: train D loss: 0.6811, train F loss: -0.6209, acc 0.9936\n",
            "epoch 1822: train D loss: 0.6857, train F loss: -0.6071, acc 0.9904\n",
            "epoch 1823: train D loss: 0.6821, train F loss: -0.6166, acc 0.9936\n",
            "epoch 1824: train D loss: 0.6849, train F loss: -0.6297, acc 0.9946\n",
            "epoch 1825: train D loss: 0.6822, train F loss: -0.6330, acc 0.9958\n",
            "epoch 1826: train D loss: 0.6836, train F loss: -0.6284, acc 0.9954\n",
            "epoch 1827: train D loss: 0.6824, train F loss: -0.6253, acc 0.9946\n",
            "epoch 1828: train D loss: 0.6801, train F loss: -0.6130, acc 0.9934\n",
            "epoch 1829: train D loss: 0.6838, train F loss: -0.6234, acc 0.9924\n",
            "epoch 1830: train D loss: 0.6789, train F loss: -0.6259, acc 0.9946\n",
            "epoch 1831: train D loss: 0.6834, train F loss: -0.6235, acc 0.9932\n",
            "epoch 1832: train D loss: 0.6806, train F loss: -0.6125, acc 0.9928\n",
            "epoch 1833: train D loss: 0.6855, train F loss: -0.6323, acc 0.9942\n",
            "epoch 1834: train D loss: 0.6822, train F loss: -0.6333, acc 0.9954\n",
            "epoch 1835: train D loss: 0.6838, train F loss: -0.6347, acc 0.9964\n",
            "epoch 1836: train D loss: 0.6813, train F loss: -0.6114, acc 0.9906\n",
            "epoch 1837: train D loss: 0.6845, train F loss: -0.6327, acc 0.9950\n",
            "epoch 1838: train D loss: 0.6836, train F loss: -0.6374, acc 0.9972\n",
            "epoch 1839: train D loss: 0.6845, train F loss: -0.6342, acc 0.9952\n",
            "epoch 1840: train D loss: 0.6830, train F loss: -0.6187, acc 0.9920\n",
            "epoch 1841: train D loss: 0.6851, train F loss: -0.6262, acc 0.9936\n",
            "epoch 1842: train D loss: 0.6831, train F loss: -0.6283, acc 0.9930\n",
            "epoch 1843: train D loss: 0.6814, train F loss: -0.6253, acc 0.9932\n",
            "epoch 1844: train D loss: 0.6831, train F loss: -0.6319, acc 0.9952\n",
            "epoch 1845: train D loss: 0.6776, train F loss: -0.6237, acc 0.9948\n",
            "epoch 1846: train D loss: 0.6836, train F loss: -0.6374, acc 0.9962\n",
            "epoch 1847: train D loss: 0.6842, train F loss: -0.6329, acc 0.9948\n",
            "epoch 1848: train D loss: 0.6825, train F loss: -0.6327, acc 0.9954\n",
            "epoch 1849: train D loss: 0.6841, train F loss: -0.6348, acc 0.9958\n",
            "epoch 1850: train D loss: 0.6796, train F loss: -0.6281, acc 0.9950\n",
            "epoch 1851: train D loss: 0.6875, train F loss: -0.6352, acc 0.9948\n",
            "epoch 1852: train D loss: 0.6832, train F loss: -0.6362, acc 0.9952\n",
            "epoch 1853: train D loss: 0.6822, train F loss: -0.6300, acc 0.9950\n",
            "epoch 1854: train D loss: 0.6806, train F loss: -0.6356, acc 0.9966\n",
            "epoch 1855: train D loss: 0.6852, train F loss: -0.6364, acc 0.9954\n",
            "epoch 1856: train D loss: 0.6797, train F loss: -0.6293, acc 0.9952\n",
            "epoch 1857: train D loss: 0.6837, train F loss: -0.6262, acc 0.9936\n",
            "epoch 1858: train D loss: 0.6833, train F loss: -0.6337, acc 0.9948\n",
            "epoch 1859: train D loss: 0.6840, train F loss: -0.6328, acc 0.9952\n",
            "epoch 1860: train D loss: 0.6827, train F loss: -0.6074, acc 0.9928\n",
            "epoch 1861: train D loss: 0.6836, train F loss: -0.6296, acc 0.9944\n",
            "epoch 1862: train D loss: 0.6849, train F loss: -0.6329, acc 0.9944\n",
            "epoch 1863: train D loss: 0.6861, train F loss: -0.6405, acc 0.9960\n",
            "epoch 1864: train D loss: 0.6844, train F loss: -0.6343, acc 0.9954\n",
            "epoch 1865: train D loss: 0.6841, train F loss: -0.6330, acc 0.9958\n",
            "epoch 1866: train D loss: 0.6833, train F loss: -0.6251, acc 0.9936\n",
            "epoch 1867: train D loss: 0.6841, train F loss: -0.6286, acc 0.9952\n",
            "epoch 1868: train D loss: 0.6861, train F loss: -0.6293, acc 0.9926\n",
            "epoch 1869: train D loss: 0.6821, train F loss: -0.6272, acc 0.9936\n",
            "epoch 1870: train D loss: 0.6853, train F loss: -0.6205, acc 0.9930\n",
            "epoch 1871: train D loss: 0.6808, train F loss: -0.6283, acc 0.9948\n",
            "epoch 1872: train D loss: 0.6828, train F loss: -0.6335, acc 0.9956\n",
            "epoch 1873: train D loss: 0.6845, train F loss: -0.6324, acc 0.9944\n",
            "epoch 1874: train D loss: 0.6843, train F loss: -0.6349, acc 0.9944\n",
            "epoch 1875: train D loss: 0.6823, train F loss: -0.6344, acc 0.9954\n",
            "epoch 1876: train D loss: 0.6818, train F loss: -0.6348, acc 0.9956\n",
            "epoch 1877: train D loss: 0.6826, train F loss: -0.6180, acc 0.9920\n",
            "epoch 1878: train D loss: 0.6847, train F loss: -0.6315, acc 0.9932\n",
            "epoch 1879: train D loss: 0.6849, train F loss: -0.6417, acc 0.9970\n",
            "epoch 1880: train D loss: 0.6828, train F loss: -0.6352, acc 0.9956\n",
            "epoch 1881: train D loss: 0.6848, train F loss: -0.6305, acc 0.9930\n",
            "epoch 1882: train D loss: 0.6786, train F loss: -0.6166, acc 0.9920\n",
            "epoch 1883: train D loss: 0.6852, train F loss: -0.6345, acc 0.9942\n",
            "epoch 1884: train D loss: 0.6869, train F loss: -0.6160, acc 0.9894\n",
            "epoch 1885: train D loss: 0.6824, train F loss: -0.6301, acc 0.9946\n",
            "epoch 1886: train D loss: 0.6803, train F loss: -0.6274, acc 0.9934\n",
            "epoch 1887: train D loss: 0.6846, train F loss: -0.6363, acc 0.9958\n",
            "epoch 1888: train D loss: 0.6807, train F loss: -0.6204, acc 0.9918\n",
            "epoch 1889: train D loss: 0.6820, train F loss: -0.6339, acc 0.9956\n",
            "epoch 1890: train D loss: 0.6802, train F loss: -0.6363, acc 0.9966\n",
            "epoch 1891: train D loss: 0.6819, train F loss: -0.6360, acc 0.9964\n",
            "epoch 1892: train D loss: 0.6802, train F loss: -0.6226, acc 0.9930\n",
            "epoch 1893: train D loss: 0.6854, train F loss: -0.6291, acc 0.9928\n",
            "epoch 1894: train D loss: 0.6797, train F loss: -0.6308, acc 0.9946\n",
            "epoch 1895: train D loss: 0.6820, train F loss: -0.6382, acc 0.9960\n",
            "epoch 1896: train D loss: 0.6831, train F loss: -0.6392, acc 0.9956\n",
            "epoch 1897: train D loss: 0.6813, train F loss: -0.6338, acc 0.9948\n",
            "epoch 1898: train D loss: 0.6776, train F loss: -0.6282, acc 0.9946\n",
            "epoch 1899: train D loss: 0.6868, train F loss: -0.6369, acc 0.9942\n",
            "epoch 1900: train D loss: 0.6852, train F loss: -0.6257, acc 0.9922\n",
            "epoch 1901: train D loss: 0.6831, train F loss: -0.6363, acc 0.9952\n",
            "epoch 1902: train D loss: 0.6850, train F loss: -0.6279, acc 0.9928\n",
            "epoch 1903: train D loss: 0.6825, train F loss: -0.6209, acc 0.9942\n",
            "epoch 1904: train D loss: 0.6821, train F loss: -0.6233, acc 0.9932\n",
            "epoch 1905: train D loss: 0.6813, train F loss: -0.6325, acc 0.9946\n",
            "epoch 1906: train D loss: 0.6833, train F loss: -0.6366, acc 0.9960\n",
            "epoch 1907: train D loss: 0.6823, train F loss: -0.6336, acc 0.9954\n",
            "epoch 1908: train D loss: 0.6796, train F loss: -0.6292, acc 0.9954\n",
            "epoch 1909: train D loss: 0.6820, train F loss: -0.6313, acc 0.9956\n",
            "epoch 1910: train D loss: 0.6825, train F loss: -0.6345, acc 0.9954\n",
            "epoch 1911: train D loss: 0.6837, train F loss: -0.6343, acc 0.9940\n",
            "epoch 1912: train D loss: 0.6787, train F loss: -0.6289, acc 0.9952\n",
            "epoch 1913: train D loss: 0.6865, train F loss: -0.6383, acc 0.9944\n",
            "epoch 1914: train D loss: 0.6834, train F loss: -0.6373, acc 0.9950\n",
            "epoch 1915: train D loss: 0.6824, train F loss: -0.6272, acc 0.9932\n",
            "epoch 1916: train D loss: 0.6816, train F loss: -0.6369, acc 0.9960\n",
            "epoch 1917: train D loss: 0.6823, train F loss: -0.6349, acc 0.9958\n",
            "epoch 1918: train D loss: 0.6850, train F loss: -0.6336, acc 0.9944\n",
            "epoch 1919: train D loss: 0.6839, train F loss: -0.6370, acc 0.9956\n",
            "epoch 1920: train D loss: 0.6867, train F loss: -0.6340, acc 0.9932\n",
            "epoch 1921: train D loss: 0.6821, train F loss: -0.6287, acc 0.9932\n",
            "epoch 1922: train D loss: 0.6864, train F loss: -0.6432, acc 0.9962\n",
            "epoch 1923: train D loss: 0.6833, train F loss: -0.6401, acc 0.9966\n",
            "epoch 1924: train D loss: 0.6806, train F loss: -0.6336, acc 0.9944\n",
            "epoch 1925: train D loss: 0.6837, train F loss: -0.6391, acc 0.9958\n",
            "epoch 1926: train D loss: 0.6817, train F loss: -0.6376, acc 0.9958\n",
            "epoch 1927: train D loss: 0.6830, train F loss: -0.6355, acc 0.9952\n",
            "epoch 1928: train D loss: 0.6828, train F loss: -0.6302, acc 0.9938\n",
            "epoch 1929: train D loss: 0.6812, train F loss: -0.6330, acc 0.9944\n",
            "epoch 1930: train D loss: 0.6835, train F loss: -0.6340, acc 0.9934\n",
            "epoch 1931: train D loss: 0.6815, train F loss: -0.6336, acc 0.9950\n",
            "epoch 1932: train D loss: 0.6815, train F loss: -0.6291, acc 0.9942\n",
            "epoch 1933: train D loss: 0.6855, train F loss: -0.6366, acc 0.9942\n",
            "epoch 1934: train D loss: 0.6864, train F loss: -0.6308, acc 0.9928\n",
            "epoch 1935: train D loss: 0.6838, train F loss: -0.6374, acc 0.9948\n",
            "epoch 1936: train D loss: 0.6831, train F loss: -0.6368, acc 0.9950\n",
            "epoch 1937: train D loss: 0.6811, train F loss: -0.6335, acc 0.9928\n",
            "epoch 1938: train D loss: 0.6868, train F loss: -0.6390, acc 0.9940\n",
            "epoch 1939: train D loss: 0.6853, train F loss: -0.6387, acc 0.9956\n",
            "epoch 1940: train D loss: 0.6836, train F loss: -0.6351, acc 0.9936\n",
            "epoch 1941: train D loss: 0.6809, train F loss: -0.6368, acc 0.9950\n",
            "epoch 1942: train D loss: 0.6825, train F loss: -0.6358, acc 0.9948\n",
            "epoch 1943: train D loss: 0.6821, train F loss: -0.6331, acc 0.9946\n",
            "epoch 1944: train D loss: 0.6840, train F loss: -0.6384, acc 0.9952\n",
            "epoch 1945: train D loss: 0.6811, train F loss: -0.6327, acc 0.9938\n",
            "epoch 1946: train D loss: 0.6825, train F loss: -0.6395, acc 0.9942\n",
            "epoch 1947: train D loss: 0.6848, train F loss: -0.6287, acc 0.9932\n",
            "epoch 1948: train D loss: 0.6853, train F loss: -0.6404, acc 0.9934\n",
            "epoch 1949: train D loss: 0.6841, train F loss: -0.6343, acc 0.9940\n",
            "epoch 1950: train D loss: 0.6828, train F loss: -0.6345, acc 0.9924\n",
            "epoch 1951: train D loss: 0.6808, train F loss: -0.6353, acc 0.9942\n",
            "epoch 1952: train D loss: 0.6833, train F loss: -0.6227, acc 0.9922\n",
            "epoch 1953: train D loss: 0.6819, train F loss: -0.6351, acc 0.9940\n",
            "epoch 1954: train D loss: 0.6838, train F loss: -0.5983, acc 0.9908\n",
            "epoch 1955: train D loss: 0.6852, train F loss: -0.6417, acc 0.9958\n",
            "epoch 1956: train D loss: 0.6816, train F loss: -0.6397, acc 0.9952\n",
            "epoch 1957: train D loss: 0.6827, train F loss: -0.6386, acc 0.9948\n",
            "epoch 1958: train D loss: 0.6846, train F loss: -0.6369, acc 0.9942\n",
            "epoch 1959: train D loss: 0.6807, train F loss: -0.6360, acc 0.9944\n",
            "epoch 1960: train D loss: 0.6845, train F loss: -0.6404, acc 0.9948\n",
            "epoch 1961: train D loss: 0.6788, train F loss: -0.6231, acc 0.9932\n",
            "epoch 1962: train D loss: 0.6845, train F loss: -0.6366, acc 0.9948\n",
            "epoch 1963: train D loss: 0.6829, train F loss: -0.6408, acc 0.9952\n",
            "epoch 1964: train D loss: 0.6830, train F loss: -0.6397, acc 0.9950\n",
            "epoch 1965: train D loss: 0.6833, train F loss: -0.6438, acc 0.9958\n",
            "epoch 1966: train D loss: 0.6845, train F loss: -0.6402, acc 0.9958\n",
            "epoch 1967: train D loss: 0.6846, train F loss: -0.6306, acc 0.9944\n",
            "epoch 1968: train D loss: 0.6830, train F loss: -0.6315, acc 0.9934\n",
            "epoch 1969: train D loss: 0.6819, train F loss: -0.6251, acc 0.9946\n",
            "epoch 1970: train D loss: 0.6825, train F loss: -0.6236, acc 0.9914\n",
            "epoch 1971: train D loss: 0.6851, train F loss: -0.6380, acc 0.9934\n",
            "epoch 1972: train D loss: 0.6820, train F loss: -0.6376, acc 0.9946\n",
            "epoch 1973: train D loss: 0.6842, train F loss: -0.6396, acc 0.9946\n",
            "epoch 1974: train D loss: 0.6837, train F loss: -0.6384, acc 0.9934\n",
            "epoch 1975: train D loss: 0.6850, train F loss: -0.6424, acc 0.9958\n",
            "epoch 1976: train D loss: 0.6863, train F loss: -0.6364, acc 0.9944\n",
            "epoch 1977: train D loss: 0.6842, train F loss: -0.6350, acc 0.9928\n",
            "epoch 1978: train D loss: 0.6816, train F loss: -0.6416, acc 0.9950\n",
            "epoch 1979: train D loss: 0.6833, train F loss: -0.6440, acc 0.9958\n",
            "epoch 1980: train D loss: 0.6827, train F loss: -0.6391, acc 0.9956\n",
            "epoch 1981: train D loss: 0.6754, train F loss: -0.6325, acc 0.9950\n",
            "epoch 1982: train D loss: 0.6831, train F loss: -0.6375, acc 0.9948\n",
            "epoch 1983: train D loss: 0.6795, train F loss: -0.6406, acc 0.9960\n",
            "epoch 1984: train D loss: 0.6852, train F loss: -0.6420, acc 0.9956\n",
            "epoch 1985: train D loss: 0.6870, train F loss: -0.6492, acc 0.9964\n",
            "epoch 1986: train D loss: 0.6855, train F loss: -0.6381, acc 0.9936\n",
            "epoch 1987: train D loss: 0.6851, train F loss: -0.6203, acc 0.9894\n",
            "epoch 1988: train D loss: 0.6814, train F loss: -0.6346, acc 0.9946\n",
            "epoch 1989: train D loss: 0.6782, train F loss: -0.6341, acc 0.9946\n",
            "epoch 1990: train D loss: 0.6844, train F loss: -0.6393, acc 0.9950\n",
            "epoch 1991: train D loss: 0.6825, train F loss: -0.6434, acc 0.9960\n",
            "epoch 1992: train D loss: 0.6815, train F loss: -0.6401, acc 0.9962\n",
            "epoch 1993: train D loss: 0.6853, train F loss: -0.6463, acc 0.9958\n",
            "epoch 1994: train D loss: 0.6838, train F loss: -0.6395, acc 0.9944\n",
            "epoch 1995: train D loss: 0.6862, train F loss: -0.6364, acc 0.9936\n",
            "epoch 1996: train D loss: 0.6821, train F loss: -0.6388, acc 0.9950\n",
            "epoch 1997: train D loss: 0.6827, train F loss: -0.6344, acc 0.9960\n",
            "epoch 1998: train D loss: 0.6817, train F loss: -0.6390, acc 0.9958\n",
            "epoch 1999: train D loss: 0.6845, train F loss: -0.6400, acc 0.9940\n",
            "epoch 2000: train D loss: 0.6793, train F loss: -0.6387, acc 0.9948\n",
            "epoch 2001: train D loss: 0.6861, train F loss: -0.6440, acc 0.9952\n",
            "epoch 2002: train D loss: 0.6865, train F loss: -0.6384, acc 0.9934\n",
            "epoch 2003: train D loss: 0.6852, train F loss: -0.6404, acc 0.9948\n",
            "epoch 2004: train D loss: 0.6861, train F loss: -0.6463, acc 0.9958\n",
            "epoch 2005: train D loss: 0.6841, train F loss: -0.6362, acc 0.9938\n",
            "epoch 2006: train D loss: 0.6820, train F loss: -0.6418, acc 0.9956\n",
            "epoch 2007: train D loss: 0.6824, train F loss: -0.6426, acc 0.9958\n",
            "epoch 2008: train D loss: 0.6801, train F loss: -0.6382, acc 0.9952\n",
            "epoch 2009: train D loss: 0.6805, train F loss: -0.6302, acc 0.9924\n",
            "epoch 2010: train D loss: 0.6832, train F loss: -0.6408, acc 0.9940\n",
            "epoch 2011: train D loss: 0.6816, train F loss: -0.6393, acc 0.9954\n",
            "epoch 2012: train D loss: 0.6835, train F loss: -0.6390, acc 0.9942\n",
            "epoch 2013: train D loss: 0.6862, train F loss: -0.6312, acc 0.9920\n",
            "epoch 2014: train D loss: 0.6838, train F loss: -0.6421, acc 0.9944\n",
            "epoch 2015: train D loss: 0.6847, train F loss: -0.6464, acc 0.9948\n",
            "epoch 2016: train D loss: 0.6785, train F loss: -0.6370, acc 0.9944\n",
            "epoch 2017: train D loss: 0.6817, train F loss: -0.6424, acc 0.9952\n",
            "epoch 2018: train D loss: 0.6849, train F loss: -0.6379, acc 0.9944\n",
            "epoch 2019: train D loss: 0.6795, train F loss: -0.6352, acc 0.9952\n",
            "epoch 2020: train D loss: 0.6827, train F loss: -0.6335, acc 0.9936\n",
            "epoch 2021: train D loss: 0.6843, train F loss: -0.6422, acc 0.9952\n",
            "epoch 2022: train D loss: 0.6787, train F loss: -0.6421, acc 0.9968\n",
            "epoch 2023: train D loss: 0.6840, train F loss: -0.6468, acc 0.9956\n",
            "epoch 2024: train D loss: 0.6859, train F loss: -0.6406, acc 0.9946\n",
            "epoch 2025: train D loss: 0.6839, train F loss: -0.6415, acc 0.9938\n",
            "epoch 2026: train D loss: 0.6831, train F loss: -0.6396, acc 0.9950\n",
            "epoch 2027: train D loss: 0.6829, train F loss: -0.6368, acc 0.9940\n",
            "epoch 2028: train D loss: 0.6833, train F loss: -0.6465, acc 0.9968\n",
            "epoch 2029: train D loss: 0.6865, train F loss: -0.6355, acc 0.9926\n",
            "epoch 2030: train D loss: 0.6812, train F loss: -0.6390, acc 0.9938\n",
            "epoch 2031: train D loss: 0.6829, train F loss: -0.6458, acc 0.9960\n",
            "epoch 2032: train D loss: 0.6825, train F loss: -0.6404, acc 0.9954\n",
            "epoch 2033: train D loss: 0.6851, train F loss: -0.6429, acc 0.9938\n",
            "epoch 2034: train D loss: 0.6829, train F loss: -0.6366, acc 0.9948\n",
            "epoch 2035: train D loss: 0.6843, train F loss: -0.6398, acc 0.9952\n",
            "epoch 2036: train D loss: 0.6832, train F loss: -0.6259, acc 0.9908\n",
            "epoch 2037: train D loss: 0.6839, train F loss: -0.6398, acc 0.9946\n",
            "epoch 2038: train D loss: 0.6838, train F loss: -0.6418, acc 0.9952\n",
            "epoch 2039: train D loss: 0.6838, train F loss: -0.6386, acc 0.9946\n",
            "epoch 2040: train D loss: 0.6826, train F loss: -0.6432, acc 0.9950\n",
            "epoch 2041: train D loss: 0.6879, train F loss: -0.6108, acc 0.9890\n",
            "epoch 2042: train D loss: 0.6829, train F loss: -0.6350, acc 0.9928\n",
            "epoch 2043: train D loss: 0.6817, train F loss: -0.6417, acc 0.9952\n",
            "epoch 2044: train D loss: 0.6813, train F loss: -0.6299, acc 0.9930\n",
            "epoch 2045: train D loss: 0.6824, train F loss: -0.6422, acc 0.9954\n",
            "epoch 2046: train D loss: 0.6861, train F loss: -0.6421, acc 0.9936\n",
            "epoch 2047: train D loss: 0.6834, train F loss: -0.6409, acc 0.9936\n",
            "epoch 2048: train D loss: 0.6874, train F loss: -0.6481, acc 0.9956\n",
            "epoch 2049: train D loss: 0.6838, train F loss: -0.6399, acc 0.9944\n",
            "epoch 2050: train D loss: 0.6797, train F loss: -0.6424, acc 0.9954\n",
            "epoch 2051: train D loss: 0.6853, train F loss: -0.6403, acc 0.9950\n",
            "epoch 2052: train D loss: 0.6849, train F loss: -0.6253, acc 0.9904\n",
            "epoch 2053: train D loss: 0.6838, train F loss: -0.6387, acc 0.9924\n",
            "epoch 2054: train D loss: 0.6790, train F loss: -0.6333, acc 0.9928\n",
            "epoch 2055: train D loss: 0.6846, train F loss: -0.6375, acc 0.9930\n",
            "epoch 2056: train D loss: 0.6818, train F loss: -0.6445, acc 0.9964\n",
            "epoch 2057: train D loss: 0.6836, train F loss: -0.6411, acc 0.9952\n",
            "epoch 2058: train D loss: 0.6833, train F loss: -0.6480, acc 0.9962\n",
            "epoch 2059: train D loss: 0.6835, train F loss: -0.6347, acc 0.9930\n",
            "epoch 2060: train D loss: 0.6861, train F loss: -0.6338, acc 0.9914\n",
            "epoch 2061: train D loss: 0.6810, train F loss: -0.6379, acc 0.9948\n",
            "epoch 2062: train D loss: 0.6825, train F loss: -0.6390, acc 0.9952\n",
            "epoch 2063: train D loss: 0.6821, train F loss: -0.6443, acc 0.9956\n",
            "epoch 2064: train D loss: 0.6857, train F loss: -0.6482, acc 0.9956\n",
            "epoch 2065: train D loss: 0.6862, train F loss: -0.6507, acc 0.9962\n",
            "epoch 2066: train D loss: 0.6835, train F loss: -0.6421, acc 0.9944\n",
            "epoch 2067: train D loss: 0.6895, train F loss: -0.6521, acc 0.9958\n",
            "epoch 2068: train D loss: 0.6832, train F loss: -0.6246, acc 0.9954\n",
            "epoch 2069: train D loss: 0.6855, train F loss: -0.6438, acc 0.9930\n",
            "epoch 2070: train D loss: 0.6851, train F loss: -0.6423, acc 0.9942\n",
            "epoch 2071: train D loss: 0.6846, train F loss: -0.6466, acc 0.9946\n",
            "epoch 2072: train D loss: 0.6808, train F loss: -0.6419, acc 0.9946\n",
            "epoch 2073: train D loss: 0.6836, train F loss: -0.6235, acc 0.9928\n",
            "epoch 2074: train D loss: 0.6851, train F loss: -0.6203, acc 0.9906\n",
            "epoch 2075: train D loss: 0.6820, train F loss: -0.6436, acc 0.9948\n",
            "epoch 2076: train D loss: 0.6842, train F loss: -0.6449, acc 0.9946\n",
            "epoch 2077: train D loss: 0.6826, train F loss: -0.6405, acc 0.9940\n",
            "epoch 2078: train D loss: 0.6817, train F loss: -0.6452, acc 0.9946\n",
            "epoch 2079: train D loss: 0.6832, train F loss: -0.6469, acc 0.9964\n",
            "epoch 2080: train D loss: 0.6845, train F loss: -0.6427, acc 0.9942\n",
            "epoch 2081: train D loss: 0.6833, train F loss: -0.6381, acc 0.9938\n",
            "epoch 2082: train D loss: 0.6833, train F loss: -0.6431, acc 0.9952\n",
            "epoch 2083: train D loss: 0.6830, train F loss: -0.6493, acc 0.9968\n",
            "epoch 2084: train D loss: 0.6844, train F loss: -0.6433, acc 0.9942\n",
            "epoch 2085: train D loss: 0.6878, train F loss: -0.6480, acc 0.9950\n",
            "epoch 2086: train D loss: 0.6859, train F loss: -0.6364, acc 0.9922\n",
            "epoch 2087: train D loss: 0.6853, train F loss: -0.6473, acc 0.9956\n",
            "epoch 2088: train D loss: 0.6810, train F loss: -0.6421, acc 0.9950\n",
            "epoch 2089: train D loss: 0.6831, train F loss: -0.6315, acc 0.9932\n",
            "epoch 2090: train D loss: 0.6858, train F loss: -0.6384, acc 0.9942\n",
            "epoch 2091: train D loss: 0.6860, train F loss: -0.6468, acc 0.9946\n",
            "epoch 2092: train D loss: 0.6816, train F loss: -0.6462, acc 0.9948\n",
            "epoch 2093: train D loss: 0.6819, train F loss: -0.6476, acc 0.9966\n",
            "epoch 2094: train D loss: 0.6863, train F loss: -0.6526, acc 0.9960\n",
            "epoch 2095: train D loss: 0.6846, train F loss: -0.6441, acc 0.9952\n",
            "epoch 2096: train D loss: 0.6866, train F loss: -0.6390, acc 0.9914\n",
            "epoch 2097: train D loss: 0.6820, train F loss: -0.6405, acc 0.9942\n",
            "epoch 2098: train D loss: 0.6880, train F loss: -0.6400, acc 0.9924\n",
            "epoch 2099: train D loss: 0.6823, train F loss: -0.6456, acc 0.9950\n",
            "epoch 2100: train D loss: 0.6825, train F loss: -0.6489, acc 0.9962\n",
            "epoch 2101: train D loss: 0.6804, train F loss: -0.6465, acc 0.9960\n",
            "epoch 2102: train D loss: 0.6850, train F loss: -0.6319, acc 0.9934\n",
            "epoch 2103: train D loss: 0.6825, train F loss: -0.6392, acc 0.9936\n",
            "epoch 2104: train D loss: 0.6822, train F loss: -0.6476, acc 0.9960\n",
            "epoch 2105: train D loss: 0.6858, train F loss: -0.6452, acc 0.9944\n",
            "epoch 2106: train D loss: 0.6856, train F loss: -0.6386, acc 0.9920\n",
            "epoch 2107: train D loss: 0.6872, train F loss: -0.6463, acc 0.9940\n",
            "epoch 2108: train D loss: 0.6847, train F loss: -0.6357, acc 0.9924\n",
            "epoch 2109: train D loss: 0.6825, train F loss: -0.6401, acc 0.9944\n",
            "epoch 2110: train D loss: 0.6812, train F loss: -0.6470, acc 0.9958\n",
            "epoch 2111: train D loss: 0.6799, train F loss: -0.6416, acc 0.9958\n",
            "epoch 2112: train D loss: 0.6833, train F loss: -0.6449, acc 0.9950\n",
            "epoch 2113: train D loss: 0.6821, train F loss: -0.6462, acc 0.9956\n",
            "epoch 2114: train D loss: 0.6828, train F loss: -0.6472, acc 0.9958\n",
            "epoch 2115: train D loss: 0.6814, train F loss: -0.6384, acc 0.9948\n",
            "epoch 2116: train D loss: 0.6845, train F loss: -0.6333, acc 0.9930\n",
            "epoch 2117: train D loss: 0.6852, train F loss: -0.6447, acc 0.9938\n",
            "epoch 2118: train D loss: 0.6844, train F loss: -0.6470, acc 0.9946\n",
            "epoch 2119: train D loss: 0.6825, train F loss: -0.6522, acc 0.9970\n",
            "epoch 2120: train D loss: 0.6804, train F loss: -0.6484, acc 0.9964\n",
            "epoch 2121: train D loss: 0.6864, train F loss: -0.6489, acc 0.9946\n",
            "epoch 2122: train D loss: 0.6851, train F loss: -0.6404, acc 0.9928\n",
            "epoch 2123: train D loss: 0.6838, train F loss: -0.6418, acc 0.9936\n",
            "epoch 2124: train D loss: 0.6821, train F loss: -0.6426, acc 0.9938\n",
            "epoch 2125: train D loss: 0.6852, train F loss: -0.6379, acc 0.9928\n",
            "epoch 2126: train D loss: 0.6832, train F loss: -0.6414, acc 0.9958\n",
            "epoch 2127: train D loss: 0.6827, train F loss: -0.6459, acc 0.9954\n",
            "epoch 2128: train D loss: 0.6860, train F loss: -0.6444, acc 0.9930\n",
            "epoch 2129: train D loss: 0.6796, train F loss: -0.6422, acc 0.9944\n",
            "epoch 2130: train D loss: 0.6847, train F loss: -0.6474, acc 0.9952\n",
            "epoch 2131: train D loss: 0.6850, train F loss: -0.6409, acc 0.9940\n",
            "epoch 2132: train D loss: 0.6845, train F loss: -0.6343, acc 0.9922\n",
            "epoch 2133: train D loss: 0.6828, train F loss: -0.6490, acc 0.9958\n",
            "epoch 2134: train D loss: 0.6835, train F loss: -0.6384, acc 0.9926\n",
            "epoch 2135: train D loss: 0.6862, train F loss: -0.6463, acc 0.9932\n",
            "epoch 2136: train D loss: 0.6822, train F loss: -0.6419, acc 0.9954\n",
            "epoch 2137: train D loss: 0.6854, train F loss: -0.6494, acc 0.9954\n",
            "epoch 2138: train D loss: 0.6869, train F loss: -0.6515, acc 0.9956\n",
            "epoch 2139: train D loss: 0.6812, train F loss: -0.6419, acc 0.9948\n",
            "epoch 2140: train D loss: 0.6840, train F loss: -0.6442, acc 0.9938\n",
            "epoch 2141: train D loss: 0.6849, train F loss: -0.6465, acc 0.9948\n",
            "epoch 2142: train D loss: 0.6830, train F loss: -0.6479, acc 0.9950\n",
            "epoch 2143: train D loss: 0.6828, train F loss: -0.6465, acc 0.9950\n",
            "epoch 2144: train D loss: 0.6833, train F loss: -0.6489, acc 0.9956\n",
            "epoch 2145: train D loss: 0.6853, train F loss: -0.6487, acc 0.9948\n",
            "epoch 2146: train D loss: 0.6867, train F loss: -0.6489, acc 0.9948\n",
            "epoch 2147: train D loss: 0.6792, train F loss: -0.6456, acc 0.9952\n",
            "epoch 2148: train D loss: 0.6880, train F loss: -0.6522, acc 0.9960\n",
            "epoch 2149: train D loss: 0.6820, train F loss: -0.6429, acc 0.9954\n",
            "epoch 2150: train D loss: 0.6863, train F loss: -0.6490, acc 0.9942\n",
            "epoch 2151: train D loss: 0.6883, train F loss: -0.6498, acc 0.9942\n",
            "epoch 2152: train D loss: 0.6819, train F loss: -0.6467, acc 0.9956\n",
            "epoch 2153: train D loss: 0.6844, train F loss: -0.6495, acc 0.9954\n",
            "epoch 2154: train D loss: 0.6827, train F loss: -0.6448, acc 0.9940\n",
            "epoch 2155: train D loss: 0.6836, train F loss: -0.6436, acc 0.9950\n",
            "epoch 2156: train D loss: 0.6845, train F loss: -0.6478, acc 0.9956\n",
            "epoch 2157: train D loss: 0.6827, train F loss: -0.6459, acc 0.9948\n",
            "epoch 2158: train D loss: 0.6844, train F loss: -0.6482, acc 0.9956\n",
            "epoch 2159: train D loss: 0.6860, train F loss: -0.6394, acc 0.9928\n",
            "epoch 2160: train D loss: 0.6796, train F loss: -0.6437, acc 0.9950\n",
            "epoch 2161: train D loss: 0.6846, train F loss: -0.6406, acc 0.9934\n",
            "epoch 2162: train D loss: 0.6813, train F loss: -0.6394, acc 0.9930\n",
            "epoch 2163: train D loss: 0.6852, train F loss: -0.6459, acc 0.9938\n",
            "epoch 2164: train D loss: 0.6859, train F loss: -0.6351, acc 0.9934\n",
            "epoch 2165: train D loss: 0.6858, train F loss: -0.6405, acc 0.9930\n",
            "epoch 2166: train D loss: 0.6819, train F loss: -0.6438, acc 0.9942\n",
            "epoch 2167: train D loss: 0.6813, train F loss: -0.6475, acc 0.9954\n",
            "epoch 2168: train D loss: 0.6835, train F loss: -0.6486, acc 0.9952\n",
            "epoch 2169: train D loss: 0.6854, train F loss: -0.6444, acc 0.9938\n",
            "epoch 2170: train D loss: 0.6818, train F loss: -0.6520, acc 0.9968\n",
            "epoch 2171: train D loss: 0.6834, train F loss: -0.6432, acc 0.9952\n",
            "epoch 2172: train D loss: 0.6850, train F loss: -0.6484, acc 0.9946\n",
            "epoch 2173: train D loss: 0.6828, train F loss: -0.6504, acc 0.9964\n",
            "epoch 2174: train D loss: 0.6844, train F loss: -0.6467, acc 0.9940\n",
            "epoch 2175: train D loss: 0.6806, train F loss: -0.6474, acc 0.9954\n",
            "epoch 2176: train D loss: 0.6846, train F loss: -0.6459, acc 0.9950\n",
            "epoch 2177: train D loss: 0.6826, train F loss: -0.6427, acc 0.9940\n",
            "epoch 2178: train D loss: 0.6867, train F loss: -0.6514, acc 0.9946\n",
            "epoch 2179: train D loss: 0.6821, train F loss: -0.6462, acc 0.9944\n",
            "epoch 2180: train D loss: 0.6833, train F loss: -0.6480, acc 0.9946\n",
            "epoch 2181: train D loss: 0.6833, train F loss: -0.6480, acc 0.9960\n",
            "epoch 2182: train D loss: 0.6850, train F loss: -0.6483, acc 0.9956\n",
            "epoch 2183: train D loss: 0.6859, train F loss: -0.6524, acc 0.9952\n",
            "epoch 2184: train D loss: 0.6833, train F loss: -0.6476, acc 0.9956\n",
            "epoch 2185: train D loss: 0.6839, train F loss: -0.6453, acc 0.9946\n",
            "epoch 2186: train D loss: 0.6842, train F loss: -0.6460, acc 0.9936\n",
            "epoch 2187: train D loss: 0.6868, train F loss: -0.6509, acc 0.9950\n",
            "epoch 2188: train D loss: 0.6868, train F loss: -0.6538, acc 0.9956\n",
            "epoch 2189: train D loss: 0.6857, train F loss: -0.6534, acc 0.9954\n",
            "epoch 2190: train D loss: 0.6846, train F loss: -0.6482, acc 0.9948\n",
            "epoch 2191: train D loss: 0.6845, train F loss: -0.6444, acc 0.9936\n",
            "epoch 2192: train D loss: 0.6838, train F loss: -0.6497, acc 0.9952\n",
            "epoch 2193: train D loss: 0.6795, train F loss: -0.6454, acc 0.9952\n",
            "epoch 2194: train D loss: 0.6867, train F loss: -0.6551, acc 0.9960\n",
            "epoch 2195: train D loss: 0.6856, train F loss: -0.6483, acc 0.9944\n",
            "epoch 2196: train D loss: 0.6844, train F loss: -0.6498, acc 0.9954\n",
            "epoch 2197: train D loss: 0.6818, train F loss: -0.6518, acc 0.9964\n",
            "epoch 2198: train D loss: 0.6875, train F loss: -0.6431, acc 0.9928\n",
            "epoch 2199: train D loss: 0.6836, train F loss: -0.6299, acc 0.9900\n",
            "epoch 2200: train D loss: 0.6880, train F loss: -0.6486, acc 0.9936\n",
            "epoch 2201: train D loss: 0.6861, train F loss: -0.6522, acc 0.9948\n",
            "epoch 2202: train D loss: 0.6801, train F loss: -0.6355, acc 0.9920\n",
            "epoch 2203: train D loss: 0.6821, train F loss: -0.6459, acc 0.9948\n",
            "epoch 2204: train D loss: 0.6870, train F loss: -0.6554, acc 0.9958\n",
            "epoch 2205: train D loss: 0.6847, train F loss: -0.6539, acc 0.9964\n",
            "epoch 2206: train D loss: 0.6855, train F loss: -0.6352, acc 0.9918\n",
            "epoch 2207: train D loss: 0.6805, train F loss: -0.6435, acc 0.9944\n",
            "epoch 2208: train D loss: 0.6851, train F loss: -0.6468, acc 0.9946\n",
            "epoch 2209: train D loss: 0.6840, train F loss: -0.6525, acc 0.9960\n",
            "epoch 2210: train D loss: 0.6849, train F loss: -0.6506, acc 0.9950\n",
            "epoch 2211: train D loss: 0.6843, train F loss: -0.6444, acc 0.9928\n",
            "epoch 2212: train D loss: 0.6874, train F loss: -0.6565, acc 0.9954\n",
            "epoch 2213: train D loss: 0.6827, train F loss: -0.6355, acc 0.9934\n",
            "epoch 2214: train D loss: 0.6808, train F loss: -0.6459, acc 0.9952\n",
            "epoch 2215: train D loss: 0.6807, train F loss: -0.6502, acc 0.9960\n",
            "epoch 2216: train D loss: 0.6863, train F loss: -0.6406, acc 0.9936\n",
            "epoch 2217: train D loss: 0.6832, train F loss: -0.6488, acc 0.9956\n",
            "epoch 2218: train D loss: 0.6848, train F loss: -0.6536, acc 0.9960\n",
            "epoch 2219: train D loss: 0.6847, train F loss: -0.6476, acc 0.9930\n",
            "epoch 2220: train D loss: 0.6829, train F loss: -0.6441, acc 0.9932\n",
            "epoch 2221: train D loss: 0.6822, train F loss: -0.6258, acc 0.9896\n",
            "epoch 2222: train D loss: 0.6832, train F loss: -0.6421, acc 0.9930\n",
            "epoch 2223: train D loss: 0.6849, train F loss: -0.6535, acc 0.9952\n",
            "epoch 2224: train D loss: 0.6862, train F loss: -0.6534, acc 0.9950\n",
            "epoch 2225: train D loss: 0.6809, train F loss: -0.6463, acc 0.9946\n",
            "epoch 2226: train D loss: 0.6844, train F loss: -0.6505, acc 0.9956\n",
            "epoch 2227: train D loss: 0.6844, train F loss: -0.6541, acc 0.9956\n",
            "epoch 2228: train D loss: 0.6866, train F loss: -0.6483, acc 0.9958\n",
            "epoch 2229: train D loss: 0.6839, train F loss: -0.6487, acc 0.9958\n",
            "epoch 2230: train D loss: 0.6845, train F loss: -0.6543, acc 0.9954\n",
            "epoch 2231: train D loss: 0.6847, train F loss: -0.6563, acc 0.9964\n",
            "epoch 2232: train D loss: 0.6845, train F loss: -0.6488, acc 0.9944\n",
            "epoch 2233: train D loss: 0.6822, train F loss: -0.6516, acc 0.9954\n",
            "epoch 2234: train D loss: 0.6872, train F loss: -0.6537, acc 0.9952\n",
            "epoch 2235: train D loss: 0.6833, train F loss: -0.6479, acc 0.9944\n",
            "epoch 2236: train D loss: 0.6873, train F loss: -0.6378, acc 0.9926\n",
            "epoch 2237: train D loss: 0.6819, train F loss: -0.6504, acc 0.9956\n",
            "epoch 2238: train D loss: 0.6815, train F loss: -0.6480, acc 0.9952\n",
            "epoch 2239: train D loss: 0.6869, train F loss: -0.6458, acc 0.9930\n",
            "epoch 2240: train D loss: 0.6836, train F loss: -0.6494, acc 0.9944\n",
            "epoch 2241: train D loss: 0.6858, train F loss: -0.6493, acc 0.9936\n",
            "epoch 2242: train D loss: 0.6822, train F loss: -0.6496, acc 0.9958\n",
            "epoch 2243: train D loss: 0.6857, train F loss: -0.6595, acc 0.9968\n",
            "epoch 2244: train D loss: 0.6847, train F loss: -0.6543, acc 0.9958\n",
            "epoch 2245: train D loss: 0.6869, train F loss: -0.6499, acc 0.9942\n",
            "epoch 2246: train D loss: 0.6865, train F loss: -0.6506, acc 0.9942\n",
            "epoch 2247: train D loss: 0.6868, train F loss: -0.6577, acc 0.9954\n",
            "epoch 2248: train D loss: 0.6867, train F loss: -0.6491, acc 0.9948\n",
            "epoch 2249: train D loss: 0.6839, train F loss: -0.6470, acc 0.9940\n",
            "epoch 2250: train D loss: 0.6834, train F loss: -0.6489, acc 0.9942\n",
            "epoch 2251: train D loss: 0.6846, train F loss: -0.6534, acc 0.9956\n",
            "epoch 2252: train D loss: 0.6836, train F loss: -0.6468, acc 0.9944\n",
            "epoch 2253: train D loss: 0.6831, train F loss: -0.6465, acc 0.9944\n",
            "epoch 2254: train D loss: 0.6848, train F loss: -0.6404, acc 0.9926\n",
            "epoch 2255: train D loss: 0.6839, train F loss: -0.6541, acc 0.9956\n",
            "epoch 2256: train D loss: 0.6860, train F loss: -0.6485, acc 0.9932\n",
            "epoch 2257: train D loss: 0.6879, train F loss: -0.6558, acc 0.9954\n",
            "epoch 2258: train D loss: 0.6832, train F loss: -0.6268, acc 0.9916\n",
            "epoch 2259: train D loss: 0.6826, train F loss: -0.6430, acc 0.9944\n",
            "epoch 2260: train D loss: 0.6837, train F loss: -0.6370, acc 0.9928\n",
            "epoch 2261: train D loss: 0.6878, train F loss: -0.6437, acc 0.9918\n",
            "epoch 2262: train D loss: 0.6833, train F loss: -0.6519, acc 0.9950\n",
            "epoch 2263: train D loss: 0.6835, train F loss: -0.6454, acc 0.9944\n",
            "epoch 2264: train D loss: 0.6845, train F loss: -0.6554, acc 0.9956\n",
            "epoch 2265: train D loss: 0.6791, train F loss: -0.6506, acc 0.9960\n",
            "epoch 2266: train D loss: 0.6827, train F loss: -0.6434, acc 0.9932\n",
            "epoch 2267: train D loss: 0.6837, train F loss: -0.6513, acc 0.9960\n",
            "epoch 2268: train D loss: 0.6861, train F loss: -0.6502, acc 0.9942\n",
            "epoch 2269: train D loss: 0.6872, train F loss: -0.6541, acc 0.9954\n",
            "epoch 2270: train D loss: 0.6845, train F loss: -0.6545, acc 0.9946\n",
            "epoch 2271: train D loss: 0.6828, train F loss: -0.6550, acc 0.9958\n",
            "epoch 2272: train D loss: 0.6796, train F loss: -0.6511, acc 0.9960\n",
            "epoch 2273: train D loss: 0.6849, train F loss: -0.6543, acc 0.9954\n",
            "epoch 2274: train D loss: 0.6852, train F loss: -0.6532, acc 0.9946\n",
            "epoch 2275: train D loss: 0.6825, train F loss: -0.6501, acc 0.9950\n",
            "epoch 2276: train D loss: 0.6843, train F loss: -0.6483, acc 0.9942\n",
            "epoch 2277: train D loss: 0.6811, train F loss: -0.6445, acc 0.9942\n",
            "epoch 2278: train D loss: 0.6847, train F loss: -0.6486, acc 0.9938\n",
            "epoch 2279: train D loss: 0.6875, train F loss: -0.6498, acc 0.9936\n",
            "epoch 2280: train D loss: 0.6837, train F loss: -0.6533, acc 0.9948\n",
            "epoch 2281: train D loss: 0.6845, train F loss: -0.6472, acc 0.9944\n",
            "epoch 2282: train D loss: 0.6826, train F loss: -0.6523, acc 0.9952\n",
            "epoch 2283: train D loss: 0.6826, train F loss: -0.6559, acc 0.9964\n",
            "epoch 2284: train D loss: 0.6810, train F loss: -0.6525, acc 0.9960\n",
            "epoch 2285: train D loss: 0.6824, train F loss: -0.6520, acc 0.9954\n",
            "epoch 2286: train D loss: 0.6894, train F loss: -0.6563, acc 0.9952\n",
            "epoch 2287: train D loss: 0.6852, train F loss: -0.6571, acc 0.9956\n",
            "epoch 2288: train D loss: 0.6839, train F loss: -0.6477, acc 0.9938\n",
            "epoch 2289: train D loss: 0.6849, train F loss: -0.6557, acc 0.9952\n",
            "epoch 2290: train D loss: 0.6835, train F loss: -0.6538, acc 0.9956\n",
            "epoch 2291: train D loss: 0.6858, train F loss: -0.6534, acc 0.9948\n",
            "epoch 2292: train D loss: 0.6838, train F loss: -0.6520, acc 0.9956\n",
            "epoch 2293: train D loss: 0.6867, train F loss: -0.6526, acc 0.9946\n",
            "epoch 2294: train D loss: 0.6838, train F loss: -0.6549, acc 0.9954\n",
            "epoch 2295: train D loss: 0.6814, train F loss: -0.6487, acc 0.9948\n",
            "epoch 2296: train D loss: 0.6856, train F loss: -0.6461, acc 0.9934\n",
            "epoch 2297: train D loss: 0.6874, train F loss: -0.6587, acc 0.9954\n",
            "epoch 2298: train D loss: 0.6870, train F loss: -0.6506, acc 0.9934\n",
            "epoch 2299: train D loss: 0.6835, train F loss: -0.6480, acc 0.9940\n",
            "epoch 2300: train D loss: 0.6850, train F loss: -0.6525, acc 0.9938\n",
            "epoch 2301: train D loss: 0.6844, train F loss: -0.6514, acc 0.9948\n",
            "epoch 2302: train D loss: 0.6808, train F loss: -0.6444, acc 0.9942\n",
            "epoch 2303: train D loss: 0.6806, train F loss: -0.6306, acc 0.9920\n",
            "epoch 2304: train D loss: 0.6829, train F loss: -0.6510, acc 0.9952\n",
            "epoch 2305: train D loss: 0.6830, train F loss: -0.6544, acc 0.9964\n",
            "epoch 2306: train D loss: 0.6823, train F loss: -0.6424, acc 0.9930\n",
            "epoch 2307: train D loss: 0.6841, train F loss: -0.6483, acc 0.9948\n",
            "epoch 2308: train D loss: 0.6808, train F loss: -0.6521, acc 0.9960\n",
            "epoch 2309: train D loss: 0.6820, train F loss: -0.6491, acc 0.9954\n",
            "epoch 2310: train D loss: 0.6823, train F loss: -0.6533, acc 0.9958\n",
            "epoch 2311: train D loss: 0.6850, train F loss: -0.6568, acc 0.9956\n",
            "epoch 2312: train D loss: 0.6840, train F loss: -0.6475, acc 0.9946\n",
            "epoch 2313: train D loss: 0.6862, train F loss: -0.6137, acc 0.9898\n",
            "epoch 2314: train D loss: 0.6862, train F loss: -0.6523, acc 0.9934\n",
            "epoch 2315: train D loss: 0.6855, train F loss: -0.6546, acc 0.9952\n",
            "epoch 2316: train D loss: 0.6831, train F loss: -0.6538, acc 0.9952\n",
            "epoch 2317: train D loss: 0.6805, train F loss: -0.6526, acc 0.9960\n",
            "epoch 2318: train D loss: 0.6841, train F loss: -0.6511, acc 0.9954\n",
            "epoch 2319: train D loss: 0.6814, train F loss: -0.6417, acc 0.9936\n",
            "epoch 2320: train D loss: 0.6826, train F loss: -0.6448, acc 0.9932\n",
            "epoch 2321: train D loss: 0.6837, train F loss: -0.6550, acc 0.9952\n",
            "epoch 2322: train D loss: 0.6830, train F loss: -0.6544, acc 0.9954\n",
            "epoch 2323: train D loss: 0.6853, train F loss: -0.6555, acc 0.9954\n",
            "epoch 2324: train D loss: 0.6846, train F loss: -0.6465, acc 0.9926\n",
            "epoch 2325: train D loss: 0.6827, train F loss: -0.6520, acc 0.9942\n",
            "epoch 2326: train D loss: 0.6835, train F loss: -0.6495, acc 0.9942\n",
            "epoch 2327: train D loss: 0.6845, train F loss: -0.6478, acc 0.9954\n",
            "epoch 2328: train D loss: 0.6798, train F loss: -0.6405, acc 0.9922\n",
            "epoch 2329: train D loss: 0.6810, train F loss: -0.6475, acc 0.9948\n",
            "epoch 2330: train D loss: 0.6836, train F loss: -0.6526, acc 0.9960\n",
            "epoch 2331: train D loss: 0.6794, train F loss: -0.6499, acc 0.9956\n",
            "epoch 2332: train D loss: 0.6849, train F loss: -0.6045, acc 0.9866\n",
            "epoch 2333: train D loss: 0.6869, train F loss: -0.6491, acc 0.9920\n",
            "epoch 2334: train D loss: 0.6809, train F loss: -0.6514, acc 0.9958\n",
            "epoch 2335: train D loss: 0.6857, train F loss: -0.6565, acc 0.9952\n",
            "epoch 2336: train D loss: 0.6828, train F loss: -0.6412, acc 0.9936\n",
            "epoch 2337: train D loss: 0.6846, train F loss: -0.6596, acc 0.9966\n",
            "epoch 2338: train D loss: 0.6856, train F loss: -0.6594, acc 0.9962\n",
            "epoch 2339: train D loss: 0.6854, train F loss: -0.6477, acc 0.9942\n",
            "epoch 2340: train D loss: 0.6822, train F loss: -0.6545, acc 0.9958\n",
            "epoch 2341: train D loss: 0.6850, train F loss: -0.6551, acc 0.9956\n",
            "epoch 2342: train D loss: 0.6867, train F loss: -0.6619, acc 0.9964\n",
            "epoch 2343: train D loss: 0.6848, train F loss: -0.6579, acc 0.9960\n",
            "epoch 2344: train D loss: 0.6874, train F loss: -0.6371, acc 0.9892\n",
            "epoch 2345: train D loss: 0.6830, train F loss: -0.6301, acc 0.9924\n",
            "epoch 2346: train D loss: 0.6820, train F loss: -0.6557, acc 0.9958\n",
            "epoch 2347: train D loss: 0.6825, train F loss: -0.6531, acc 0.9954\n",
            "epoch 2348: train D loss: 0.6787, train F loss: -0.6496, acc 0.9956\n",
            "epoch 2349: train D loss: 0.6834, train F loss: -0.6309, acc 0.9920\n",
            "epoch 2350: train D loss: 0.6848, train F loss: -0.6566, acc 0.9950\n",
            "epoch 2351: train D loss: 0.6827, train F loss: -0.6482, acc 0.9936\n",
            "epoch 2352: train D loss: 0.6851, train F loss: -0.6547, acc 0.9960\n",
            "epoch 2353: train D loss: 0.6856, train F loss: -0.6563, acc 0.9950\n",
            "epoch 2354: train D loss: 0.6805, train F loss: -0.6512, acc 0.9960\n",
            "epoch 2355: train D loss: 0.6847, train F loss: -0.6534, acc 0.9946\n",
            "epoch 2356: train D loss: 0.6843, train F loss: -0.6513, acc 0.9950\n",
            "epoch 2357: train D loss: 0.6850, train F loss: -0.6588, acc 0.9964\n",
            "epoch 2358: train D loss: 0.6835, train F loss: -0.6572, acc 0.9960\n",
            "epoch 2359: train D loss: 0.6844, train F loss: -0.6493, acc 0.9936\n",
            "epoch 2360: train D loss: 0.6806, train F loss: -0.6496, acc 0.9944\n",
            "epoch 2361: train D loss: 0.6865, train F loss: -0.6565, acc 0.9954\n",
            "epoch 2362: train D loss: 0.6832, train F loss: -0.6397, acc 0.9934\n",
            "epoch 2363: train D loss: 0.6857, train F loss: -0.6528, acc 0.9958\n",
            "epoch 2364: train D loss: 0.6834, train F loss: -0.6528, acc 0.9944\n",
            "epoch 2365: train D loss: 0.6838, train F loss: -0.6565, acc 0.9958\n",
            "epoch 2366: train D loss: 0.6860, train F loss: -0.6533, acc 0.9946\n",
            "epoch 2367: train D loss: 0.6824, train F loss: -0.6526, acc 0.9950\n",
            "epoch 2368: train D loss: 0.6834, train F loss: -0.6595, acc 0.9960\n",
            "epoch 2369: train D loss: 0.6880, train F loss: -0.6619, acc 0.9962\n",
            "epoch 2370: train D loss: 0.6862, train F loss: -0.6530, acc 0.9940\n",
            "epoch 2371: train D loss: 0.6807, train F loss: -0.6503, acc 0.9950\n",
            "epoch 2372: train D loss: 0.6841, train F loss: -0.6531, acc 0.9958\n",
            "epoch 2373: train D loss: 0.6824, train F loss: -0.6568, acc 0.9956\n",
            "epoch 2374: train D loss: 0.6845, train F loss: -0.6503, acc 0.9946\n",
            "epoch 2375: train D loss: 0.6789, train F loss: -0.6311, acc 0.9912\n",
            "epoch 2376: train D loss: 0.6828, train F loss: -0.6509, acc 0.9944\n",
            "epoch 2377: train D loss: 0.6847, train F loss: -0.6561, acc 0.9956\n",
            "epoch 2378: train D loss: 0.6871, train F loss: -0.6613, acc 0.9954\n",
            "epoch 2379: train D loss: 0.6840, train F loss: -0.6553, acc 0.9952\n",
            "epoch 2380: train D loss: 0.6842, train F loss: -0.6512, acc 0.9948\n",
            "epoch 2381: train D loss: 0.6843, train F loss: -0.6521, acc 0.9944\n",
            "epoch 2382: train D loss: 0.6840, train F loss: -0.6481, acc 0.9940\n",
            "epoch 2383: train D loss: 0.6843, train F loss: -0.6557, acc 0.9956\n",
            "epoch 2384: train D loss: 0.6829, train F loss: -0.6545, acc 0.9954\n",
            "epoch 2385: train D loss: 0.6826, train F loss: -0.6497, acc 0.9940\n",
            "epoch 2386: train D loss: 0.6863, train F loss: -0.6544, acc 0.9944\n",
            "epoch 2387: train D loss: 0.6848, train F loss: -0.6554, acc 0.9952\n",
            "epoch 2388: train D loss: 0.6848, train F loss: -0.6374, acc 0.9918\n",
            "epoch 2389: train D loss: 0.6831, train F loss: -0.6528, acc 0.9940\n",
            "epoch 2390: train D loss: 0.6800, train F loss: -0.6502, acc 0.9948\n",
            "epoch 2391: train D loss: 0.6852, train F loss: -0.6594, acc 0.9960\n",
            "epoch 2392: train D loss: 0.6860, train F loss: -0.6597, acc 0.9960\n",
            "epoch 2393: train D loss: 0.6808, train F loss: -0.6517, acc 0.9950\n",
            "epoch 2394: train D loss: 0.6872, train F loss: -0.5894, acc 0.9838\n",
            "epoch 2395: train D loss: 0.6843, train F loss: -0.6494, acc 0.9942\n",
            "epoch 2396: train D loss: 0.6851, train F loss: -0.6442, acc 0.9928\n",
            "epoch 2397: train D loss: 0.6839, train F loss: -0.6512, acc 0.9946\n",
            "epoch 2398: train D loss: 0.6835, train F loss: -0.6596, acc 0.9970\n",
            "epoch 2399: train D loss: 0.6878, train F loss: -0.6596, acc 0.9952\n",
            "epoch 2400: train D loss: 0.6827, train F loss: -0.6536, acc 0.9958\n",
            "epoch 2401: train D loss: 0.6809, train F loss: -0.6539, acc 0.9954\n",
            "epoch 2402: train D loss: 0.6819, train F loss: -0.6539, acc 0.9956\n",
            "epoch 2403: train D loss: 0.6826, train F loss: -0.6362, acc 0.9910\n",
            "epoch 2404: train D loss: 0.6879, train F loss: -0.6587, acc 0.9944\n",
            "epoch 2405: train D loss: 0.6838, train F loss: -0.6564, acc 0.9950\n",
            "epoch 2406: train D loss: 0.6842, train F loss: -0.6446, acc 0.9934\n",
            "epoch 2407: train D loss: 0.6847, train F loss: -0.6396, acc 0.9916\n",
            "epoch 2408: train D loss: 0.6814, train F loss: -0.6455, acc 0.9942\n",
            "epoch 2409: train D loss: 0.6838, train F loss: -0.6551, acc 0.9952\n",
            "epoch 2410: train D loss: 0.6846, train F loss: -0.6579, acc 0.9958\n",
            "epoch 2411: train D loss: 0.6845, train F loss: -0.6595, acc 0.9966\n",
            "epoch 2412: train D loss: 0.6830, train F loss: -0.6573, acc 0.9954\n",
            "epoch 2413: train D loss: 0.6831, train F loss: -0.6504, acc 0.9942\n",
            "epoch 2414: train D loss: 0.6832, train F loss: -0.6568, acc 0.9954\n",
            "epoch 2415: train D loss: 0.6855, train F loss: -0.6593, acc 0.9948\n",
            "epoch 2416: train D loss: 0.6828, train F loss: -0.6512, acc 0.9948\n",
            "epoch 2417: train D loss: 0.6835, train F loss: -0.6545, acc 0.9958\n",
            "epoch 2418: train D loss: 0.6854, train F loss: -0.6533, acc 0.9954\n",
            "epoch 2419: train D loss: 0.6847, train F loss: -0.6585, acc 0.9948\n",
            "epoch 2420: train D loss: 0.6867, train F loss: -0.6601, acc 0.9960\n",
            "epoch 2421: train D loss: 0.6838, train F loss: -0.6563, acc 0.9950\n",
            "epoch 2422: train D loss: 0.6859, train F loss: -0.6589, acc 0.9958\n",
            "epoch 2423: train D loss: 0.6857, train F loss: -0.6637, acc 0.9972\n",
            "epoch 2424: train D loss: 0.6863, train F loss: -0.6604, acc 0.9950\n",
            "epoch 2425: train D loss: 0.6842, train F loss: -0.6574, acc 0.9948\n",
            "epoch 2426: train D loss: 0.6859, train F loss: -0.6590, acc 0.9948\n",
            "epoch 2427: train D loss: 0.6861, train F loss: -0.6579, acc 0.9952\n",
            "epoch 2428: train D loss: 0.6862, train F loss: -0.6295, acc 0.9912\n",
            "epoch 2429: train D loss: 0.6825, train F loss: -0.6496, acc 0.9948\n",
            "epoch 2430: train D loss: 0.6842, train F loss: -0.6447, acc 0.9926\n",
            "epoch 2431: train D loss: 0.6851, train F loss: -0.6486, acc 0.9940\n",
            "epoch 2432: train D loss: 0.6827, train F loss: -0.6568, acc 0.9954\n",
            "epoch 2433: train D loss: 0.6863, train F loss: -0.6598, acc 0.9960\n",
            "epoch 2434: train D loss: 0.6841, train F loss: -0.6611, acc 0.9964\n",
            "epoch 2435: train D loss: 0.6840, train F loss: -0.6574, acc 0.9952\n",
            "epoch 2436: train D loss: 0.6887, train F loss: -0.6634, acc 0.9950\n",
            "epoch 2437: train D loss: 0.6841, train F loss: -0.6378, acc 0.9902\n",
            "epoch 2438: train D loss: 0.6845, train F loss: -0.6479, acc 0.9932\n",
            "epoch 2439: train D loss: 0.6830, train F loss: -0.6537, acc 0.9944\n",
            "epoch 2440: train D loss: 0.6807, train F loss: -0.6538, acc 0.9954\n",
            "epoch 2441: train D loss: 0.6836, train F loss: -0.6524, acc 0.9942\n",
            "epoch 2442: train D loss: 0.6839, train F loss: -0.6588, acc 0.9952\n",
            "epoch 2443: train D loss: 0.6846, train F loss: -0.6602, acc 0.9964\n",
            "epoch 2444: train D loss: 0.6875, train F loss: -0.6641, acc 0.9958\n",
            "epoch 2445: train D loss: 0.6829, train F loss: -0.6569, acc 0.9954\n",
            "epoch 2446: train D loss: 0.6847, train F loss: -0.6598, acc 0.9956\n",
            "epoch 2447: train D loss: 0.6833, train F loss: -0.6606, acc 0.9958\n",
            "epoch 2448: train D loss: 0.6842, train F loss: -0.6397, acc 0.9920\n",
            "epoch 2449: train D loss: 0.6828, train F loss: -0.6519, acc 0.9940\n",
            "epoch 2450: train D loss: 0.6849, train F loss: -0.6527, acc 0.9938\n",
            "epoch 2451: train D loss: 0.6867, train F loss: -0.6551, acc 0.9944\n",
            "epoch 2452: train D loss: 0.6833, train F loss: -0.6590, acc 0.9964\n",
            "epoch 2453: train D loss: 0.6846, train F loss: -0.6591, acc 0.9952\n",
            "epoch 2454: train D loss: 0.6857, train F loss: -0.6584, acc 0.9952\n",
            "epoch 2455: train D loss: 0.6840, train F loss: -0.6586, acc 0.9960\n",
            "epoch 2456: train D loss: 0.6849, train F loss: -0.6547, acc 0.9956\n",
            "epoch 2457: train D loss: 0.6849, train F loss: -0.6611, acc 0.9954\n",
            "epoch 2458: train D loss: 0.6860, train F loss: -0.6598, acc 0.9946\n",
            "epoch 2459: train D loss: 0.6889, train F loss: -0.6581, acc 0.9938\n",
            "epoch 2460: train D loss: 0.6884, train F loss: -0.6596, acc 0.9946\n",
            "epoch 2461: train D loss: 0.6826, train F loss: -0.6555, acc 0.9948\n",
            "epoch 2462: train D loss: 0.6860, train F loss: -0.6571, acc 0.9944\n",
            "epoch 2463: train D loss: 0.6822, train F loss: -0.6530, acc 0.9948\n",
            "epoch 2464: train D loss: 0.6853, train F loss: -0.6623, acc 0.9964\n",
            "epoch 2465: train D loss: 0.6861, train F loss: -0.6569, acc 0.9942\n",
            "epoch 2466: train D loss: 0.6855, train F loss: -0.6508, acc 0.9936\n",
            "epoch 2467: train D loss: 0.6822, train F loss: -0.6526, acc 0.9946\n",
            "epoch 2468: train D loss: 0.6839, train F loss: -0.6551, acc 0.9942\n",
            "epoch 2469: train D loss: 0.6849, train F loss: -0.6499, acc 0.9938\n",
            "epoch 2470: train D loss: 0.6863, train F loss: -0.6542, acc 0.9934\n",
            "epoch 2471: train D loss: 0.6846, train F loss: -0.6599, acc 0.9956\n",
            "epoch 2472: train D loss: 0.6878, train F loss: -0.6598, acc 0.9940\n",
            "epoch 2473: train D loss: 0.6847, train F loss: -0.6591, acc 0.9950\n",
            "epoch 2474: train D loss: 0.6839, train F loss: -0.6238, acc 0.9886\n",
            "epoch 2475: train D loss: 0.6811, train F loss: -0.6486, acc 0.9938\n",
            "epoch 2476: train D loss: 0.6836, train F loss: -0.6597, acc 0.9960\n",
            "epoch 2477: train D loss: 0.6826, train F loss: -0.6510, acc 0.9944\n",
            "epoch 2478: train D loss: 0.6840, train F loss: -0.6516, acc 0.9946\n",
            "epoch 2479: train D loss: 0.6865, train F loss: -0.6610, acc 0.9954\n",
            "epoch 2480: train D loss: 0.6841, train F loss: -0.6595, acc 0.9948\n",
            "epoch 2481: train D loss: 0.6823, train F loss: -0.6525, acc 0.9940\n",
            "epoch 2482: train D loss: 0.6862, train F loss: -0.6606, acc 0.9952\n",
            "epoch 2483: train D loss: 0.6849, train F loss: -0.6623, acc 0.9964\n",
            "epoch 2484: train D loss: 0.6854, train F loss: -0.6529, acc 0.9960\n",
            "epoch 2485: train D loss: 0.6847, train F loss: -0.6520, acc 0.9938\n",
            "epoch 2486: train D loss: 0.6830, train F loss: -0.6564, acc 0.9950\n",
            "epoch 2487: train D loss: 0.6831, train F loss: -0.6549, acc 0.9946\n",
            "epoch 2488: train D loss: 0.6796, train F loss: -0.6561, acc 0.9956\n",
            "epoch 2489: train D loss: 0.6866, train F loss: -0.6613, acc 0.9952\n",
            "epoch 2490: train D loss: 0.6844, train F loss: -0.6574, acc 0.9950\n",
            "epoch 2491: train D loss: 0.6855, train F loss: -0.6601, acc 0.9950\n",
            "epoch 2492: train D loss: 0.6819, train F loss: -0.6512, acc 0.9946\n",
            "epoch 2493: train D loss: 0.6842, train F loss: -0.6599, acc 0.9954\n",
            "epoch 2494: train D loss: 0.6838, train F loss: -0.6569, acc 0.9954\n",
            "epoch 2495: train D loss: 0.6859, train F loss: -0.6600, acc 0.9956\n",
            "epoch 2496: train D loss: 0.6842, train F loss: -0.6620, acc 0.9962\n",
            "epoch 2497: train D loss: 0.6867, train F loss: -0.6549, acc 0.9936\n",
            "epoch 2498: train D loss: 0.6827, train F loss: -0.6620, acc 0.9970\n",
            "epoch 2499: train D loss: 0.6825, train F loss: -0.6561, acc 0.9950\n",
            "epoch 2500: train D loss: 0.6880, train F loss: -0.6230, acc 0.9900\n",
            "epoch 2501: train D loss: 0.6843, train F loss: -0.6612, acc 0.9960\n",
            "epoch 2502: train D loss: 0.6823, train F loss: -0.6589, acc 0.9958\n",
            "epoch 2503: train D loss: 0.6840, train F loss: -0.6567, acc 0.9958\n",
            "epoch 2504: train D loss: 0.6843, train F loss: -0.6612, acc 0.9958\n",
            "epoch 2505: train D loss: 0.6866, train F loss: -0.6483, acc 0.9950\n",
            "epoch 2506: train D loss: 0.6860, train F loss: -0.6633, acc 0.9956\n",
            "epoch 2507: train D loss: 0.6821, train F loss: -0.6624, acc 0.9970\n",
            "epoch 2508: train D loss: 0.6859, train F loss: -0.6612, acc 0.9954\n",
            "epoch 2509: train D loss: 0.6832, train F loss: -0.6590, acc 0.9954\n",
            "epoch 2510: train D loss: 0.6857, train F loss: -0.6563, acc 0.9948\n",
            "epoch 2511: train D loss: 0.6875, train F loss: -0.6526, acc 0.9926\n",
            "epoch 2512: train D loss: 0.6867, train F loss: -0.6552, acc 0.9934\n",
            "epoch 2513: train D loss: 0.6834, train F loss: -0.6530, acc 0.9930\n",
            "epoch 2514: train D loss: 0.6836, train F loss: -0.6527, acc 0.9930\n",
            "epoch 2515: train D loss: 0.6857, train F loss: -0.6546, acc 0.9942\n",
            "epoch 2516: train D loss: 0.6836, train F loss: -0.6505, acc 0.9956\n",
            "epoch 2517: train D loss: 0.6867, train F loss: -0.6397, acc 0.9902\n",
            "epoch 2518: train D loss: 0.6834, train F loss: -0.6472, acc 0.9936\n",
            "epoch 2519: train D loss: 0.6857, train F loss: -0.6622, acc 0.9960\n",
            "epoch 2520: train D loss: 0.6840, train F loss: -0.6586, acc 0.9960\n",
            "epoch 2521: train D loss: 0.6823, train F loss: -0.6593, acc 0.9958\n",
            "epoch 2522: train D loss: 0.6856, train F loss: -0.6585, acc 0.9954\n",
            "epoch 2523: train D loss: 0.6866, train F loss: -0.6618, acc 0.9950\n",
            "epoch 2524: train D loss: 0.6839, train F loss: -0.6628, acc 0.9964\n",
            "epoch 2525: train D loss: 0.6829, train F loss: -0.6570, acc 0.9960\n",
            "epoch 2526: train D loss: 0.6863, train F loss: -0.6645, acc 0.9968\n",
            "epoch 2527: train D loss: 0.6843, train F loss: -0.6605, acc 0.9956\n",
            "epoch 2528: train D loss: 0.6835, train F loss: -0.6486, acc 0.9938\n",
            "epoch 2529: train D loss: 0.6852, train F loss: -0.6615, acc 0.9950\n",
            "epoch 2530: train D loss: 0.6841, train F loss: -0.6435, acc 0.9910\n",
            "epoch 2531: train D loss: 0.6846, train F loss: -0.6561, acc 0.9932\n",
            "epoch 2532: train D loss: 0.6846, train F loss: -0.6576, acc 0.9948\n",
            "epoch 2533: train D loss: 0.6838, train F loss: -0.6600, acc 0.9960\n",
            "epoch 2534: train D loss: 0.6852, train F loss: -0.6542, acc 0.9958\n",
            "epoch 2535: train D loss: 0.6853, train F loss: -0.6586, acc 0.9948\n",
            "epoch 2536: train D loss: 0.6858, train F loss: -0.6593, acc 0.9956\n",
            "epoch 2537: train D loss: 0.6831, train F loss: -0.6560, acc 0.9950\n",
            "epoch 2538: train D loss: 0.6859, train F loss: -0.6356, acc 0.9892\n",
            "epoch 2539: train D loss: 0.6829, train F loss: -0.6488, acc 0.9932\n",
            "epoch 2540: train D loss: 0.6820, train F loss: -0.6520, acc 0.9942\n",
            "epoch 2541: train D loss: 0.6793, train F loss: -0.6538, acc 0.9954\n",
            "epoch 2542: train D loss: 0.6829, train F loss: -0.6544, acc 0.9954\n",
            "epoch 2543: train D loss: 0.6839, train F loss: -0.6606, acc 0.9958\n",
            "epoch 2544: train D loss: 0.6822, train F loss: -0.6602, acc 0.9962\n",
            "epoch 2545: train D loss: 0.6844, train F loss: -0.6365, acc 0.9900\n",
            "epoch 2546: train D loss: 0.6808, train F loss: -0.6494, acc 0.9934\n",
            "epoch 2547: train D loss: 0.6823, train F loss: -0.6576, acc 0.9954\n",
            "epoch 2548: train D loss: 0.6849, train F loss: -0.6604, acc 0.9946\n",
            "epoch 2549: train D loss: 0.6860, train F loss: -0.6641, acc 0.9962\n",
            "epoch 2550: train D loss: 0.6848, train F loss: -0.6636, acc 0.9964\n",
            "epoch 2551: train D loss: 0.6847, train F loss: -0.6590, acc 0.9950\n",
            "epoch 2552: train D loss: 0.6838, train F loss: -0.6415, acc 0.9906\n",
            "epoch 2553: train D loss: 0.6805, train F loss: -0.6432, acc 0.9930\n",
            "epoch 2554: train D loss: 0.6796, train F loss: -0.6558, acc 0.9958\n",
            "epoch 2555: train D loss: 0.6846, train F loss: -0.6556, acc 0.9948\n",
            "epoch 2556: train D loss: 0.6863, train F loss: -0.6615, acc 0.9954\n",
            "epoch 2557: train D loss: 0.6840, train F loss: -0.6512, acc 0.9948\n",
            "epoch 2558: train D loss: 0.6833, train F loss: -0.6483, acc 0.9928\n",
            "epoch 2559: train D loss: 0.6836, train F loss: -0.6573, acc 0.9952\n",
            "epoch 2560: train D loss: 0.6845, train F loss: -0.6622, acc 0.9956\n",
            "epoch 2561: train D loss: 0.6854, train F loss: -0.6518, acc 0.9932\n",
            "epoch 2562: train D loss: 0.6839, train F loss: -0.6583, acc 0.9944\n",
            "epoch 2563: train D loss: 0.6820, train F loss: -0.6491, acc 0.9938\n",
            "epoch 2564: train D loss: 0.6806, train F loss: -0.6572, acc 0.9960\n",
            "epoch 2565: train D loss: 0.6835, train F loss: -0.6586, acc 0.9962\n",
            "epoch 2566: train D loss: 0.6838, train F loss: -0.6591, acc 0.9952\n",
            "epoch 2567: train D loss: 0.6866, train F loss: -0.6606, acc 0.9950\n",
            "epoch 2568: train D loss: 0.6834, train F loss: -0.6558, acc 0.9958\n",
            "epoch 2569: train D loss: 0.6851, train F loss: -0.6607, acc 0.9958\n",
            "epoch 2570: train D loss: 0.6837, train F loss: -0.6527, acc 0.9946\n",
            "epoch 2571: train D loss: 0.6846, train F loss: -0.6583, acc 0.9948\n",
            "epoch 2572: train D loss: 0.6839, train F loss: -0.6502, acc 0.9934\n",
            "epoch 2573: train D loss: 0.6803, train F loss: -0.6539, acc 0.9954\n",
            "epoch 2574: train D loss: 0.6839, train F loss: -0.6439, acc 0.9934\n",
            "epoch 2575: train D loss: 0.6827, train F loss: -0.6534, acc 0.9928\n",
            "epoch 2576: train D loss: 0.6820, train F loss: -0.6514, acc 0.9944\n",
            "epoch 2577: train D loss: 0.6855, train F loss: -0.6630, acc 0.9962\n",
            "epoch 2578: train D loss: 0.6851, train F loss: -0.6629, acc 0.9956\n",
            "epoch 2579: train D loss: 0.6806, train F loss: -0.6565, acc 0.9960\n",
            "epoch 2580: train D loss: 0.6850, train F loss: -0.6610, acc 0.9962\n",
            "epoch 2581: train D loss: 0.6852, train F loss: -0.6629, acc 0.9964\n",
            "epoch 2582: train D loss: 0.6837, train F loss: -0.6611, acc 0.9960\n",
            "epoch 2583: train D loss: 0.6861, train F loss: -0.6571, acc 0.9946\n",
            "epoch 2584: train D loss: 0.6874, train F loss: -0.6531, acc 0.9928\n",
            "epoch 2585: train D loss: 0.6886, train F loss: -0.6651, acc 0.9946\n",
            "epoch 2586: train D loss: 0.6843, train F loss: -0.6637, acc 0.9962\n",
            "epoch 2587: train D loss: 0.6832, train F loss: -0.6579, acc 0.9948\n",
            "epoch 2588: train D loss: 0.6852, train F loss: -0.6586, acc 0.9950\n",
            "epoch 2589: train D loss: 0.6846, train F loss: -0.6609, acc 0.9950\n",
            "epoch 2590: train D loss: 0.6863, train F loss: -0.6442, acc 0.9914\n",
            "epoch 2591: train D loss: 0.6847, train F loss: -0.6644, acc 0.9966\n",
            "epoch 2592: train D loss: 0.6858, train F loss: -0.6595, acc 0.9950\n",
            "epoch 2593: train D loss: 0.6844, train F loss: -0.6578, acc 0.9948\n",
            "epoch 2594: train D loss: 0.6857, train F loss: -0.6555, acc 0.9936\n",
            "epoch 2595: train D loss: 0.6841, train F loss: -0.6586, acc 0.9942\n",
            "epoch 2596: train D loss: 0.6872, train F loss: -0.6559, acc 0.9946\n",
            "epoch 2597: train D loss: 0.6850, train F loss: -0.6527, acc 0.9954\n",
            "epoch 2598: train D loss: 0.6882, train F loss: -0.6278, acc 0.9872\n",
            "epoch 2599: train D loss: 0.6858, train F loss: -0.6455, acc 0.9906\n",
            "epoch 2600: train D loss: 0.6834, train F loss: -0.6441, acc 0.9940\n",
            "epoch 2601: train D loss: 0.6842, train F loss: -0.6465, acc 0.9920\n",
            "epoch 2602: train D loss: 0.6812, train F loss: -0.6582, acc 0.9958\n",
            "epoch 2603: train D loss: 0.6804, train F loss: -0.6556, acc 0.9952\n",
            "epoch 2604: train D loss: 0.6825, train F loss: -0.6568, acc 0.9948\n",
            "epoch 2605: train D loss: 0.6839, train F loss: -0.6621, acc 0.9962\n",
            "epoch 2606: train D loss: 0.6831, train F loss: -0.6593, acc 0.9958\n",
            "epoch 2607: train D loss: 0.6839, train F loss: -0.6621, acc 0.9956\n",
            "epoch 2608: train D loss: 0.6837, train F loss: -0.6642, acc 0.9966\n",
            "epoch 2609: train D loss: 0.6859, train F loss: -0.6637, acc 0.9962\n",
            "epoch 2610: train D loss: 0.6849, train F loss: -0.6578, acc 0.9948\n",
            "epoch 2611: train D loss: 0.6837, train F loss: -0.6599, acc 0.9956\n",
            "epoch 2612: train D loss: 0.6829, train F loss: -0.6619, acc 0.9968\n",
            "epoch 2613: train D loss: 0.6837, train F loss: -0.6551, acc 0.9940\n",
            "epoch 2614: train D loss: 0.6860, train F loss: -0.6374, acc 0.9894\n",
            "epoch 2615: train D loss: 0.6816, train F loss: -0.6455, acc 0.9922\n",
            "epoch 2616: train D loss: 0.6896, train F loss: -0.6658, acc 0.9954\n",
            "epoch 2617: train D loss: 0.6881, train F loss: -0.6643, acc 0.9952\n",
            "epoch 2618: train D loss: 0.6837, train F loss: -0.6629, acc 0.9956\n",
            "epoch 2619: train D loss: 0.6865, train F loss: -0.6625, acc 0.9952\n",
            "epoch 2620: train D loss: 0.6819, train F loss: -0.6255, acc 0.9908\n",
            "epoch 2621: train D loss: 0.6820, train F loss: -0.6529, acc 0.9948\n",
            "epoch 2622: train D loss: 0.6835, train F loss: -0.6578, acc 0.9944\n",
            "epoch 2623: train D loss: 0.6836, train F loss: -0.6592, acc 0.9958\n",
            "epoch 2624: train D loss: 0.6798, train F loss: -0.6495, acc 0.9952\n",
            "epoch 2625: train D loss: 0.6854, train F loss: -0.6580, acc 0.9952\n",
            "epoch 2626: train D loss: 0.6836, train F loss: -0.6599, acc 0.9950\n",
            "epoch 2627: train D loss: 0.6826, train F loss: -0.6608, acc 0.9950\n",
            "epoch 2628: train D loss: 0.6827, train F loss: -0.6619, acc 0.9962\n",
            "epoch 2629: train D loss: 0.6845, train F loss: -0.6599, acc 0.9956\n",
            "epoch 2630: train D loss: 0.6820, train F loss: -0.6568, acc 0.9948\n",
            "epoch 2631: train D loss: 0.6841, train F loss: -0.6617, acc 0.9960\n",
            "epoch 2632: train D loss: 0.6889, train F loss: -0.6551, acc 0.9938\n",
            "epoch 2633: train D loss: 0.6850, train F loss: -0.6586, acc 0.9944\n",
            "epoch 2634: train D loss: 0.6863, train F loss: -0.6675, acc 0.9964\n",
            "epoch 2635: train D loss: 0.6847, train F loss: -0.6645, acc 0.9952\n",
            "epoch 2636: train D loss: 0.6837, train F loss: -0.6608, acc 0.9952\n",
            "epoch 2637: train D loss: 0.6870, train F loss: -0.6616, acc 0.9956\n",
            "epoch 2638: train D loss: 0.6864, train F loss: -0.6596, acc 0.9936\n",
            "epoch 2639: train D loss: 0.6828, train F loss: -0.6509, acc 0.9946\n",
            "epoch 2640: train D loss: 0.6830, train F loss: -0.6587, acc 0.9960\n",
            "epoch 2641: train D loss: 0.6855, train F loss: -0.6622, acc 0.9956\n",
            "epoch 2642: train D loss: 0.6870, train F loss: -0.6564, acc 0.9940\n",
            "epoch 2643: train D loss: 0.6837, train F loss: -0.6624, acc 0.9956\n",
            "epoch 2644: train D loss: 0.6837, train F loss: -0.6620, acc 0.9956\n",
            "epoch 2645: train D loss: 0.6845, train F loss: -0.6609, acc 0.9954\n",
            "epoch 2646: train D loss: 0.6863, train F loss: -0.6582, acc 0.9936\n",
            "epoch 2647: train D loss: 0.6852, train F loss: -0.6595, acc 0.9940\n",
            "epoch 2648: train D loss: 0.6855, train F loss: -0.6645, acc 0.9956\n",
            "epoch 2649: train D loss: 0.6892, train F loss: -0.6652, acc 0.9948\n",
            "epoch 2650: train D loss: 0.6850, train F loss: -0.6634, acc 0.9962\n",
            "epoch 2651: train D loss: 0.6871, train F loss: -0.6653, acc 0.9956\n",
            "epoch 2652: train D loss: 0.6878, train F loss: -0.6602, acc 0.9932\n",
            "epoch 2653: train D loss: 0.6819, train F loss: -0.6422, acc 0.9918\n",
            "epoch 2654: train D loss: 0.6841, train F loss: -0.6588, acc 0.9938\n",
            "epoch 2655: train D loss: 0.6818, train F loss: -0.6497, acc 0.9930\n",
            "epoch 2656: train D loss: 0.6825, train F loss: -0.6557, acc 0.9948\n",
            "epoch 2657: train D loss: 0.6822, train F loss: -0.6555, acc 0.9942\n",
            "epoch 2658: train D loss: 0.6855, train F loss: -0.6585, acc 0.9948\n",
            "epoch 2659: train D loss: 0.6852, train F loss: -0.6555, acc 0.9926\n",
            "epoch 2660: train D loss: 0.6834, train F loss: -0.6625, acc 0.9954\n",
            "epoch 2661: train D loss: 0.6862, train F loss: -0.6620, acc 0.9956\n",
            "epoch 2662: train D loss: 0.6838, train F loss: -0.6630, acc 0.9958\n",
            "epoch 2663: train D loss: 0.6871, train F loss: -0.6657, acc 0.9956\n",
            "epoch 2664: train D loss: 0.6839, train F loss: -0.6546, acc 0.9932\n",
            "epoch 2665: train D loss: 0.6821, train F loss: -0.6624, acc 0.9958\n",
            "epoch 2666: train D loss: 0.6820, train F loss: -0.6581, acc 0.9950\n",
            "epoch 2667: train D loss: 0.6822, train F loss: -0.6522, acc 0.9932\n",
            "epoch 2668: train D loss: 0.6868, train F loss: -0.6643, acc 0.9958\n",
            "epoch 2669: train D loss: 0.6853, train F loss: -0.6626, acc 0.9956\n",
            "epoch 2670: train D loss: 0.6859, train F loss: -0.6605, acc 0.9944\n",
            "epoch 2671: train D loss: 0.6818, train F loss: -0.6529, acc 0.9932\n",
            "epoch 2672: train D loss: 0.6855, train F loss: -0.6581, acc 0.9942\n",
            "epoch 2673: train D loss: 0.6832, train F loss: -0.6642, acc 0.9960\n",
            "epoch 2674: train D loss: 0.6828, train F loss: -0.6502, acc 0.9930\n",
            "epoch 2675: train D loss: 0.6824, train F loss: -0.6576, acc 0.9948\n",
            "epoch 2676: train D loss: 0.6847, train F loss: -0.6482, acc 0.9934\n",
            "epoch 2677: train D loss: 0.6842, train F loss: -0.6515, acc 0.9930\n",
            "epoch 2678: train D loss: 0.6813, train F loss: -0.6503, acc 0.9932\n",
            "epoch 2679: train D loss: 0.6850, train F loss: -0.6561, acc 0.9932\n",
            "epoch 2680: train D loss: 0.6853, train F loss: -0.6635, acc 0.9952\n",
            "epoch 2681: train D loss: 0.6822, train F loss: -0.6408, acc 0.9920\n",
            "epoch 2682: train D loss: 0.6827, train F loss: -0.6546, acc 0.9938\n",
            "epoch 2683: train D loss: 0.6831, train F loss: -0.6354, acc 0.9908\n",
            "epoch 2684: train D loss: 0.6804, train F loss: -0.6415, acc 0.9928\n",
            "epoch 2685: train D loss: 0.6819, train F loss: -0.6579, acc 0.9958\n",
            "epoch 2686: train D loss: 0.6779, train F loss: -0.6565, acc 0.9952\n",
            "epoch 2687: train D loss: 0.6825, train F loss: -0.6566, acc 0.9950\n",
            "epoch 2688: train D loss: 0.6842, train F loss: -0.6546, acc 0.9946\n",
            "epoch 2689: train D loss: 0.6832, train F loss: -0.6611, acc 0.9956\n",
            "epoch 2690: train D loss: 0.6845, train F loss: -0.6656, acc 0.9966\n",
            "epoch 2691: train D loss: 0.6870, train F loss: -0.6660, acc 0.9962\n",
            "epoch 2692: train D loss: 0.6829, train F loss: -0.6628, acc 0.9956\n",
            "epoch 2693: train D loss: 0.6825, train F loss: -0.6639, acc 0.9968\n",
            "epoch 2694: train D loss: 0.6854, train F loss: -0.6626, acc 0.9956\n",
            "epoch 2695: train D loss: 0.6836, train F loss: -0.6618, acc 0.9958\n",
            "epoch 2696: train D loss: 0.6850, train F loss: -0.6650, acc 0.9962\n",
            "epoch 2697: train D loss: 0.6867, train F loss: -0.6661, acc 0.9960\n",
            "epoch 2698: train D loss: 0.6885, train F loss: -0.6626, acc 0.9946\n",
            "epoch 2699: train D loss: 0.6856, train F loss: -0.6611, acc 0.9960\n",
            "epoch 2700: train D loss: 0.6848, train F loss: -0.6665, acc 0.9972\n",
            "epoch 2701: train D loss: 0.6867, train F loss: -0.6655, acc 0.9970\n",
            "epoch 2702: train D loss: 0.6880, train F loss: -0.6483, acc 0.9920\n",
            "epoch 2703: train D loss: 0.6842, train F loss: -0.6517, acc 0.9928\n",
            "epoch 2704: train D loss: 0.6843, train F loss: -0.6639, acc 0.9956\n",
            "epoch 2705: train D loss: 0.6825, train F loss: -0.6561, acc 0.9940\n",
            "epoch 2706: train D loss: 0.6840, train F loss: -0.6590, acc 0.9956\n",
            "epoch 2707: train D loss: 0.6833, train F loss: -0.6602, acc 0.9958\n",
            "epoch 2708: train D loss: 0.6845, train F loss: -0.6591, acc 0.9934\n",
            "epoch 2709: train D loss: 0.6832, train F loss: -0.6402, acc 0.9912\n",
            "epoch 2710: train D loss: 0.6868, train F loss: -0.6531, acc 0.9924\n",
            "epoch 2711: train D loss: 0.6862, train F loss: -0.6570, acc 0.9934\n",
            "epoch 2712: train D loss: 0.6841, train F loss: -0.6463, acc 0.9932\n",
            "epoch 2713: train D loss: 0.6802, train F loss: -0.6509, acc 0.9928\n",
            "epoch 2714: train D loss: 0.6826, train F loss: -0.6458, acc 0.9924\n",
            "epoch 2715: train D loss: 0.6804, train F loss: -0.6614, acc 0.9964\n",
            "epoch 2716: train D loss: 0.6845, train F loss: -0.6563, acc 0.9948\n",
            "epoch 2717: train D loss: 0.6824, train F loss: -0.6558, acc 0.9942\n",
            "epoch 2718: train D loss: 0.6833, train F loss: -0.6615, acc 0.9958\n",
            "epoch 2719: train D loss: 0.6830, train F loss: -0.6648, acc 0.9960\n",
            "epoch 2720: train D loss: 0.6875, train F loss: -0.6664, acc 0.9952\n",
            "epoch 2721: train D loss: 0.6854, train F loss: -0.6657, acc 0.9960\n",
            "epoch 2722: train D loss: 0.6870, train F loss: -0.6648, acc 0.9954\n",
            "epoch 2723: train D loss: 0.6800, train F loss: -0.6613, acc 0.9960\n",
            "epoch 2724: train D loss: 0.6826, train F loss: -0.6615, acc 0.9956\n",
            "epoch 2725: train D loss: 0.6851, train F loss: -0.6566, acc 0.9944\n",
            "epoch 2726: train D loss: 0.6886, train F loss: -0.6612, acc 0.9942\n",
            "epoch 2727: train D loss: 0.6853, train F loss: -0.6621, acc 0.9956\n",
            "epoch 2728: train D loss: 0.6855, train F loss: -0.6664, acc 0.9960\n",
            "epoch 2729: train D loss: 0.6851, train F loss: -0.6535, acc 0.9932\n",
            "epoch 2730: train D loss: 0.6860, train F loss: -0.6484, acc 0.9916\n",
            "epoch 2731: train D loss: 0.6852, train F loss: -0.6630, acc 0.9950\n",
            "epoch 2732: train D loss: 0.6809, train F loss: -0.6585, acc 0.9952\n",
            "epoch 2733: train D loss: 0.6860, train F loss: -0.6656, acc 0.9958\n",
            "epoch 2734: train D loss: 0.6839, train F loss: -0.6600, acc 0.9952\n",
            "epoch 2735: train D loss: 0.6834, train F loss: -0.6561, acc 0.9952\n",
            "epoch 2736: train D loss: 0.6840, train F loss: -0.6652, acc 0.9960\n",
            "epoch 2737: train D loss: 0.6841, train F loss: -0.6595, acc 0.9952\n",
            "epoch 2738: train D loss: 0.6871, train F loss: -0.6636, acc 0.9942\n",
            "epoch 2739: train D loss: 0.6828, train F loss: -0.6600, acc 0.9952\n",
            "epoch 2740: train D loss: 0.6844, train F loss: -0.6606, acc 0.9950\n",
            "epoch 2741: train D loss: 0.6846, train F loss: -0.6609, acc 0.9958\n",
            "epoch 2742: train D loss: 0.6847, train F loss: -0.6433, acc 0.9936\n",
            "epoch 2743: train D loss: 0.6830, train F loss: -0.6519, acc 0.9936\n",
            "epoch 2744: train D loss: 0.6841, train F loss: -0.6563, acc 0.9944\n",
            "epoch 2745: train D loss: 0.6847, train F loss: -0.6638, acc 0.9958\n",
            "epoch 2746: train D loss: 0.6851, train F loss: -0.6617, acc 0.9954\n",
            "epoch 2747: train D loss: 0.6850, train F loss: -0.6667, acc 0.9964\n",
            "epoch 2748: train D loss: 0.6835, train F loss: -0.6610, acc 0.9950\n",
            "epoch 2749: train D loss: 0.6865, train F loss: -0.6632, acc 0.9948\n",
            "epoch 2750: train D loss: 0.6877, train F loss: -0.6544, acc 0.9938\n",
            "epoch 2751: train D loss: 0.6833, train F loss: -0.6613, acc 0.9948\n",
            "epoch 2752: train D loss: 0.6849, train F loss: -0.6328, acc 0.9916\n",
            "epoch 2753: train D loss: 0.6838, train F loss: -0.6537, acc 0.9932\n",
            "epoch 2754: train D loss: 0.6848, train F loss: -0.6647, acc 0.9962\n",
            "epoch 2755: train D loss: 0.6850, train F loss: -0.6601, acc 0.9940\n",
            "epoch 2756: train D loss: 0.6851, train F loss: -0.6440, acc 0.9924\n",
            "epoch 2757: train D loss: 0.6806, train F loss: -0.6572, acc 0.9938\n",
            "epoch 2758: train D loss: 0.6797, train F loss: -0.6588, acc 0.9956\n",
            "epoch 2759: train D loss: 0.6787, train F loss: -0.6575, acc 0.9958\n",
            "epoch 2760: train D loss: 0.6858, train F loss: -0.6562, acc 0.9940\n",
            "epoch 2761: train D loss: 0.6839, train F loss: -0.6540, acc 0.9936\n",
            "epoch 2762: train D loss: 0.6834, train F loss: -0.6555, acc 0.9942\n",
            "epoch 2763: train D loss: 0.6815, train F loss: -0.6550, acc 0.9944\n",
            "epoch 2764: train D loss: 0.6832, train F loss: -0.6606, acc 0.9956\n",
            "epoch 2765: train D loss: 0.6844, train F loss: -0.6602, acc 0.9952\n",
            "epoch 2766: train D loss: 0.6848, train F loss: -0.6632, acc 0.9962\n",
            "epoch 2767: train D loss: 0.6880, train F loss: -0.6633, acc 0.9950\n",
            "epoch 2768: train D loss: 0.6852, train F loss: -0.6609, acc 0.9944\n",
            "epoch 2769: train D loss: 0.6834, train F loss: -0.6636, acc 0.9952\n",
            "epoch 2770: train D loss: 0.6876, train F loss: -0.6633, acc 0.9950\n",
            "epoch 2771: train D loss: 0.6884, train F loss: -0.6595, acc 0.9930\n",
            "epoch 2772: train D loss: 0.6867, train F loss: -0.6656, acc 0.9954\n",
            "epoch 2773: train D loss: 0.6843, train F loss: -0.6666, acc 0.9966\n",
            "epoch 2774: train D loss: 0.6864, train F loss: -0.6654, acc 0.9960\n",
            "epoch 2775: train D loss: 0.6845, train F loss: -0.6561, acc 0.9944\n",
            "epoch 2776: train D loss: 0.6853, train F loss: -0.5759, acc 0.9816\n",
            "epoch 2777: train D loss: 0.6839, train F loss: -0.6526, acc 0.9924\n",
            "epoch 2778: train D loss: 0.6828, train F loss: -0.6533, acc 0.9938\n",
            "epoch 2779: train D loss: 0.6838, train F loss: -0.6643, acc 0.9962\n",
            "epoch 2780: train D loss: 0.6818, train F loss: -0.6608, acc 0.9954\n",
            "epoch 2781: train D loss: 0.6803, train F loss: -0.6623, acc 0.9958\n",
            "epoch 2782: train D loss: 0.6821, train F loss: -0.6592, acc 0.9950\n",
            "epoch 2783: train D loss: 0.6860, train F loss: -0.5843, acc 0.9862\n",
            "epoch 2784: train D loss: 0.6834, train F loss: -0.6500, acc 0.9920\n",
            "epoch 2785: train D loss: 0.6836, train F loss: -0.6582, acc 0.9942\n",
            "epoch 2786: train D loss: 0.6828, train F loss: -0.6602, acc 0.9954\n",
            "epoch 2787: train D loss: 0.6801, train F loss: -0.6583, acc 0.9960\n",
            "epoch 2788: train D loss: 0.6819, train F loss: -0.6509, acc 0.9936\n",
            "epoch 2789: train D loss: 0.6837, train F loss: -0.6615, acc 0.9960\n",
            "epoch 2790: train D loss: 0.6855, train F loss: -0.6636, acc 0.9962\n",
            "epoch 2791: train D loss: 0.6813, train F loss: -0.6602, acc 0.9952\n",
            "epoch 2792: train D loss: 0.6842, train F loss: -0.6658, acc 0.9958\n",
            "epoch 2793: train D loss: 0.6846, train F loss: -0.6612, acc 0.9948\n",
            "epoch 2794: train D loss: 0.6835, train F loss: -0.6638, acc 0.9954\n",
            "epoch 2795: train D loss: 0.6845, train F loss: -0.6627, acc 0.9960\n",
            "epoch 2796: train D loss: 0.6841, train F loss: -0.6650, acc 0.9964\n",
            "epoch 2797: train D loss: 0.6841, train F loss: -0.6598, acc 0.9946\n",
            "epoch 2798: train D loss: 0.6844, train F loss: -0.6617, acc 0.9956\n",
            "epoch 2799: train D loss: 0.6828, train F loss: -0.6609, acc 0.9952\n",
            "epoch 2800: train D loss: 0.6837, train F loss: -0.6658, acc 0.9960\n",
            "epoch 2801: train D loss: 0.6851, train F loss: -0.6661, acc 0.9958\n",
            "epoch 2802: train D loss: 0.6838, train F loss: -0.6651, acc 0.9962\n",
            "epoch 2803: train D loss: 0.6859, train F loss: -0.6667, acc 0.9964\n",
            "epoch 2804: train D loss: 0.6830, train F loss: -0.6659, acc 0.9962\n",
            "epoch 2805: train D loss: 0.6850, train F loss: -0.6586, acc 0.9948\n",
            "epoch 2806: train D loss: 0.6831, train F loss: -0.6606, acc 0.9960\n",
            "epoch 2807: train D loss: 0.6880, train F loss: -0.6655, acc 0.9954\n",
            "epoch 2808: train D loss: 0.6867, train F loss: -0.6646, acc 0.9954\n",
            "epoch 2809: train D loss: 0.6859, train F loss: -0.6622, acc 0.9956\n",
            "epoch 2810: train D loss: 0.6902, train F loss: -0.6736, acc 0.9960\n",
            "epoch 2811: train D loss: 0.6867, train F loss: -0.6695, acc 0.9966\n",
            "epoch 2812: train D loss: 0.6866, train F loss: -0.6645, acc 0.9950\n",
            "epoch 2813: train D loss: 0.6884, train F loss: -0.6407, acc 0.9912\n",
            "epoch 2814: train D loss: 0.6813, train F loss: -0.6466, acc 0.9908\n",
            "epoch 2815: train D loss: 0.6867, train F loss: -0.6589, acc 0.9930\n",
            "epoch 2816: train D loss: 0.6832, train F loss: -0.6575, acc 0.9948\n",
            "epoch 2817: train D loss: 0.6817, train F loss: -0.6615, acc 0.9956\n",
            "epoch 2818: train D loss: 0.6850, train F loss: -0.6698, acc 0.9972\n",
            "epoch 2819: train D loss: 0.6848, train F loss: -0.6641, acc 0.9954\n",
            "epoch 2820: train D loss: 0.6849, train F loss: -0.6675, acc 0.9970\n",
            "epoch 2821: train D loss: 0.6850, train F loss: -0.6681, acc 0.9964\n",
            "epoch 2822: train D loss: 0.6879, train F loss: -0.6712, acc 0.9968\n",
            "epoch 2823: train D loss: 0.6849, train F loss: -0.6660, acc 0.9962\n",
            "epoch 2824: train D loss: 0.6848, train F loss: -0.6639, acc 0.9960\n",
            "epoch 2825: train D loss: 0.6845, train F loss: -0.6582, acc 0.9946\n",
            "epoch 2826: train D loss: 0.6853, train F loss: -0.6619, acc 0.9946\n",
            "epoch 2827: train D loss: 0.6852, train F loss: -0.6639, acc 0.9956\n",
            "epoch 2828: train D loss: 0.6858, train F loss: -0.6548, acc 0.9928\n",
            "epoch 2829: train D loss: 0.6848, train F loss: -0.6533, acc 0.9926\n",
            "epoch 2830: train D loss: 0.6809, train F loss: -0.6594, acc 0.9946\n",
            "epoch 2831: train D loss: 0.6858, train F loss: -0.6619, acc 0.9950\n",
            "epoch 2832: train D loss: 0.6871, train F loss: -0.6674, acc 0.9960\n",
            "epoch 2833: train D loss: 0.6851, train F loss: -0.6635, acc 0.9950\n",
            "epoch 2834: train D loss: 0.6872, train F loss: -0.6629, acc 0.9956\n",
            "epoch 2835: train D loss: 0.6853, train F loss: -0.6666, acc 0.9964\n",
            "epoch 2836: train D loss: 0.6878, train F loss: -0.6648, acc 0.9946\n",
            "epoch 2837: train D loss: 0.6861, train F loss: -0.6514, acc 0.9944\n",
            "epoch 2838: train D loss: 0.6838, train F loss: -0.6453, acc 0.9928\n",
            "epoch 2839: train D loss: 0.6843, train F loss: -0.6624, acc 0.9944\n",
            "epoch 2840: train D loss: 0.6829, train F loss: -0.6586, acc 0.9952\n",
            "epoch 2841: train D loss: 0.6834, train F loss: -0.6558, acc 0.9946\n",
            "epoch 2842: train D loss: 0.6846, train F loss: -0.6617, acc 0.9952\n",
            "epoch 2843: train D loss: 0.6871, train F loss: -0.6432, acc 0.9912\n",
            "epoch 2844: train D loss: 0.6831, train F loss: -0.6607, acc 0.9948\n",
            "epoch 2845: train D loss: 0.6869, train F loss: -0.6695, acc 0.9966\n",
            "epoch 2846: train D loss: 0.6839, train F loss: -0.6649, acc 0.9962\n",
            "epoch 2847: train D loss: 0.6848, train F loss: -0.6607, acc 0.9946\n",
            "epoch 2848: train D loss: 0.6865, train F loss: -0.6629, acc 0.9946\n",
            "epoch 2849: train D loss: 0.6836, train F loss: -0.6659, acc 0.9968\n",
            "epoch 2850: train D loss: 0.6824, train F loss: -0.6558, acc 0.9948\n",
            "epoch 2851: train D loss: 0.6863, train F loss: -0.6642, acc 0.9954\n",
            "epoch 2852: train D loss: 0.6863, train F loss: -0.6572, acc 0.9946\n",
            "epoch 2853: train D loss: 0.6860, train F loss: -0.6673, acc 0.9964\n",
            "epoch 2854: train D loss: 0.6843, train F loss: -0.6644, acc 0.9956\n",
            "epoch 2855: train D loss: 0.6860, train F loss: -0.6685, acc 0.9964\n",
            "epoch 2856: train D loss: 0.6858, train F loss: -0.6654, acc 0.9958\n",
            "epoch 2857: train D loss: 0.6836, train F loss: -0.6632, acc 0.9954\n",
            "epoch 2858: train D loss: 0.6847, train F loss: -0.6630, acc 0.9950\n",
            "epoch 2859: train D loss: 0.6843, train F loss: -0.6674, acc 0.9962\n",
            "epoch 2860: train D loss: 0.6843, train F loss: -0.6617, acc 0.9958\n",
            "epoch 2861: train D loss: 0.6865, train F loss: -0.6654, acc 0.9950\n",
            "epoch 2862: train D loss: 0.6891, train F loss: -0.6642, acc 0.9942\n",
            "epoch 2863: train D loss: 0.6862, train F loss: -0.6558, acc 0.9938\n",
            "epoch 2864: train D loss: 0.6867, train F loss: -0.6571, acc 0.9928\n",
            "epoch 2865: train D loss: 0.6822, train F loss: -0.6587, acc 0.9944\n",
            "epoch 2866: train D loss: 0.6819, train F loss: -0.6532, acc 0.9946\n",
            "epoch 2867: train D loss: 0.6853, train F loss: -0.6656, acc 0.9952\n",
            "epoch 2868: train D loss: 0.6852, train F loss: -0.6608, acc 0.9946\n",
            "epoch 2869: train D loss: 0.6867, train F loss: -0.6652, acc 0.9942\n",
            "epoch 2870: train D loss: 0.6842, train F loss: -0.6626, acc 0.9954\n",
            "epoch 2871: train D loss: 0.6855, train F loss: -0.6613, acc 0.9956\n",
            "epoch 2872: train D loss: 0.6875, train F loss: -0.6542, acc 0.9924\n",
            "epoch 2873: train D loss: 0.6873, train F loss: -0.6688, acc 0.9956\n",
            "epoch 2874: train D loss: 0.6890, train F loss: -0.6692, acc 0.9950\n",
            "epoch 2875: train D loss: 0.6856, train F loss: -0.6715, acc 0.9970\n",
            "epoch 2876: train D loss: 0.6864, train F loss: -0.6682, acc 0.9966\n",
            "epoch 2877: train D loss: 0.6882, train F loss: -0.6592, acc 0.9936\n",
            "epoch 2878: train D loss: 0.6851, train F loss: -0.6569, acc 0.9936\n",
            "epoch 2879: train D loss: 0.6847, train F loss: -0.6639, acc 0.9958\n",
            "epoch 2880: train D loss: 0.6834, train F loss: -0.6684, acc 0.9972\n",
            "epoch 2881: train D loss: 0.6841, train F loss: -0.6632, acc 0.9960\n",
            "epoch 2882: train D loss: 0.6852, train F loss: -0.6585, acc 0.9940\n",
            "epoch 2883: train D loss: 0.6853, train F loss: -0.6569, acc 0.9938\n",
            "epoch 2884: train D loss: 0.6879, train F loss: -0.6675, acc 0.9956\n",
            "epoch 2885: train D loss: 0.6853, train F loss: -0.6668, acc 0.9954\n",
            "epoch 2886: train D loss: 0.6821, train F loss: -0.6682, acc 0.9974\n",
            "epoch 2887: train D loss: 0.6864, train F loss: -0.6554, acc 0.9944\n",
            "epoch 2888: train D loss: 0.6820, train F loss: -0.6585, acc 0.9940\n",
            "epoch 2889: train D loss: 0.6829, train F loss: -0.6570, acc 0.9954\n",
            "epoch 2890: train D loss: 0.6844, train F loss: -0.6594, acc 0.9948\n",
            "epoch 2891: train D loss: 0.6850, train F loss: -0.6665, acc 0.9950\n",
            "epoch 2892: train D loss: 0.6825, train F loss: -0.6594, acc 0.9946\n",
            "epoch 2893: train D loss: 0.6804, train F loss: -0.6517, acc 0.9930\n",
            "epoch 2894: train D loss: 0.6873, train F loss: -0.6443, acc 0.9926\n",
            "epoch 2895: train D loss: 0.6854, train F loss: -0.6536, acc 0.9932\n",
            "epoch 2896: train D loss: 0.6826, train F loss: -0.6590, acc 0.9954\n",
            "epoch 2897: train D loss: 0.6875, train F loss: -0.6668, acc 0.9956\n",
            "epoch 2898: train D loss: 0.6876, train F loss: -0.6321, acc 0.9896\n",
            "epoch 2899: train D loss: 0.6854, train F loss: -0.6609, acc 0.9930\n",
            "epoch 2900: train D loss: 0.6854, train F loss: -0.6594, acc 0.9946\n",
            "epoch 2901: train D loss: 0.6827, train F loss: -0.6567, acc 0.9936\n",
            "epoch 2902: train D loss: 0.6826, train F loss: -0.6609, acc 0.9946\n",
            "epoch 2903: train D loss: 0.6871, train F loss: -0.6177, acc 0.9904\n",
            "epoch 2904: train D loss: 0.6875, train F loss: -0.6633, acc 0.9940\n",
            "epoch 2905: train D loss: 0.6803, train F loss: -0.6592, acc 0.9952\n",
            "epoch 2906: train D loss: 0.6823, train F loss: -0.6588, acc 0.9948\n",
            "epoch 2907: train D loss: 0.6809, train F loss: -0.6619, acc 0.9956\n",
            "epoch 2908: train D loss: 0.6836, train F loss: -0.6639, acc 0.9948\n",
            "epoch 2909: train D loss: 0.6843, train F loss: -0.6670, acc 0.9958\n",
            "epoch 2910: train D loss: 0.6852, train F loss: -0.6630, acc 0.9960\n",
            "epoch 2911: train D loss: 0.6856, train F loss: -0.6680, acc 0.9958\n",
            "epoch 2912: train D loss: 0.6831, train F loss: -0.6628, acc 0.9956\n",
            "epoch 2913: train D loss: 0.6855, train F loss: -0.6679, acc 0.9958\n",
            "epoch 2914: train D loss: 0.6874, train F loss: -0.6629, acc 0.9950\n",
            "epoch 2915: train D loss: 0.6860, train F loss: -0.6590, acc 0.9936\n",
            "epoch 2916: train D loss: 0.6861, train F loss: -0.6618, acc 0.9948\n",
            "epoch 2917: train D loss: 0.6843, train F loss: -0.6675, acc 0.9964\n",
            "epoch 2918: train D loss: 0.6867, train F loss: -0.6589, acc 0.9948\n",
            "epoch 2919: train D loss: 0.6843, train F loss: -0.6538, acc 0.9924\n",
            "epoch 2920: train D loss: 0.6865, train F loss: -0.6650, acc 0.9954\n",
            "epoch 2921: train D loss: 0.6828, train F loss: -0.6658, acc 0.9958\n",
            "epoch 2922: train D loss: 0.6854, train F loss: -0.6646, acc 0.9956\n",
            "epoch 2923: train D loss: 0.6838, train F loss: -0.6637, acc 0.9958\n",
            "epoch 2924: train D loss: 0.6812, train F loss: -0.6534, acc 0.9942\n",
            "epoch 2925: train D loss: 0.6810, train F loss: -0.6523, acc 0.9928\n",
            "epoch 2926: train D loss: 0.6857, train F loss: -0.6512, acc 0.9932\n",
            "epoch 2927: train D loss: 0.6818, train F loss: -0.6597, acc 0.9946\n",
            "epoch 2928: train D loss: 0.6843, train F loss: -0.6610, acc 0.9952\n",
            "epoch 2929: train D loss: 0.6869, train F loss: -0.6603, acc 0.9944\n",
            "epoch 2930: train D loss: 0.6834, train F loss: -0.6118, acc 0.9878\n",
            "epoch 2931: train D loss: 0.6861, train F loss: -0.6568, acc 0.9936\n",
            "epoch 2932: train D loss: 0.6810, train F loss: -0.6596, acc 0.9950\n",
            "epoch 2933: train D loss: 0.6840, train F loss: -0.6541, acc 0.9948\n",
            "epoch 2934: train D loss: 0.6809, train F loss: -0.6546, acc 0.9946\n",
            "epoch 2935: train D loss: 0.6838, train F loss: -0.6645, acc 0.9960\n",
            "epoch 2936: train D loss: 0.6837, train F loss: -0.6670, acc 0.9954\n",
            "epoch 2937: train D loss: 0.6832, train F loss: -0.6653, acc 0.9968\n",
            "epoch 2938: train D loss: 0.6846, train F loss: -0.6637, acc 0.9950\n",
            "epoch 2939: train D loss: 0.6841, train F loss: -0.6584, acc 0.9944\n",
            "epoch 2940: train D loss: 0.6845, train F loss: -0.6643, acc 0.9950\n",
            "epoch 2941: train D loss: 0.6848, train F loss: -0.6706, acc 0.9970\n",
            "epoch 2942: train D loss: 0.6860, train F loss: -0.6713, acc 0.9968\n",
            "epoch 2943: train D loss: 0.6817, train F loss: -0.6587, acc 0.9954\n",
            "epoch 2944: train D loss: 0.6858, train F loss: -0.6617, acc 0.9936\n",
            "epoch 2945: train D loss: 0.6873, train F loss: -0.6567, acc 0.9928\n",
            "epoch 2946: train D loss: 0.6844, train F loss: -0.6653, acc 0.9956\n",
            "epoch 2947: train D loss: 0.6868, train F loss: -0.6573, acc 0.9936\n",
            "epoch 2948: train D loss: 0.6885, train F loss: -0.6652, acc 0.9940\n",
            "epoch 2949: train D loss: 0.6833, train F loss: -0.6649, acc 0.9962\n",
            "epoch 2950: train D loss: 0.6841, train F loss: -0.6500, acc 0.9938\n",
            "epoch 2951: train D loss: 0.6872, train F loss: -0.6645, acc 0.9946\n",
            "epoch 2952: train D loss: 0.6832, train F loss: -0.6649, acc 0.9964\n",
            "epoch 2953: train D loss: 0.6856, train F loss: -0.6676, acc 0.9962\n",
            "epoch 2954: train D loss: 0.6844, train F loss: -0.6626, acc 0.9950\n",
            "epoch 2955: train D loss: 0.6879, train F loss: -0.6652, acc 0.9948\n",
            "epoch 2956: train D loss: 0.6822, train F loss: -0.6605, acc 0.9948\n",
            "epoch 2957: train D loss: 0.6823, train F loss: -0.6584, acc 0.9958\n",
            "epoch 2958: train D loss: 0.6874, train F loss: -0.6603, acc 0.9944\n",
            "epoch 2959: train D loss: 0.6864, train F loss: -0.6502, acc 0.9920\n",
            "epoch 2960: train D loss: 0.6824, train F loss: -0.6610, acc 0.9946\n",
            "epoch 2961: train D loss: 0.6847, train F loss: -0.6452, acc 0.9914\n",
            "epoch 2962: train D loss: 0.6820, train F loss: -0.6583, acc 0.9948\n",
            "epoch 2963: train D loss: 0.6829, train F loss: -0.6617, acc 0.9948\n",
            "epoch 2964: train D loss: 0.6787, train F loss: -0.6563, acc 0.9958\n",
            "epoch 2965: train D loss: 0.6825, train F loss: -0.6635, acc 0.9962\n",
            "epoch 2966: train D loss: 0.6847, train F loss: -0.6666, acc 0.9960\n",
            "epoch 2967: train D loss: 0.6861, train F loss: -0.6663, acc 0.9948\n",
            "epoch 2968: train D loss: 0.6837, train F loss: -0.6617, acc 0.9952\n",
            "epoch 2969: train D loss: 0.6848, train F loss: -0.6571, acc 0.9948\n",
            "epoch 2970: train D loss: 0.6837, train F loss: -0.6668, acc 0.9968\n",
            "epoch 2971: train D loss: 0.6854, train F loss: -0.6578, acc 0.9946\n",
            "epoch 2972: train D loss: 0.6864, train F loss: -0.6610, acc 0.9936\n",
            "epoch 2973: train D loss: 0.6848, train F loss: -0.6644, acc 0.9948\n",
            "epoch 2974: train D loss: 0.6868, train F loss: -0.6636, acc 0.9936\n",
            "epoch 2975: train D loss: 0.6833, train F loss: -0.6647, acc 0.9958\n",
            "epoch 2976: train D loss: 0.6855, train F loss: -0.6689, acc 0.9968\n",
            "epoch 2977: train D loss: 0.6867, train F loss: -0.6661, acc 0.9952\n",
            "epoch 2978: train D loss: 0.6829, train F loss: -0.6641, acc 0.9954\n",
            "epoch 2979: train D loss: 0.6870, train F loss: -0.6715, acc 0.9960\n",
            "epoch 2980: train D loss: 0.6840, train F loss: -0.6686, acc 0.9962\n",
            "epoch 2981: train D loss: 0.6854, train F loss: -0.6473, acc 0.9914\n",
            "epoch 2982: train D loss: 0.6831, train F loss: -0.6547, acc 0.9930\n",
            "epoch 2983: train D loss: 0.6822, train F loss: -0.6604, acc 0.9950\n",
            "epoch 2984: train D loss: 0.6823, train F loss: -0.6601, acc 0.9954\n",
            "epoch 2985: train D loss: 0.6843, train F loss: -0.6659, acc 0.9962\n",
            "epoch 2986: train D loss: 0.6849, train F loss: -0.6664, acc 0.9954\n",
            "epoch 2987: train D loss: 0.6864, train F loss: -0.6661, acc 0.9954\n",
            "epoch 2988: train D loss: 0.6809, train F loss: -0.6632, acc 0.9960\n",
            "epoch 2989: train D loss: 0.6868, train F loss: -0.6663, acc 0.9962\n",
            "epoch 2990: train D loss: 0.6868, train F loss: -0.6657, acc 0.9946\n",
            "epoch 2991: train D loss: 0.6881, train F loss: -0.6680, acc 0.9950\n",
            "epoch 2992: train D loss: 0.6879, train F loss: -0.6690, acc 0.9964\n",
            "epoch 2993: train D loss: 0.6834, train F loss: -0.6577, acc 0.9938\n",
            "epoch 2994: train D loss: 0.6850, train F loss: -0.6609, acc 0.9946\n",
            "epoch 2995: train D loss: 0.6839, train F loss: -0.6651, acc 0.9954\n",
            "epoch 2996: train D loss: 0.6867, train F loss: -0.6637, acc 0.9942\n",
            "epoch 2997: train D loss: 0.6832, train F loss: -0.6598, acc 0.9944\n",
            "epoch 2998: train D loss: 0.6813, train F loss: -0.6618, acc 0.9954\n",
            "epoch 2999: train D loss: 0.6866, train F loss: -0.6666, acc 0.9948\n",
            "epoch 3000: train D loss: 0.6856, train F loss: -0.6640, acc 0.9954\n",
            "epoch 3001: train D loss: 0.6858, train F loss: -0.6698, acc 0.9958\n",
            "epoch 3002: train D loss: 0.6851, train F loss: -0.6673, acc 0.9958\n",
            "epoch 3003: train D loss: 0.6862, train F loss: -0.6455, acc 0.9924\n",
            "epoch 3004: train D loss: 0.6838, train F loss: -0.6586, acc 0.9924\n",
            "epoch 3005: train D loss: 0.6822, train F loss: -0.6634, acc 0.9958\n",
            "epoch 3006: train D loss: 0.6872, train F loss: -0.6670, acc 0.9950\n",
            "epoch 3007: train D loss: 0.6875, train F loss: -0.6650, acc 0.9946\n",
            "epoch 3008: train D loss: 0.6865, train F loss: -0.6635, acc 0.9948\n",
            "epoch 3009: train D loss: 0.6852, train F loss: -0.6695, acc 0.9962\n",
            "epoch 3010: train D loss: 0.6858, train F loss: -0.6575, acc 0.9932\n",
            "epoch 3011: train D loss: 0.6831, train F loss: -0.6637, acc 0.9950\n",
            "epoch 3012: train D loss: 0.6861, train F loss: -0.6664, acc 0.9956\n",
            "epoch 3013: train D loss: 0.6858, train F loss: -0.6676, acc 0.9954\n",
            "epoch 3014: train D loss: 0.6802, train F loss: -0.6573, acc 0.9954\n",
            "epoch 3015: train D loss: 0.6855, train F loss: -0.6596, acc 0.9942\n",
            "epoch 3016: train D loss: 0.6824, train F loss: -0.6636, acc 0.9954\n",
            "epoch 3017: train D loss: 0.6826, train F loss: -0.6351, acc 0.9928\n",
            "epoch 3018: train D loss: 0.6846, train F loss: -0.6554, acc 0.9944\n",
            "epoch 3019: train D loss: 0.6857, train F loss: -0.6653, acc 0.9954\n",
            "epoch 3020: train D loss: 0.6857, train F loss: -0.6706, acc 0.9966\n",
            "epoch 3021: train D loss: 0.6847, train F loss: -0.6323, acc 0.9918\n",
            "epoch 3022: train D loss: 0.6841, train F loss: -0.6630, acc 0.9946\n",
            "epoch 3023: train D loss: 0.6832, train F loss: -0.6638, acc 0.9960\n",
            "epoch 3024: train D loss: 0.6829, train F loss: -0.6636, acc 0.9944\n",
            "epoch 3025: train D loss: 0.6849, train F loss: -0.6619, acc 0.9942\n",
            "epoch 3026: train D loss: 0.6862, train F loss: -0.6691, acc 0.9962\n",
            "epoch 3027: train D loss: 0.6823, train F loss: -0.6619, acc 0.9950\n",
            "epoch 3028: train D loss: 0.6841, train F loss: -0.6564, acc 0.9938\n",
            "epoch 3029: train D loss: 0.6862, train F loss: -0.6580, acc 0.9932\n",
            "epoch 3030: train D loss: 0.6853, train F loss: -0.6676, acc 0.9956\n",
            "epoch 3031: train D loss: 0.6867, train F loss: -0.6632, acc 0.9948\n",
            "epoch 3032: train D loss: 0.6851, train F loss: -0.6591, acc 0.9934\n",
            "epoch 3033: train D loss: 0.6850, train F loss: -0.6550, acc 0.9930\n",
            "epoch 3034: train D loss: 0.6841, train F loss: -0.6624, acc 0.9948\n",
            "epoch 3035: train D loss: 0.6863, train F loss: -0.6649, acc 0.9956\n",
            "epoch 3036: train D loss: 0.6844, train F loss: -0.6646, acc 0.9952\n",
            "epoch 3037: train D loss: 0.6836, train F loss: -0.6662, acc 0.9958\n",
            "epoch 3038: train D loss: 0.6865, train F loss: -0.6694, acc 0.9964\n",
            "epoch 3039: train D loss: 0.6863, train F loss: -0.6708, acc 0.9960\n",
            "epoch 3040: train D loss: 0.6822, train F loss: -0.6633, acc 0.9958\n",
            "epoch 3041: train D loss: 0.6833, train F loss: -0.6640, acc 0.9964\n",
            "epoch 3042: train D loss: 0.6862, train F loss: -0.6694, acc 0.9948\n",
            "epoch 3043: train D loss: 0.6844, train F loss: -0.6573, acc 0.9942\n",
            "epoch 3044: train D loss: 0.6813, train F loss: -0.6619, acc 0.9954\n",
            "epoch 3045: train D loss: 0.6883, train F loss: -0.6757, acc 0.9976\n",
            "epoch 3046: train D loss: 0.6878, train F loss: -0.6625, acc 0.9950\n",
            "epoch 3047: train D loss: 0.6844, train F loss: -0.6702, acc 0.9966\n",
            "epoch 3048: train D loss: 0.6816, train F loss: -0.6644, acc 0.9956\n",
            "epoch 3049: train D loss: 0.6858, train F loss: -0.6565, acc 0.9936\n",
            "epoch 3050: train D loss: 0.6877, train F loss: -0.6635, acc 0.9942\n",
            "epoch 3051: train D loss: 0.6858, train F loss: -0.6653, acc 0.9944\n",
            "epoch 3052: train D loss: 0.6901, train F loss: -0.6670, acc 0.9944\n",
            "epoch 3053: train D loss: 0.6839, train F loss: -0.6579, acc 0.9936\n",
            "epoch 3054: train D loss: 0.6853, train F loss: -0.6649, acc 0.9956\n",
            "epoch 3055: train D loss: 0.6836, train F loss: -0.6634, acc 0.9946\n",
            "epoch 3056: train D loss: 0.6882, train F loss: -0.6709, acc 0.9958\n",
            "epoch 3057: train D loss: 0.6858, train F loss: -0.6688, acc 0.9958\n",
            "epoch 3058: train D loss: 0.6858, train F loss: -0.6671, acc 0.9956\n",
            "epoch 3059: train D loss: 0.6870, train F loss: -0.6676, acc 0.9950\n",
            "epoch 3060: train D loss: 0.6843, train F loss: -0.6570, acc 0.9924\n",
            "epoch 3061: train D loss: 0.6832, train F loss: -0.6603, acc 0.9948\n",
            "epoch 3062: train D loss: 0.6843, train F loss: -0.6651, acc 0.9964\n",
            "epoch 3063: train D loss: 0.6857, train F loss: -0.6693, acc 0.9970\n",
            "epoch 3064: train D loss: 0.6867, train F loss: -0.6630, acc 0.9956\n",
            "epoch 3065: train D loss: 0.6868, train F loss: -0.6668, acc 0.9956\n",
            "epoch 3066: train D loss: 0.6821, train F loss: -0.6660, acc 0.9950\n",
            "epoch 3067: train D loss: 0.6881, train F loss: -0.6737, acc 0.9968\n",
            "epoch 3068: train D loss: 0.6849, train F loss: -0.6605, acc 0.9942\n",
            "epoch 3069: train D loss: 0.6846, train F loss: -0.6571, acc 0.9936\n",
            "epoch 3070: train D loss: 0.6852, train F loss: -0.6621, acc 0.9942\n",
            "epoch 3071: train D loss: 0.6858, train F loss: -0.6547, acc 0.9936\n",
            "epoch 3072: train D loss: 0.6832, train F loss: -0.6656, acc 0.9956\n",
            "epoch 3073: train D loss: 0.6839, train F loss: -0.6649, acc 0.9954\n",
            "epoch 3074: train D loss: 0.6824, train F loss: -0.6628, acc 0.9954\n",
            "epoch 3075: train D loss: 0.6886, train F loss: -0.6704, acc 0.9964\n",
            "epoch 3076: train D loss: 0.6859, train F loss: -0.6683, acc 0.9962\n",
            "epoch 3077: train D loss: 0.6846, train F loss: -0.6671, acc 0.9960\n",
            "epoch 3078: train D loss: 0.6866, train F loss: -0.6694, acc 0.9958\n",
            "epoch 3079: train D loss: 0.6862, train F loss: -0.6568, acc 0.9928\n",
            "epoch 3080: train D loss: 0.6874, train F loss: -0.6632, acc 0.9934\n",
            "epoch 3081: train D loss: 0.6833, train F loss: -0.6612, acc 0.9952\n",
            "epoch 3082: train D loss: 0.6827, train F loss: -0.6626, acc 0.9958\n",
            "epoch 3083: train D loss: 0.6853, train F loss: -0.6665, acc 0.9958\n",
            "epoch 3084: train D loss: 0.6857, train F loss: -0.6605, acc 0.9944\n",
            "epoch 3085: train D loss: 0.6851, train F loss: -0.6610, acc 0.9944\n",
            "epoch 3086: train D loss: 0.6831, train F loss: -0.6636, acc 0.9950\n",
            "epoch 3087: train D loss: 0.6870, train F loss: -0.6602, acc 0.9932\n",
            "epoch 3088: train D loss: 0.6868, train F loss: -0.6654, acc 0.9960\n",
            "epoch 3089: train D loss: 0.6869, train F loss: -0.6664, acc 0.9948\n",
            "epoch 3090: train D loss: 0.6874, train F loss: -0.6658, acc 0.9948\n",
            "epoch 3091: train D loss: 0.6860, train F loss: -0.6534, acc 0.9938\n",
            "epoch 3092: train D loss: 0.6838, train F loss: -0.6669, acc 0.9956\n",
            "epoch 3093: train D loss: 0.6852, train F loss: -0.6684, acc 0.9960\n",
            "epoch 3094: train D loss: 0.6828, train F loss: -0.6677, acc 0.9964\n",
            "epoch 3095: train D loss: 0.6852, train F loss: -0.6677, acc 0.9952\n",
            "epoch 3096: train D loss: 0.6858, train F loss: -0.6699, acc 0.9966\n",
            "epoch 3097: train D loss: 0.6832, train F loss: -0.6617, acc 0.9952\n",
            "epoch 3098: train D loss: 0.6853, train F loss: -0.6528, acc 0.9920\n",
            "epoch 3099: train D loss: 0.6879, train F loss: -0.6670, acc 0.9954\n",
            "epoch 3100: train D loss: 0.6847, train F loss: -0.6648, acc 0.9950\n",
            "epoch 3101: train D loss: 0.6849, train F loss: -0.6673, acc 0.9956\n",
            "epoch 3102: train D loss: 0.6871, train F loss: -0.6703, acc 0.9954\n",
            "epoch 3103: train D loss: 0.6841, train F loss: -0.6646, acc 0.9962\n",
            "epoch 3104: train D loss: 0.6875, train F loss: -0.6665, acc 0.9948\n",
            "epoch 3105: train D loss: 0.6842, train F loss: -0.6683, acc 0.9962\n",
            "epoch 3106: train D loss: 0.6879, train F loss: -0.6702, acc 0.9962\n",
            "epoch 3107: train D loss: 0.6851, train F loss: -0.6665, acc 0.9950\n",
            "epoch 3108: train D loss: 0.6854, train F loss: -0.6613, acc 0.9952\n",
            "epoch 3109: train D loss: 0.6856, train F loss: -0.6422, acc 0.9892\n",
            "epoch 3110: train D loss: 0.6903, train F loss: -0.6671, acc 0.9948\n",
            "epoch 3111: train D loss: 0.6860, train F loss: -0.6666, acc 0.9942\n",
            "epoch 3112: train D loss: 0.6857, train F loss: -0.6638, acc 0.9942\n",
            "epoch 3113: train D loss: 0.6837, train F loss: -0.6671, acc 0.9958\n",
            "epoch 3114: train D loss: 0.6836, train F loss: -0.6570, acc 0.9942\n",
            "epoch 3115: train D loss: 0.6853, train F loss: -0.6689, acc 0.9962\n",
            "epoch 3116: train D loss: 0.6852, train F loss: -0.6679, acc 0.9954\n",
            "epoch 3117: train D loss: 0.6832, train F loss: -0.6649, acc 0.9958\n",
            "epoch 3118: train D loss: 0.6867, train F loss: -0.6599, acc 0.9944\n",
            "epoch 3119: train D loss: 0.6840, train F loss: -0.6626, acc 0.9950\n",
            "epoch 3120: train D loss: 0.6858, train F loss: -0.6669, acc 0.9958\n",
            "epoch 3121: train D loss: 0.6873, train F loss: -0.6686, acc 0.9962\n",
            "epoch 3122: train D loss: 0.6861, train F loss: -0.6657, acc 0.9954\n",
            "epoch 3123: train D loss: 0.6869, train F loss: -0.6658, acc 0.9942\n",
            "epoch 3124: train D loss: 0.6856, train F loss: -0.6674, acc 0.9948\n",
            "epoch 3125: train D loss: 0.6863, train F loss: -0.6621, acc 0.9948\n",
            "epoch 3126: train D loss: 0.6839, train F loss: -0.6536, acc 0.9930\n",
            "epoch 3127: train D loss: 0.6845, train F loss: -0.6646, acc 0.9950\n",
            "epoch 3128: train D loss: 0.6858, train F loss: -0.6635, acc 0.9946\n",
            "epoch 3129: train D loss: 0.6841, train F loss: -0.6678, acc 0.9962\n",
            "epoch 3130: train D loss: 0.6888, train F loss: -0.6757, acc 0.9972\n",
            "epoch 3131: train D loss: 0.6862, train F loss: -0.6676, acc 0.9952\n",
            "epoch 3132: train D loss: 0.6830, train F loss: -0.6442, acc 0.9916\n",
            "epoch 3133: train D loss: 0.6814, train F loss: -0.6539, acc 0.9934\n",
            "epoch 3134: train D loss: 0.6849, train F loss: -0.6641, acc 0.9944\n",
            "epoch 3135: train D loss: 0.6860, train F loss: -0.6669, acc 0.9946\n",
            "epoch 3136: train D loss: 0.6867, train F loss: -0.6680, acc 0.9960\n",
            "epoch 3137: train D loss: 0.6847, train F loss: -0.6644, acc 0.9960\n",
            "epoch 3138: train D loss: 0.6850, train F loss: -0.6664, acc 0.9948\n",
            "epoch 3139: train D loss: 0.6861, train F loss: -0.6610, acc 0.9946\n",
            "epoch 3140: train D loss: 0.6842, train F loss: -0.6654, acc 0.9948\n",
            "epoch 3141: train D loss: 0.6848, train F loss: -0.6665, acc 0.9968\n",
            "epoch 3142: train D loss: 0.6817, train F loss: -0.6571, acc 0.9950\n",
            "epoch 3143: train D loss: 0.6867, train F loss: -0.6607, acc 0.9936\n",
            "epoch 3144: train D loss: 0.6832, train F loss: -0.6685, acc 0.9966\n",
            "epoch 3145: train D loss: 0.6869, train F loss: -0.6635, acc 0.9950\n",
            "epoch 3146: train D loss: 0.6843, train F loss: -0.6661, acc 0.9950\n",
            "epoch 3147: train D loss: 0.6852, train F loss: -0.6672, acc 0.9956\n",
            "epoch 3148: train D loss: 0.6834, train F loss: -0.6560, acc 0.9946\n",
            "epoch 3149: train D loss: 0.6835, train F loss: -0.6653, acc 0.9960\n",
            "epoch 3150: train D loss: 0.6866, train F loss: -0.6553, acc 0.9930\n",
            "epoch 3151: train D loss: 0.6843, train F loss: -0.6651, acc 0.9950\n",
            "epoch 3152: train D loss: 0.6819, train F loss: -0.6600, acc 0.9962\n",
            "epoch 3153: train D loss: 0.6851, train F loss: -0.6661, acc 0.9958\n",
            "epoch 3154: train D loss: 0.6843, train F loss: -0.6656, acc 0.9956\n",
            "epoch 3155: train D loss: 0.6832, train F loss: -0.6431, acc 0.9916\n",
            "epoch 3156: train D loss: 0.6851, train F loss: -0.6648, acc 0.9952\n",
            "epoch 3157: train D loss: 0.6805, train F loss: -0.6575, acc 0.9944\n",
            "epoch 3158: train D loss: 0.6855, train F loss: -0.6697, acc 0.9968\n",
            "epoch 3159: train D loss: 0.6839, train F loss: -0.6634, acc 0.9952\n",
            "epoch 3160: train D loss: 0.6826, train F loss: -0.6567, acc 0.9934\n",
            "epoch 3161: train D loss: 0.6805, train F loss: -0.6583, acc 0.9948\n",
            "epoch 3162: train D loss: 0.6811, train F loss: -0.6616, acc 0.9962\n",
            "epoch 3163: train D loss: 0.6842, train F loss: -0.6662, acc 0.9958\n",
            "epoch 3164: train D loss: 0.6837, train F loss: -0.6700, acc 0.9970\n",
            "epoch 3165: train D loss: 0.6795, train F loss: -0.6583, acc 0.9944\n",
            "epoch 3166: train D loss: 0.6843, train F loss: -0.6405, acc 0.9916\n",
            "epoch 3167: train D loss: 0.6810, train F loss: -0.6561, acc 0.9946\n",
            "epoch 3168: train D loss: 0.6822, train F loss: -0.6548, acc 0.9948\n",
            "epoch 3169: train D loss: 0.6826, train F loss: -0.6619, acc 0.9942\n",
            "epoch 3170: train D loss: 0.6844, train F loss: -0.6611, acc 0.9960\n",
            "epoch 3171: train D loss: 0.6867, train F loss: -0.6669, acc 0.9948\n",
            "epoch 3172: train D loss: 0.6809, train F loss: -0.6649, acc 0.9964\n",
            "epoch 3173: train D loss: 0.6850, train F loss: -0.6633, acc 0.9958\n",
            "epoch 3174: train D loss: 0.6864, train F loss: -0.6677, acc 0.9964\n",
            "epoch 3175: train D loss: 0.6865, train F loss: -0.6669, acc 0.9954\n",
            "epoch 3176: train D loss: 0.6850, train F loss: -0.6614, acc 0.9936\n",
            "epoch 3177: train D loss: 0.6875, train F loss: -0.6644, acc 0.9934\n",
            "epoch 3178: train D loss: 0.6858, train F loss: -0.6628, acc 0.9936\n",
            "epoch 3179: train D loss: 0.6829, train F loss: -0.6687, acc 0.9966\n",
            "epoch 3180: train D loss: 0.6866, train F loss: -0.6679, acc 0.9954\n",
            "epoch 3181: train D loss: 0.6842, train F loss: -0.6692, acc 0.9966\n",
            "epoch 3182: train D loss: 0.6829, train F loss: -0.6645, acc 0.9950\n",
            "epoch 3183: train D loss: 0.6839, train F loss: -0.6668, acc 0.9962\n",
            "epoch 3184: train D loss: 0.6855, train F loss: -0.6648, acc 0.9958\n",
            "epoch 3185: train D loss: 0.6852, train F loss: -0.6682, acc 0.9950\n",
            "epoch 3186: train D loss: 0.6855, train F loss: -0.6673, acc 0.9954\n",
            "epoch 3187: train D loss: 0.6846, train F loss: -0.6674, acc 0.9958\n",
            "epoch 3188: train D loss: 0.6878, train F loss: -0.6688, acc 0.9958\n",
            "epoch 3189: train D loss: 0.6846, train F loss: -0.6678, acc 0.9954\n",
            "epoch 3190: train D loss: 0.6847, train F loss: -0.6688, acc 0.9960\n",
            "epoch 3191: train D loss: 0.6871, train F loss: -0.6644, acc 0.9940\n",
            "epoch 3192: train D loss: 0.6880, train F loss: -0.6656, acc 0.9940\n",
            "epoch 3193: train D loss: 0.6824, train F loss: -0.6512, acc 0.9930\n",
            "epoch 3194: train D loss: 0.6818, train F loss: -0.6608, acc 0.9938\n",
            "epoch 3195: train D loss: 0.6844, train F loss: -0.6649, acc 0.9952\n",
            "epoch 3196: train D loss: 0.6858, train F loss: -0.6653, acc 0.9950\n",
            "epoch 3197: train D loss: 0.6858, train F loss: -0.6704, acc 0.9958\n",
            "epoch 3198: train D loss: 0.6877, train F loss: -0.6677, acc 0.9950\n",
            "epoch 3199: train D loss: 0.6851, train F loss: -0.6649, acc 0.9958\n",
            "epoch 3200: train D loss: 0.6857, train F loss: -0.6649, acc 0.9944\n",
            "epoch 3201: train D loss: 0.6855, train F loss: -0.6552, acc 0.9926\n",
            "epoch 3202: train D loss: 0.6859, train F loss: -0.6628, acc 0.9956\n",
            "epoch 3203: train D loss: 0.6826, train F loss: -0.6637, acc 0.9954\n",
            "epoch 3204: train D loss: 0.6832, train F loss: -0.6607, acc 0.9948\n",
            "epoch 3205: train D loss: 0.6805, train F loss: -0.6569, acc 0.9936\n",
            "epoch 3206: train D loss: 0.6850, train F loss: -0.6660, acc 0.9954\n",
            "epoch 3207: train D loss: 0.6847, train F loss: -0.6701, acc 0.9966\n",
            "epoch 3208: train D loss: 0.6846, train F loss: -0.6668, acc 0.9952\n",
            "epoch 3209: train D loss: 0.6848, train F loss: -0.6578, acc 0.9940\n",
            "epoch 3210: train D loss: 0.6833, train F loss: -0.6638, acc 0.9958\n",
            "epoch 3211: train D loss: 0.6879, train F loss: -0.6682, acc 0.9944\n",
            "epoch 3212: train D loss: 0.6862, train F loss: -0.6646, acc 0.9938\n",
            "epoch 3213: train D loss: 0.6856, train F loss: -0.6599, acc 0.9948\n",
            "epoch 3214: train D loss: 0.6833, train F loss: -0.6665, acc 0.9958\n",
            "epoch 3215: train D loss: 0.6839, train F loss: -0.6621, acc 0.9946\n",
            "epoch 3216: train D loss: 0.6821, train F loss: -0.6582, acc 0.9946\n",
            "epoch 3217: train D loss: 0.6864, train F loss: -0.6675, acc 0.9954\n",
            "epoch 3218: train D loss: 0.6901, train F loss: -0.6660, acc 0.9938\n",
            "epoch 3219: train D loss: 0.6835, train F loss: -0.6204, acc 0.9902\n",
            "epoch 3220: train D loss: 0.6833, train F loss: -0.6481, acc 0.9906\n",
            "epoch 3221: train D loss: 0.6827, train F loss: -0.6588, acc 0.9934\n",
            "epoch 3222: train D loss: 0.6834, train F loss: -0.6362, acc 0.9938\n",
            "epoch 3223: train D loss: 0.6821, train F loss: -0.6576, acc 0.9932\n",
            "epoch 3224: train D loss: 0.6819, train F loss: -0.6634, acc 0.9954\n",
            "epoch 3225: train D loss: 0.6843, train F loss: -0.6601, acc 0.9944\n",
            "epoch 3226: train D loss: 0.6803, train F loss: -0.6660, acc 0.9974\n",
            "epoch 3227: train D loss: 0.6857, train F loss: -0.6677, acc 0.9960\n",
            "epoch 3228: train D loss: 0.6855, train F loss: -0.6664, acc 0.9950\n",
            "epoch 3229: train D loss: 0.6854, train F loss: -0.6714, acc 0.9968\n",
            "epoch 3230: train D loss: 0.6818, train F loss: -0.6650, acc 0.9958\n",
            "epoch 3231: train D loss: 0.6864, train F loss: -0.6685, acc 0.9968\n",
            "epoch 3232: train D loss: 0.6832, train F loss: -0.6692, acc 0.9968\n",
            "epoch 3233: train D loss: 0.6907, train F loss: -0.6739, acc 0.9956\n",
            "epoch 3234: train D loss: 0.6852, train F loss: -0.6673, acc 0.9958\n",
            "epoch 3235: train D loss: 0.6863, train F loss: -0.6686, acc 0.9956\n",
            "epoch 3236: train D loss: 0.6842, train F loss: -0.6679, acc 0.9964\n",
            "epoch 3237: train D loss: 0.6881, train F loss: -0.6631, acc 0.9942\n",
            "epoch 3238: train D loss: 0.6827, train F loss: -0.6641, acc 0.9948\n",
            "epoch 3239: train D loss: 0.6838, train F loss: -0.6704, acc 0.9966\n",
            "epoch 3240: train D loss: 0.6845, train F loss: -0.6610, acc 0.9948\n",
            "epoch 3241: train D loss: 0.6843, train F loss: -0.6598, acc 0.9938\n",
            "epoch 3242: train D loss: 0.6862, train F loss: -0.6698, acc 0.9962\n",
            "epoch 3243: train D loss: 0.6827, train F loss: -0.6658, acc 0.9954\n",
            "epoch 3244: train D loss: 0.6884, train F loss: -0.6262, acc 0.9874\n",
            "epoch 3245: train D loss: 0.6819, train F loss: -0.6563, acc 0.9942\n",
            "epoch 3246: train D loss: 0.6810, train F loss: -0.6668, acc 0.9964\n",
            "epoch 3247: train D loss: 0.6817, train F loss: -0.6669, acc 0.9968\n",
            "epoch 3248: train D loss: 0.6877, train F loss: -0.6666, acc 0.9950\n",
            "epoch 3249: train D loss: 0.6836, train F loss: -0.6701, acc 0.9970\n",
            "epoch 3250: train D loss: 0.6869, train F loss: -0.6646, acc 0.9946\n",
            "epoch 3251: train D loss: 0.6833, train F loss: -0.6673, acc 0.9960\n",
            "epoch 3252: train D loss: 0.6832, train F loss: -0.6628, acc 0.9952\n",
            "epoch 3253: train D loss: 0.6867, train F loss: -0.6652, acc 0.9946\n",
            "epoch 3254: train D loss: 0.6841, train F loss: -0.6658, acc 0.9954\n",
            "epoch 3255: train D loss: 0.6850, train F loss: -0.6664, acc 0.9952\n",
            "epoch 3256: train D loss: 0.6851, train F loss: -0.6654, acc 0.9952\n",
            "epoch 3257: train D loss: 0.6853, train F loss: -0.6657, acc 0.9948\n",
            "epoch 3258: train D loss: 0.6869, train F loss: -0.6708, acc 0.9956\n",
            "epoch 3259: train D loss: 0.6855, train F loss: -0.6642, acc 0.9942\n",
            "epoch 3260: train D loss: 0.6836, train F loss: -0.6663, acc 0.9954\n",
            "epoch 3261: train D loss: 0.6862, train F loss: -0.6675, acc 0.9948\n",
            "epoch 3262: train D loss: 0.6850, train F loss: -0.6645, acc 0.9948\n",
            "epoch 3263: train D loss: 0.6838, train F loss: -0.6668, acc 0.9956\n",
            "epoch 3264: train D loss: 0.6879, train F loss: -0.6688, acc 0.9946\n",
            "epoch 3265: train D loss: 0.6865, train F loss: -0.6654, acc 0.9948\n",
            "epoch 3266: train D loss: 0.6859, train F loss: -0.6699, acc 0.9960\n",
            "epoch 3267: train D loss: 0.6859, train F loss: -0.6703, acc 0.9958\n",
            "epoch 3268: train D loss: 0.6849, train F loss: -0.6682, acc 0.9964\n",
            "epoch 3269: train D loss: 0.6873, train F loss: -0.6397, acc 0.9894\n",
            "epoch 3270: train D loss: 0.6875, train F loss: -0.6635, acc 0.9940\n",
            "epoch 3271: train D loss: 0.6820, train F loss: -0.6570, acc 0.9942\n",
            "epoch 3272: train D loss: 0.6853, train F loss: -0.6602, acc 0.9938\n",
            "epoch 3273: train D loss: 0.6888, train F loss: -0.6756, acc 0.9970\n",
            "epoch 3274: train D loss: 0.6854, train F loss: -0.6651, acc 0.9952\n",
            "epoch 3275: train D loss: 0.6858, train F loss: -0.6558, acc 0.9946\n",
            "epoch 3276: train D loss: 0.6846, train F loss: -0.6597, acc 0.9938\n",
            "epoch 3277: train D loss: 0.6857, train F loss: -0.6661, acc 0.9958\n",
            "epoch 3278: train D loss: 0.6853, train F loss: -0.6677, acc 0.9952\n",
            "epoch 3279: train D loss: 0.6848, train F loss: -0.6699, acc 0.9966\n",
            "epoch 3280: train D loss: 0.6835, train F loss: -0.6651, acc 0.9950\n",
            "epoch 3281: train D loss: 0.6865, train F loss: -0.6661, acc 0.9954\n",
            "epoch 3282: train D loss: 0.6878, train F loss: -0.6571, acc 0.9922\n",
            "epoch 3283: train D loss: 0.6839, train F loss: -0.6319, acc 0.9888\n",
            "epoch 3284: train D loss: 0.6856, train F loss: -0.6533, acc 0.9920\n",
            "epoch 3285: train D loss: 0.6840, train F loss: -0.6562, acc 0.9934\n",
            "epoch 3286: train D loss: 0.6856, train F loss: -0.6561, acc 0.9920\n",
            "epoch 3287: train D loss: 0.6851, train F loss: -0.6653, acc 0.9960\n",
            "epoch 3288: train D loss: 0.6828, train F loss: -0.6657, acc 0.9958\n",
            "epoch 3289: train D loss: 0.6863, train F loss: -0.6671, acc 0.9948\n",
            "epoch 3290: train D loss: 0.6838, train F loss: -0.6673, acc 0.9958\n",
            "epoch 3291: train D loss: 0.6830, train F loss: -0.6662, acc 0.9956\n",
            "epoch 3292: train D loss: 0.6842, train F loss: -0.6682, acc 0.9956\n",
            "epoch 3293: train D loss: 0.6844, train F loss: -0.6671, acc 0.9962\n",
            "epoch 3294: train D loss: 0.6863, train F loss: -0.6710, acc 0.9962\n",
            "epoch 3295: train D loss: 0.6856, train F loss: -0.6679, acc 0.9968\n",
            "epoch 3296: train D loss: 0.6844, train F loss: -0.6640, acc 0.9946\n",
            "epoch 3297: train D loss: 0.6834, train F loss: -0.6686, acc 0.9968\n",
            "epoch 3298: train D loss: 0.6869, train F loss: -0.6675, acc 0.9958\n",
            "epoch 3299: train D loss: 0.6856, train F loss: -0.6685, acc 0.9956\n",
            "epoch 3300: train D loss: 0.6853, train F loss: -0.6683, acc 0.9960\n",
            "epoch 3301: train D loss: 0.6854, train F loss: -0.6430, acc 0.9938\n",
            "epoch 3302: train D loss: 0.6845, train F loss: -0.6634, acc 0.9956\n",
            "epoch 3303: train D loss: 0.6866, train F loss: -0.6630, acc 0.9950\n",
            "epoch 3304: train D loss: 0.6831, train F loss: -0.6663, acc 0.9956\n",
            "epoch 3305: train D loss: 0.6873, train F loss: -0.6690, acc 0.9950\n",
            "epoch 3306: train D loss: 0.6844, train F loss: -0.6669, acc 0.9960\n",
            "epoch 3307: train D loss: 0.6880, train F loss: -0.6734, acc 0.9964\n",
            "epoch 3308: train D loss: 0.6862, train F loss: -0.6515, acc 0.9926\n",
            "epoch 3309: train D loss: 0.6849, train F loss: -0.6650, acc 0.9954\n",
            "epoch 3310: train D loss: 0.6855, train F loss: -0.6643, acc 0.9940\n",
            "epoch 3311: train D loss: 0.6849, train F loss: -0.6696, acc 0.9958\n",
            "epoch 3312: train D loss: 0.6879, train F loss: -0.6668, acc 0.9944\n",
            "epoch 3313: train D loss: 0.6841, train F loss: -0.6681, acc 0.9956\n",
            "epoch 3314: train D loss: 0.6855, train F loss: -0.6700, acc 0.9956\n",
            "epoch 3315: train D loss: 0.6866, train F loss: -0.6582, acc 0.9930\n",
            "epoch 3316: train D loss: 0.6836, train F loss: -0.6549, acc 0.9922\n",
            "epoch 3317: train D loss: 0.6842, train F loss: -0.6660, acc 0.9966\n",
            "epoch 3318: train D loss: 0.6869, train F loss: -0.6698, acc 0.9958\n",
            "epoch 3319: train D loss: 0.6859, train F loss: -0.6677, acc 0.9952\n",
            "epoch 3320: train D loss: 0.6864, train F loss: -0.6670, acc 0.9958\n",
            "epoch 3321: train D loss: 0.6865, train F loss: -0.6720, acc 0.9966\n",
            "epoch 3322: train D loss: 0.6852, train F loss: -0.6709, acc 0.9966\n",
            "epoch 3323: train D loss: 0.6878, train F loss: -0.6706, acc 0.9956\n",
            "epoch 3324: train D loss: 0.6854, train F loss: -0.6654, acc 0.9950\n",
            "epoch 3325: train D loss: 0.6860, train F loss: -0.6672, acc 0.9944\n",
            "epoch 3326: train D loss: 0.6850, train F loss: -0.6698, acc 0.9962\n",
            "epoch 3327: train D loss: 0.6855, train F loss: -0.6699, acc 0.9952\n",
            "epoch 3328: train D loss: 0.6867, train F loss: -0.6276, acc 0.9870\n",
            "epoch 3329: train D loss: 0.6826, train F loss: -0.6563, acc 0.9928\n",
            "epoch 3330: train D loss: 0.6843, train F loss: -0.6678, acc 0.9956\n",
            "epoch 3331: train D loss: 0.6850, train F loss: -0.6648, acc 0.9964\n",
            "epoch 3332: train D loss: 0.6853, train F loss: -0.6666, acc 0.9954\n",
            "epoch 3333: train D loss: 0.6835, train F loss: -0.6354, acc 0.9910\n",
            "epoch 3334: train D loss: 0.6879, train F loss: -0.6721, acc 0.9954\n",
            "epoch 3335: train D loss: 0.6855, train F loss: -0.6707, acc 0.9964\n",
            "epoch 3336: train D loss: 0.6860, train F loss: -0.6740, acc 0.9970\n",
            "epoch 3337: train D loss: 0.6862, train F loss: -0.6726, acc 0.9966\n",
            "epoch 3338: train D loss: 0.6849, train F loss: -0.6702, acc 0.9960\n",
            "epoch 3339: train D loss: 0.6858, train F loss: -0.6722, acc 0.9960\n",
            "epoch 3340: train D loss: 0.6857, train F loss: -0.6714, acc 0.9962\n",
            "epoch 3341: train D loss: 0.6857, train F loss: -0.6692, acc 0.9962\n",
            "epoch 3342: train D loss: 0.6865, train F loss: -0.6612, acc 0.9936\n",
            "epoch 3343: train D loss: 0.6861, train F loss: -0.6702, acc 0.9958\n",
            "epoch 3344: train D loss: 0.6828, train F loss: -0.6572, acc 0.9936\n",
            "epoch 3345: train D loss: 0.6854, train F loss: -0.6667, acc 0.9944\n",
            "epoch 3346: train D loss: 0.6855, train F loss: -0.6694, acc 0.9958\n",
            "epoch 3347: train D loss: 0.6847, train F loss: -0.6671, acc 0.9946\n",
            "epoch 3348: train D loss: 0.6856, train F loss: -0.6553, acc 0.9924\n",
            "epoch 3349: train D loss: 0.6848, train F loss: -0.6274, acc 0.9896\n",
            "epoch 3350: train D loss: 0.6850, train F loss: -0.6496, acc 0.9908\n",
            "epoch 3351: train D loss: 0.6822, train F loss: -0.6585, acc 0.9942\n",
            "epoch 3352: train D loss: 0.6825, train F loss: -0.6623, acc 0.9958\n",
            "epoch 3353: train D loss: 0.6847, train F loss: -0.6672, acc 0.9954\n",
            "epoch 3354: train D loss: 0.6821, train F loss: -0.6625, acc 0.9950\n",
            "epoch 3355: train D loss: 0.6868, train F loss: -0.6691, acc 0.9956\n",
            "epoch 3356: train D loss: 0.6834, train F loss: -0.6592, acc 0.9938\n",
            "epoch 3357: train D loss: 0.6841, train F loss: -0.6671, acc 0.9964\n",
            "epoch 3358: train D loss: 0.6825, train F loss: -0.6673, acc 0.9966\n",
            "epoch 3359: train D loss: 0.6860, train F loss: -0.6699, acc 0.9956\n",
            "epoch 3360: train D loss: 0.6872, train F loss: -0.6683, acc 0.9960\n",
            "epoch 3361: train D loss: 0.6857, train F loss: -0.6693, acc 0.9956\n",
            "epoch 3362: train D loss: 0.6839, train F loss: -0.6700, acc 0.9966\n",
            "epoch 3363: train D loss: 0.6848, train F loss: -0.6683, acc 0.9962\n",
            "epoch 3364: train D loss: 0.6887, train F loss: -0.6645, acc 0.9932\n",
            "epoch 3365: train D loss: 0.6828, train F loss: -0.6552, acc 0.9930\n",
            "epoch 3366: train D loss: 0.6852, train F loss: -0.6705, acc 0.9966\n",
            "epoch 3367: train D loss: 0.6848, train F loss: -0.6675, acc 0.9954\n",
            "epoch 3368: train D loss: 0.6846, train F loss: -0.6661, acc 0.9962\n",
            "epoch 3369: train D loss: 0.6886, train F loss: -0.6617, acc 0.9938\n",
            "epoch 3370: train D loss: 0.6872, train F loss: -0.6641, acc 0.9932\n",
            "epoch 3371: train D loss: 0.6874, train F loss: -0.6654, acc 0.9948\n",
            "epoch 3372: train D loss: 0.6865, train F loss: -0.6558, acc 0.9934\n",
            "epoch 3373: train D loss: 0.6837, train F loss: -0.6542, acc 0.9922\n",
            "epoch 3374: train D loss: 0.6835, train F loss: -0.6532, acc 0.9940\n",
            "epoch 3375: train D loss: 0.6822, train F loss: -0.6567, acc 0.9942\n",
            "epoch 3376: train D loss: 0.6855, train F loss: -0.6700, acc 0.9960\n",
            "epoch 3377: train D loss: 0.6858, train F loss: -0.6677, acc 0.9958\n",
            "epoch 3378: train D loss: 0.6852, train F loss: -0.6684, acc 0.9956\n",
            "epoch 3379: train D loss: 0.6809, train F loss: -0.6668, acc 0.9964\n",
            "epoch 3380: train D loss: 0.6872, train F loss: -0.6706, acc 0.9958\n",
            "epoch 3381: train D loss: 0.6834, train F loss: -0.6685, acc 0.9962\n",
            "epoch 3382: train D loss: 0.6859, train F loss: -0.6683, acc 0.9954\n",
            "epoch 3383: train D loss: 0.6840, train F loss: -0.6525, acc 0.9932\n",
            "epoch 3384: train D loss: 0.6855, train F loss: -0.6565, acc 0.9932\n",
            "epoch 3385: train D loss: 0.6866, train F loss: -0.6637, acc 0.9958\n",
            "epoch 3386: train D loss: 0.6858, train F loss: -0.6578, acc 0.9926\n",
            "epoch 3387: train D loss: 0.6842, train F loss: -0.6641, acc 0.9942\n",
            "epoch 3388: train D loss: 0.6837, train F loss: -0.6642, acc 0.9960\n",
            "epoch 3389: train D loss: 0.6840, train F loss: -0.6722, acc 0.9972\n",
            "epoch 3390: train D loss: 0.6850, train F loss: -0.6727, acc 0.9974\n",
            "epoch 3391: train D loss: 0.6875, train F loss: -0.6542, acc 0.9940\n",
            "epoch 3392: train D loss: 0.6845, train F loss: -0.6634, acc 0.9942\n",
            "epoch 3393: train D loss: 0.6842, train F loss: -0.6581, acc 0.9948\n",
            "epoch 3394: train D loss: 0.6842, train F loss: -0.6528, acc 0.9926\n",
            "epoch 3395: train D loss: 0.6806, train F loss: -0.6611, acc 0.9950\n",
            "epoch 3396: train D loss: 0.6841, train F loss: -0.6660, acc 0.9960\n",
            "epoch 3397: train D loss: 0.6836, train F loss: -0.6670, acc 0.9950\n",
            "epoch 3398: train D loss: 0.6866, train F loss: -0.6741, acc 0.9972\n",
            "epoch 3399: train D loss: 0.6864, train F loss: -0.6689, acc 0.9960\n",
            "epoch 3400: train D loss: 0.6844, train F loss: -0.6679, acc 0.9966\n",
            "epoch 3401: train D loss: 0.6841, train F loss: -0.6631, acc 0.9950\n",
            "epoch 3402: train D loss: 0.6839, train F loss: -0.6643, acc 0.9942\n",
            "epoch 3403: train D loss: 0.6837, train F loss: -0.6647, acc 0.9956\n",
            "epoch 3404: train D loss: 0.6863, train F loss: -0.6681, acc 0.9956\n",
            "epoch 3405: train D loss: 0.6858, train F loss: -0.6371, acc 0.9938\n",
            "epoch 3406: train D loss: 0.6891, train F loss: -0.5932, acc 0.9788\n",
            "epoch 3407: train D loss: 0.6833, train F loss: -0.6447, acc 0.9900\n",
            "epoch 3408: train D loss: 0.6807, train F loss: -0.6586, acc 0.9938\n",
            "epoch 3409: train D loss: 0.6822, train F loss: -0.6557, acc 0.9934\n",
            "epoch 3410: train D loss: 0.6837, train F loss: -0.6633, acc 0.9948\n",
            "epoch 3411: train D loss: 0.6857, train F loss: -0.6700, acc 0.9960\n",
            "epoch 3412: train D loss: 0.6837, train F loss: -0.6474, acc 0.9932\n",
            "epoch 3413: train D loss: 0.6824, train F loss: -0.6641, acc 0.9948\n",
            "epoch 3414: train D loss: 0.6832, train F loss: -0.6656, acc 0.9958\n",
            "epoch 3415: train D loss: 0.6836, train F loss: -0.6656, acc 0.9950\n",
            "epoch 3416: train D loss: 0.6837, train F loss: -0.6704, acc 0.9970\n",
            "epoch 3417: train D loss: 0.6843, train F loss: -0.6668, acc 0.9946\n",
            "epoch 3418: train D loss: 0.6858, train F loss: -0.6436, acc 0.9930\n",
            "epoch 3419: train D loss: 0.6851, train F loss: -0.6687, acc 0.9954\n",
            "epoch 3420: train D loss: 0.6820, train F loss: -0.6673, acc 0.9966\n",
            "epoch 3421: train D loss: 0.6858, train F loss: -0.6728, acc 0.9972\n",
            "epoch 3422: train D loss: 0.6850, train F loss: -0.6674, acc 0.9948\n",
            "epoch 3423: train D loss: 0.6855, train F loss: -0.6650, acc 0.9940\n",
            "epoch 3424: train D loss: 0.6858, train F loss: -0.6695, acc 0.9958\n",
            "epoch 3425: train D loss: 0.6840, train F loss: -0.6685, acc 0.9956\n",
            "epoch 3426: train D loss: 0.6830, train F loss: -0.6686, acc 0.9964\n",
            "epoch 3427: train D loss: 0.6838, train F loss: -0.6664, acc 0.9956\n",
            "epoch 3428: train D loss: 0.6869, train F loss: -0.6716, acc 0.9952\n",
            "epoch 3429: train D loss: 0.6852, train F loss: -0.6672, acc 0.9944\n",
            "epoch 3430: train D loss: 0.6846, train F loss: -0.6658, acc 0.9958\n",
            "epoch 3431: train D loss: 0.6870, train F loss: -0.6695, acc 0.9954\n",
            "epoch 3432: train D loss: 0.6860, train F loss: -0.6681, acc 0.9950\n",
            "epoch 3433: train D loss: 0.6855, train F loss: -0.6700, acc 0.9962\n",
            "epoch 3434: train D loss: 0.6866, train F loss: -0.6652, acc 0.9940\n",
            "epoch 3435: train D loss: 0.6819, train F loss: -0.6673, acc 0.9958\n",
            "epoch 3436: train D loss: 0.6855, train F loss: -0.6689, acc 0.9962\n",
            "epoch 3437: train D loss: 0.6861, train F loss: -0.6673, acc 0.9946\n",
            "epoch 3438: train D loss: 0.6846, train F loss: -0.6646, acc 0.9948\n",
            "epoch 3439: train D loss: 0.6858, train F loss: -0.6653, acc 0.9944\n",
            "epoch 3440: train D loss: 0.6838, train F loss: -0.6655, acc 0.9958\n",
            "epoch 3441: train D loss: 0.6870, train F loss: -0.6566, acc 0.9918\n",
            "epoch 3442: train D loss: 0.6825, train F loss: -0.6637, acc 0.9960\n",
            "epoch 3443: train D loss: 0.6853, train F loss: -0.6611, acc 0.9930\n",
            "epoch 3444: train D loss: 0.6836, train F loss: -0.6669, acc 0.9954\n",
            "epoch 3445: train D loss: 0.6851, train F loss: -0.6665, acc 0.9954\n",
            "epoch 3446: train D loss: 0.6820, train F loss: -0.6657, acc 0.9954\n",
            "epoch 3447: train D loss: 0.6847, train F loss: -0.6666, acc 0.9940\n",
            "epoch 3448: train D loss: 0.6877, train F loss: -0.6702, acc 0.9960\n",
            "epoch 3449: train D loss: 0.6855, train F loss: -0.6654, acc 0.9958\n",
            "epoch 3450: train D loss: 0.6860, train F loss: -0.6716, acc 0.9960\n",
            "epoch 3451: train D loss: 0.6834, train F loss: -0.6678, acc 0.9960\n",
            "epoch 3452: train D loss: 0.6868, train F loss: -0.6589, acc 0.9952\n",
            "epoch 3453: train D loss: 0.6872, train F loss: -0.6618, acc 0.9946\n",
            "epoch 3454: train D loss: 0.6858, train F loss: -0.6570, acc 0.9924\n",
            "epoch 3455: train D loss: 0.6883, train F loss: -0.6666, acc 0.9944\n",
            "epoch 3456: train D loss: 0.6859, train F loss: -0.6678, acc 0.9966\n",
            "epoch 3457: train D loss: 0.6875, train F loss: -0.6670, acc 0.9952\n",
            "epoch 3458: train D loss: 0.6860, train F loss: -0.6646, acc 0.9940\n",
            "epoch 3459: train D loss: 0.6858, train F loss: -0.6709, acc 0.9962\n",
            "epoch 3460: train D loss: 0.6847, train F loss: -0.6641, acc 0.9956\n",
            "epoch 3461: train D loss: 0.6846, train F loss: -0.6724, acc 0.9970\n",
            "epoch 3462: train D loss: 0.6870, train F loss: -0.6690, acc 0.9952\n",
            "epoch 3463: train D loss: 0.6828, train F loss: -0.6460, acc 0.9928\n",
            "epoch 3464: train D loss: 0.6844, train F loss: -0.6658, acc 0.9952\n",
            "epoch 3465: train D loss: 0.6846, train F loss: -0.6678, acc 0.9952\n",
            "epoch 3466: train D loss: 0.6852, train F loss: -0.6652, acc 0.9952\n",
            "epoch 3467: train D loss: 0.6867, train F loss: -0.6678, acc 0.9946\n",
            "epoch 3468: train D loss: 0.6868, train F loss: -0.6596, acc 0.9952\n",
            "epoch 3469: train D loss: 0.6839, train F loss: -0.6640, acc 0.9950\n",
            "epoch 3470: train D loss: 0.6873, train F loss: -0.6632, acc 0.9950\n",
            "epoch 3471: train D loss: 0.6850, train F loss: -0.6656, acc 0.9958\n",
            "epoch 3472: train D loss: 0.6842, train F loss: -0.6689, acc 0.9962\n",
            "epoch 3473: train D loss: 0.6821, train F loss: -0.6639, acc 0.9954\n",
            "epoch 3474: train D loss: 0.6847, train F loss: -0.6580, acc 0.9938\n",
            "epoch 3475: train D loss: 0.6858, train F loss: -0.6708, acc 0.9966\n",
            "epoch 3476: train D loss: 0.6882, train F loss: -0.6665, acc 0.9954\n",
            "epoch 3477: train D loss: 0.6854, train F loss: -0.6629, acc 0.9938\n",
            "epoch 3478: train D loss: 0.6845, train F loss: -0.6680, acc 0.9954\n",
            "epoch 3479: train D loss: 0.6859, train F loss: -0.6693, acc 0.9960\n",
            "epoch 3480: train D loss: 0.6853, train F loss: -0.6724, acc 0.9964\n",
            "epoch 3481: train D loss: 0.6877, train F loss: -0.6706, acc 0.9950\n",
            "epoch 3482: train D loss: 0.6881, train F loss: -0.6650, acc 0.9934\n",
            "epoch 3483: train D loss: 0.6853, train F loss: -0.6524, acc 0.9928\n",
            "epoch 3484: train D loss: 0.6869, train F loss: -0.6632, acc 0.9940\n",
            "epoch 3485: train D loss: 0.6880, train F loss: -0.6652, acc 0.9946\n",
            "epoch 3486: train D loss: 0.6862, train F loss: -0.6688, acc 0.9954\n",
            "epoch 3487: train D loss: 0.6867, train F loss: -0.6706, acc 0.9954\n",
            "epoch 3488: train D loss: 0.6856, train F loss: -0.6713, acc 0.9958\n",
            "epoch 3489: train D loss: 0.6846, train F loss: -0.6686, acc 0.9952\n",
            "epoch 3490: train D loss: 0.6837, train F loss: -0.6668, acc 0.9960\n",
            "epoch 3491: train D loss: 0.6851, train F loss: -0.6651, acc 0.9956\n",
            "epoch 3492: train D loss: 0.6875, train F loss: -0.6661, acc 0.9944\n",
            "epoch 3493: train D loss: 0.6846, train F loss: -0.6682, acc 0.9948\n",
            "epoch 3494: train D loss: 0.6852, train F loss: -0.6081, acc 0.9866\n",
            "epoch 3495: train D loss: 0.6825, train F loss: -0.6573, acc 0.9924\n",
            "epoch 3496: train D loss: 0.6890, train F loss: -0.6664, acc 0.9940\n",
            "epoch 3497: train D loss: 0.6851, train F loss: -0.6656, acc 0.9952\n",
            "epoch 3498: train D loss: 0.6819, train F loss: -0.6650, acc 0.9954\n",
            "epoch 3499: train D loss: 0.6858, train F loss: -0.6716, acc 0.9962\n",
            "epoch 3500: train D loss: 0.6850, train F loss: -0.6702, acc 0.9956\n",
            "epoch 3501: train D loss: 0.6829, train F loss: -0.6688, acc 0.9964\n",
            "epoch 3502: train D loss: 0.6813, train F loss: -0.6603, acc 0.9956\n",
            "epoch 3503: train D loss: 0.6877, train F loss: -0.6548, acc 0.9932\n",
            "epoch 3504: train D loss: 0.6850, train F loss: -0.6687, acc 0.9952\n",
            "epoch 3505: train D loss: 0.6866, train F loss: -0.6667, acc 0.9946\n",
            "epoch 3506: train D loss: 0.6823, train F loss: -0.6630, acc 0.9952\n",
            "epoch 3507: train D loss: 0.6837, train F loss: -0.6701, acc 0.9968\n",
            "epoch 3508: train D loss: 0.6871, train F loss: -0.6683, acc 0.9952\n",
            "epoch 3509: train D loss: 0.6862, train F loss: -0.6618, acc 0.9942\n",
            "epoch 3510: train D loss: 0.6828, train F loss: -0.6598, acc 0.9942\n",
            "epoch 3511: train D loss: 0.6860, train F loss: -0.6679, acc 0.9960\n",
            "epoch 3512: train D loss: 0.6845, train F loss: -0.6668, acc 0.9956\n",
            "epoch 3513: train D loss: 0.6853, train F loss: -0.6647, acc 0.9956\n",
            "epoch 3514: train D loss: 0.6852, train F loss: -0.6725, acc 0.9968\n",
            "epoch 3515: train D loss: 0.6853, train F loss: -0.6671, acc 0.9950\n",
            "epoch 3516: train D loss: 0.6864, train F loss: -0.6714, acc 0.9956\n",
            "epoch 3517: train D loss: 0.6880, train F loss: -0.6618, acc 0.9944\n",
            "epoch 3518: train D loss: 0.6851, train F loss: -0.6626, acc 0.9952\n",
            "epoch 3519: train D loss: 0.6812, train F loss: -0.6664, acc 0.9954\n",
            "epoch 3520: train D loss: 0.6864, train F loss: -0.6698, acc 0.9960\n",
            "epoch 3521: train D loss: 0.6861, train F loss: -0.6683, acc 0.9948\n",
            "epoch 3522: train D loss: 0.6858, train F loss: -0.6680, acc 0.9956\n",
            "epoch 3523: train D loss: 0.6860, train F loss: -0.6701, acc 0.9958\n",
            "epoch 3524: train D loss: 0.6874, train F loss: -0.6626, acc 0.9928\n",
            "epoch 3525: train D loss: 0.6845, train F loss: -0.6620, acc 0.9936\n",
            "epoch 3526: train D loss: 0.6840, train F loss: -0.6686, acc 0.9954\n",
            "epoch 3527: train D loss: 0.6880, train F loss: -0.6706, acc 0.9952\n",
            "epoch 3528: train D loss: 0.6836, train F loss: -0.6664, acc 0.9956\n",
            "epoch 3529: train D loss: 0.6882, train F loss: -0.6678, acc 0.9948\n",
            "epoch 3530: train D loss: 0.6848, train F loss: -0.6684, acc 0.9954\n",
            "epoch 3531: train D loss: 0.6863, train F loss: -0.6654, acc 0.9940\n",
            "epoch 3532: train D loss: 0.6841, train F loss: -0.6699, acc 0.9960\n",
            "epoch 3533: train D loss: 0.6846, train F loss: -0.6656, acc 0.9954\n",
            "epoch 3534: train D loss: 0.6887, train F loss: -0.6679, acc 0.9950\n",
            "epoch 3535: train D loss: 0.6867, train F loss: -0.6704, acc 0.9954\n",
            "epoch 3536: train D loss: 0.6860, train F loss: -0.6647, acc 0.9944\n",
            "epoch 3537: train D loss: 0.6855, train F loss: -0.6689, acc 0.9948\n",
            "epoch 3538: train D loss: 0.6871, train F loss: -0.6632, acc 0.9930\n",
            "epoch 3539: train D loss: 0.6847, train F loss: -0.6657, acc 0.9954\n",
            "epoch 3540: train D loss: 0.6841, train F loss: -0.6693, acc 0.9964\n",
            "epoch 3541: train D loss: 0.6846, train F loss: -0.6572, acc 0.9932\n",
            "epoch 3542: train D loss: 0.6821, train F loss: -0.6603, acc 0.9954\n",
            "epoch 3543: train D loss: 0.6897, train F loss: -0.6659, acc 0.9936\n",
            "epoch 3544: train D loss: 0.6879, train F loss: -0.6659, acc 0.9940\n",
            "epoch 3545: train D loss: 0.6832, train F loss: -0.6629, acc 0.9954\n",
            "epoch 3546: train D loss: 0.6853, train F loss: -0.6328, acc 0.9916\n",
            "epoch 3547: train D loss: 0.6863, train F loss: -0.6621, acc 0.9942\n",
            "epoch 3548: train D loss: 0.6856, train F loss: -0.6656, acc 0.9950\n",
            "epoch 3549: train D loss: 0.6834, train F loss: -0.6640, acc 0.9942\n",
            "epoch 3550: train D loss: 0.6830, train F loss: -0.6664, acc 0.9952\n",
            "epoch 3551: train D loss: 0.6841, train F loss: -0.6692, acc 0.9958\n",
            "epoch 3552: train D loss: 0.6823, train F loss: -0.6668, acc 0.9958\n",
            "epoch 3553: train D loss: 0.6845, train F loss: -0.6652, acc 0.9948\n",
            "epoch 3554: train D loss: 0.6854, train F loss: -0.6610, acc 0.9940\n",
            "epoch 3555: train D loss: 0.6832, train F loss: -0.6577, acc 0.9936\n",
            "epoch 3556: train D loss: 0.6866, train F loss: -0.6677, acc 0.9950\n",
            "epoch 3557: train D loss: 0.6852, train F loss: -0.6721, acc 0.9960\n",
            "epoch 3558: train D loss: 0.6832, train F loss: -0.6700, acc 0.9964\n",
            "epoch 3559: train D loss: 0.6841, train F loss: -0.6638, acc 0.9958\n",
            "epoch 3560: train D loss: 0.6830, train F loss: -0.6643, acc 0.9950\n",
            "epoch 3561: train D loss: 0.6845, train F loss: -0.6701, acc 0.9960\n",
            "epoch 3562: train D loss: 0.6861, train F loss: -0.6649, acc 0.9952\n",
            "epoch 3563: train D loss: 0.6869, train F loss: -0.6689, acc 0.9948\n",
            "epoch 3564: train D loss: 0.6849, train F loss: -0.6652, acc 0.9938\n",
            "epoch 3565: train D loss: 0.6859, train F loss: -0.6704, acc 0.9954\n",
            "epoch 3566: train D loss: 0.6851, train F loss: -0.6637, acc 0.9938\n",
            "epoch 3567: train D loss: 0.6889, train F loss: -0.6634, acc 0.9926\n",
            "epoch 3568: train D loss: 0.6842, train F loss: -0.6671, acc 0.9958\n",
            "epoch 3569: train D loss: 0.6807, train F loss: -0.6653, acc 0.9964\n",
            "epoch 3570: train D loss: 0.6851, train F loss: -0.6687, acc 0.9964\n",
            "epoch 3571: train D loss: 0.6855, train F loss: -0.6629, acc 0.9938\n",
            "epoch 3572: train D loss: 0.6841, train F loss: -0.6654, acc 0.9954\n",
            "epoch 3573: train D loss: 0.6840, train F loss: -0.6707, acc 0.9972\n",
            "epoch 3574: train D loss: 0.6823, train F loss: -0.6656, acc 0.9956\n",
            "epoch 3575: train D loss: 0.6813, train F loss: -0.6684, acc 0.9970\n",
            "epoch 3576: train D loss: 0.6850, train F loss: -0.6632, acc 0.9944\n",
            "epoch 3577: train D loss: 0.6857, train F loss: -0.6673, acc 0.9956\n",
            "epoch 3578: train D loss: 0.6847, train F loss: -0.6569, acc 0.9940\n",
            "epoch 3579: train D loss: 0.6879, train F loss: -0.6642, acc 0.9930\n",
            "epoch 3580: train D loss: 0.6835, train F loss: -0.6631, acc 0.9944\n",
            "epoch 3581: train D loss: 0.6838, train F loss: -0.6651, acc 0.9944\n",
            "epoch 3582: train D loss: 0.6873, train F loss: -0.6554, acc 0.9932\n",
            "epoch 3583: train D loss: 0.6852, train F loss: -0.6685, acc 0.9954\n",
            "epoch 3584: train D loss: 0.6826, train F loss: -0.6648, acc 0.9960\n",
            "epoch 3585: train D loss: 0.6862, train F loss: -0.6599, acc 0.9936\n",
            "epoch 3586: train D loss: 0.6827, train F loss: -0.6572, acc 0.9946\n",
            "epoch 3587: train D loss: 0.6861, train F loss: -0.6701, acc 0.9954\n",
            "epoch 3588: train D loss: 0.6806, train F loss: -0.6641, acc 0.9958\n",
            "epoch 3589: train D loss: 0.6842, train F loss: -0.6658, acc 0.9950\n",
            "epoch 3590: train D loss: 0.6858, train F loss: -0.6714, acc 0.9962\n",
            "epoch 3591: train D loss: 0.6853, train F loss: -0.6661, acc 0.9950\n",
            "epoch 3592: train D loss: 0.6826, train F loss: -0.6694, acc 0.9964\n",
            "epoch 3593: train D loss: 0.6839, train F loss: -0.6610, acc 0.9944\n",
            "epoch 3594: train D loss: 0.6877, train F loss: -0.6691, acc 0.9950\n",
            "epoch 3595: train D loss: 0.6877, train F loss: -0.6690, acc 0.9942\n",
            "epoch 3596: train D loss: 0.6844, train F loss: -0.6679, acc 0.9950\n",
            "epoch 3597: train D loss: 0.6847, train F loss: -0.6715, acc 0.9964\n",
            "epoch 3598: train D loss: 0.6870, train F loss: -0.6101, acc 0.9916\n",
            "epoch 3599: train D loss: 0.6858, train F loss: -0.6567, acc 0.9922\n",
            "epoch 3600: train D loss: 0.6848, train F loss: -0.6341, acc 0.9892\n",
            "epoch 3601: train D loss: 0.6785, train F loss: -0.6585, acc 0.9950\n",
            "epoch 3602: train D loss: 0.6834, train F loss: -0.6612, acc 0.9944\n",
            "epoch 3603: train D loss: 0.6823, train F loss: -0.6426, acc 0.9910\n",
            "epoch 3604: train D loss: 0.6826, train F loss: -0.6647, acc 0.9946\n",
            "epoch 3605: train D loss: 0.6856, train F loss: -0.6706, acc 0.9964\n",
            "epoch 3606: train D loss: 0.6837, train F loss: -0.6687, acc 0.9966\n",
            "epoch 3607: train D loss: 0.6832, train F loss: -0.6715, acc 0.9968\n",
            "epoch 3608: train D loss: 0.6847, train F loss: -0.6694, acc 0.9962\n",
            "epoch 3609: train D loss: 0.6824, train F loss: -0.6473, acc 0.9940\n",
            "epoch 3610: train D loss: 0.6862, train F loss: -0.6478, acc 0.9892\n",
            "epoch 3611: train D loss: 0.6839, train F loss: -0.6631, acc 0.9946\n",
            "epoch 3612: train D loss: 0.6811, train F loss: -0.6627, acc 0.9954\n",
            "epoch 3613: train D loss: 0.6859, train F loss: -0.6686, acc 0.9954\n",
            "epoch 3614: train D loss: 0.6845, train F loss: -0.6675, acc 0.9954\n",
            "epoch 3615: train D loss: 0.6849, train F loss: -0.6704, acc 0.9960\n",
            "epoch 3616: train D loss: 0.6843, train F loss: -0.6689, acc 0.9960\n",
            "epoch 3617: train D loss: 0.6838, train F loss: -0.6687, acc 0.9960\n",
            "epoch 3618: train D loss: 0.6831, train F loss: -0.6633, acc 0.9954\n",
            "epoch 3619: train D loss: 0.6856, train F loss: -0.6689, acc 0.9960\n",
            "epoch 3620: train D loss: 0.6836, train F loss: -0.6674, acc 0.9958\n",
            "epoch 3621: train D loss: 0.6899, train F loss: -0.6715, acc 0.9958\n",
            "epoch 3622: train D loss: 0.6819, train F loss: -0.6677, acc 0.9972\n",
            "epoch 3623: train D loss: 0.6869, train F loss: -0.6692, acc 0.9952\n",
            "epoch 3624: train D loss: 0.6873, train F loss: -0.6713, acc 0.9954\n",
            "epoch 3625: train D loss: 0.6870, train F loss: -0.6703, acc 0.9954\n",
            "epoch 3626: train D loss: 0.6847, train F loss: -0.6723, acc 0.9966\n",
            "epoch 3627: train D loss: 0.6850, train F loss: -0.6692, acc 0.9944\n",
            "epoch 3628: train D loss: 0.6883, train F loss: -0.6716, acc 0.9962\n",
            "epoch 3629: train D loss: 0.6870, train F loss: -0.6604, acc 0.9930\n",
            "epoch 3630: train D loss: 0.6858, train F loss: -0.6637, acc 0.9938\n",
            "epoch 3631: train D loss: 0.6851, train F loss: -0.6722, acc 0.9964\n",
            "epoch 3632: train D loss: 0.6821, train F loss: -0.6618, acc 0.9948\n",
            "epoch 3633: train D loss: 0.6859, train F loss: -0.6610, acc 0.9932\n",
            "epoch 3634: train D loss: 0.6847, train F loss: -0.6680, acc 0.9956\n",
            "epoch 3635: train D loss: 0.6827, train F loss: -0.6616, acc 0.9954\n",
            "epoch 3636: train D loss: 0.6857, train F loss: -0.6720, acc 0.9966\n",
            "epoch 3637: train D loss: 0.6859, train F loss: -0.6687, acc 0.9958\n",
            "epoch 3638: train D loss: 0.6849, train F loss: -0.6568, acc 0.9934\n",
            "epoch 3639: train D loss: 0.6820, train F loss: -0.6630, acc 0.9952\n",
            "epoch 3640: train D loss: 0.6835, train F loss: -0.6678, acc 0.9956\n",
            "epoch 3641: train D loss: 0.6859, train F loss: -0.6700, acc 0.9960\n",
            "epoch 3642: train D loss: 0.6855, train F loss: -0.6681, acc 0.9952\n",
            "epoch 3643: train D loss: 0.6875, train F loss: -0.6692, acc 0.9954\n",
            "epoch 3644: train D loss: 0.6842, train F loss: -0.6597, acc 0.9932\n",
            "epoch 3645: train D loss: 0.6860, train F loss: -0.6681, acc 0.9950\n",
            "epoch 3646: train D loss: 0.6882, train F loss: -0.6696, acc 0.9952\n",
            "epoch 3647: train D loss: 0.6857, train F loss: -0.6669, acc 0.9946\n",
            "epoch 3648: train D loss: 0.6850, train F loss: -0.6672, acc 0.9946\n",
            "epoch 3649: train D loss: 0.6836, train F loss: -0.6702, acc 0.9964\n",
            "epoch 3650: train D loss: 0.6848, train F loss: -0.6717, acc 0.9960\n",
            "epoch 3651: train D loss: 0.6865, train F loss: -0.6625, acc 0.9936\n",
            "epoch 3652: train D loss: 0.6867, train F loss: -0.6495, acc 0.9922\n",
            "epoch 3653: train D loss: 0.6849, train F loss: -0.6620, acc 0.9936\n",
            "epoch 3654: train D loss: 0.6830, train F loss: -0.6632, acc 0.9948\n",
            "epoch 3655: train D loss: 0.6833, train F loss: -0.6562, acc 0.9944\n",
            "epoch 3656: train D loss: 0.6848, train F loss: -0.6398, acc 0.9914\n",
            "epoch 3657: train D loss: 0.6820, train F loss: -0.6654, acc 0.9950\n",
            "epoch 3658: train D loss: 0.6831, train F loss: -0.6672, acc 0.9956\n",
            "epoch 3659: train D loss: 0.6849, train F loss: -0.6698, acc 0.9962\n",
            "epoch 3660: train D loss: 0.6835, train F loss: -0.6694, acc 0.9964\n",
            "epoch 3661: train D loss: 0.6843, train F loss: -0.6712, acc 0.9958\n",
            "epoch 3662: train D loss: 0.6878, train F loss: -0.6694, acc 0.9948\n",
            "epoch 3663: train D loss: 0.6815, train F loss: -0.6670, acc 0.9960\n",
            "epoch 3664: train D loss: 0.6873, train F loss: -0.6711, acc 0.9958\n",
            "epoch 3665: train D loss: 0.6866, train F loss: -0.6683, acc 0.9954\n",
            "epoch 3666: train D loss: 0.6822, train F loss: -0.5882, acc 0.9862\n",
            "epoch 3667: train D loss: 0.6828, train F loss: -0.6627, acc 0.9942\n",
            "epoch 3668: train D loss: 0.6855, train F loss: -0.6668, acc 0.9958\n",
            "epoch 3669: train D loss: 0.6824, train F loss: -0.6658, acc 0.9952\n",
            "epoch 3670: train D loss: 0.6828, train F loss: -0.6660, acc 0.9958\n",
            "epoch 3671: train D loss: 0.6877, train F loss: -0.6730, acc 0.9962\n",
            "epoch 3672: train D loss: 0.6849, train F loss: -0.6463, acc 0.9896\n",
            "epoch 3673: train D loss: 0.6827, train F loss: -0.6633, acc 0.9948\n",
            "epoch 3674: train D loss: 0.6853, train F loss: -0.6719, acc 0.9964\n",
            "epoch 3675: train D loss: 0.6864, train F loss: -0.6650, acc 0.9946\n",
            "epoch 3676: train D loss: 0.6829, train F loss: -0.6696, acc 0.9966\n",
            "epoch 3677: train D loss: 0.6841, train F loss: -0.6713, acc 0.9968\n",
            "epoch 3678: train D loss: 0.6857, train F loss: -0.6705, acc 0.9964\n",
            "epoch 3679: train D loss: 0.6860, train F loss: -0.6730, acc 0.9960\n",
            "epoch 3680: train D loss: 0.6863, train F loss: -0.6670, acc 0.9948\n",
            "epoch 3681: train D loss: 0.6850, train F loss: -0.6688, acc 0.9954\n",
            "epoch 3682: train D loss: 0.6892, train F loss: -0.6720, acc 0.9942\n",
            "epoch 3683: train D loss: 0.6857, train F loss: -0.6757, acc 0.9976\n",
            "epoch 3684: train D loss: 0.6861, train F loss: -0.6752, acc 0.9972\n",
            "epoch 3685: train D loss: 0.6881, train F loss: -0.6719, acc 0.9962\n",
            "epoch 3686: train D loss: 0.6843, train F loss: -0.6616, acc 0.9952\n",
            "epoch 3687: train D loss: 0.6854, train F loss: -0.6642, acc 0.9946\n",
            "epoch 3688: train D loss: 0.6835, train F loss: -0.6647, acc 0.9956\n",
            "epoch 3689: train D loss: 0.6865, train F loss: -0.6701, acc 0.9954\n",
            "epoch 3690: train D loss: 0.6886, train F loss: -0.6732, acc 0.9956\n",
            "epoch 3691: train D loss: 0.6853, train F loss: -0.6697, acc 0.9958\n",
            "epoch 3692: train D loss: 0.6863, train F loss: -0.6695, acc 0.9952\n",
            "epoch 3693: train D loss: 0.6863, train F loss: -0.6654, acc 0.9950\n",
            "epoch 3694: train D loss: 0.6871, train F loss: -0.6713, acc 0.9960\n",
            "epoch 3695: train D loss: 0.6887, train F loss: -0.6667, acc 0.9942\n",
            "epoch 3696: train D loss: 0.6888, train F loss: -0.6665, acc 0.9946\n",
            "epoch 3697: train D loss: 0.6852, train F loss: -0.6684, acc 0.9956\n",
            "epoch 3698: train D loss: 0.6848, train F loss: -0.6669, acc 0.9956\n",
            "epoch 3699: train D loss: 0.6867, train F loss: -0.6752, acc 0.9966\n",
            "epoch 3700: train D loss: 0.6888, train F loss: -0.6710, acc 0.9948\n",
            "epoch 3701: train D loss: 0.6869, train F loss: -0.6731, acc 0.9964\n",
            "epoch 3702: train D loss: 0.6819, train F loss: -0.6543, acc 0.9950\n",
            "epoch 3703: train D loss: 0.6865, train F loss: -0.5245, acc 0.9770\n",
            "epoch 3704: train D loss: 0.6869, train F loss: -0.6614, acc 0.9932\n",
            "epoch 3705: train D loss: 0.6850, train F loss: -0.6687, acc 0.9958\n",
            "epoch 3706: train D loss: 0.6835, train F loss: -0.6688, acc 0.9968\n",
            "epoch 3707: train D loss: 0.6830, train F loss: -0.6649, acc 0.9956\n",
            "epoch 3708: train D loss: 0.6846, train F loss: -0.6664, acc 0.9950\n",
            "epoch 3709: train D loss: 0.6856, train F loss: -0.6709, acc 0.9960\n",
            "epoch 3710: train D loss: 0.6853, train F loss: -0.6677, acc 0.9950\n",
            "epoch 3711: train D loss: 0.6855, train F loss: -0.6715, acc 0.9962\n",
            "epoch 3712: train D loss: 0.6872, train F loss: -0.6722, acc 0.9958\n",
            "epoch 3713: train D loss: 0.6831, train F loss: -0.6570, acc 0.9944\n",
            "epoch 3714: train D loss: 0.6872, train F loss: -0.6654, acc 0.9946\n",
            "epoch 3715: train D loss: 0.6853, train F loss: -0.6715, acc 0.9968\n",
            "epoch 3716: train D loss: 0.6858, train F loss: -0.6678, acc 0.9962\n",
            "epoch 3717: train D loss: 0.6879, train F loss: -0.6743, acc 0.9966\n",
            "epoch 3718: train D loss: 0.6836, train F loss: -0.6610, acc 0.9934\n",
            "epoch 3719: train D loss: 0.6847, train F loss: -0.6579, acc 0.9948\n",
            "epoch 3720: train D loss: 0.6849, train F loss: -0.6433, acc 0.9944\n",
            "epoch 3721: train D loss: 0.6863, train F loss: -0.6564, acc 0.9920\n",
            "epoch 3722: train D loss: 0.6839, train F loss: -0.6584, acc 0.9942\n",
            "epoch 3723: train D loss: 0.6846, train F loss: -0.6628, acc 0.9942\n",
            "epoch 3724: train D loss: 0.6879, train F loss: -0.6706, acc 0.9954\n",
            "epoch 3725: train D loss: 0.6843, train F loss: -0.6623, acc 0.9960\n",
            "epoch 3726: train D loss: 0.6859, train F loss: -0.6681, acc 0.9938\n",
            "epoch 3727: train D loss: 0.6852, train F loss: -0.6693, acc 0.9954\n",
            "epoch 3728: train D loss: 0.6849, train F loss: -0.6587, acc 0.9940\n",
            "epoch 3729: train D loss: 0.6864, train F loss: -0.6665, acc 0.9954\n",
            "epoch 3730: train D loss: 0.6827, train F loss: -0.6709, acc 0.9972\n",
            "epoch 3731: train D loss: 0.6854, train F loss: -0.6704, acc 0.9960\n",
            "epoch 3732: train D loss: 0.6882, train F loss: -0.6689, acc 0.9946\n",
            "epoch 3733: train D loss: 0.6849, train F loss: -0.6656, acc 0.9940\n",
            "epoch 3734: train D loss: 0.6850, train F loss: -0.6729, acc 0.9962\n",
            "epoch 3735: train D loss: 0.6844, train F loss: -0.6672, acc 0.9960\n",
            "epoch 3736: train D loss: 0.6838, train F loss: -0.6678, acc 0.9950\n",
            "epoch 3737: train D loss: 0.6844, train F loss: -0.6698, acc 0.9946\n",
            "epoch 3738: train D loss: 0.6847, train F loss: -0.6613, acc 0.9938\n",
            "epoch 3739: train D loss: 0.6853, train F loss: -0.6695, acc 0.9944\n",
            "epoch 3740: train D loss: 0.6859, train F loss: -0.6685, acc 0.9950\n",
            "epoch 3741: train D loss: 0.6876, train F loss: -0.6390, acc 0.9930\n",
            "epoch 3742: train D loss: 0.6884, train F loss: -0.6668, acc 0.9944\n",
            "epoch 3743: train D loss: 0.6840, train F loss: -0.6695, acc 0.9964\n",
            "epoch 3744: train D loss: 0.6843, train F loss: -0.6652, acc 0.9946\n",
            "epoch 3745: train D loss: 0.6851, train F loss: -0.6660, acc 0.9954\n",
            "epoch 3746: train D loss: 0.6896, train F loss: -0.6750, acc 0.9960\n",
            "epoch 3747: train D loss: 0.6851, train F loss: -0.6692, acc 0.9950\n",
            "epoch 3748: train D loss: 0.6869, train F loss: -0.6700, acc 0.9952\n",
            "epoch 3749: train D loss: 0.6875, train F loss: -0.6731, acc 0.9964\n",
            "epoch 3750: train D loss: 0.6877, train F loss: -0.6693, acc 0.9952\n",
            "epoch 3751: train D loss: 0.6854, train F loss: -0.6627, acc 0.9950\n",
            "epoch 3752: train D loss: 0.6876, train F loss: -0.6687, acc 0.9940\n",
            "epoch 3753: train D loss: 0.6850, train F loss: -0.6645, acc 0.9948\n",
            "epoch 3754: train D loss: 0.6835, train F loss: -0.6652, acc 0.9948\n",
            "epoch 3755: train D loss: 0.6865, train F loss: -0.6668, acc 0.9954\n",
            "epoch 3756: train D loss: 0.6851, train F loss: -0.6668, acc 0.9954\n",
            "epoch 3757: train D loss: 0.6859, train F loss: -0.6689, acc 0.9952\n",
            "epoch 3758: train D loss: 0.6859, train F loss: -0.6429, acc 0.9902\n",
            "epoch 3759: train D loss: 0.6851, train F loss: -0.6661, acc 0.9952\n",
            "epoch 3760: train D loss: 0.6863, train F loss: -0.6687, acc 0.9940\n",
            "epoch 3761: train D loss: 0.6856, train F loss: -0.6731, acc 0.9970\n",
            "epoch 3762: train D loss: 0.6877, train F loss: -0.6705, acc 0.9952\n",
            "epoch 3763: train D loss: 0.6857, train F loss: -0.6585, acc 0.9938\n",
            "epoch 3764: train D loss: 0.6859, train F loss: -0.6687, acc 0.9946\n",
            "epoch 3765: train D loss: 0.6816, train F loss: -0.6155, acc 0.9914\n",
            "epoch 3766: train D loss: 0.6856, train F loss: -0.6531, acc 0.9918\n",
            "epoch 3767: train D loss: 0.6818, train F loss: -0.6652, acc 0.9950\n",
            "epoch 3768: train D loss: 0.6827, train F loss: -0.6639, acc 0.9952\n",
            "epoch 3769: train D loss: 0.6838, train F loss: -0.6645, acc 0.9954\n",
            "epoch 3770: train D loss: 0.6865, train F loss: -0.6714, acc 0.9956\n",
            "epoch 3771: train D loss: 0.6839, train F loss: -0.6705, acc 0.9964\n",
            "epoch 3772: train D loss: 0.6881, train F loss: -0.6698, acc 0.9940\n",
            "epoch 3773: train D loss: 0.6812, train F loss: -0.6666, acc 0.9950\n",
            "epoch 3774: train D loss: 0.6854, train F loss: -0.6695, acc 0.9964\n",
            "epoch 3775: train D loss: 0.6858, train F loss: -0.6631, acc 0.9940\n",
            "epoch 3776: train D loss: 0.6834, train F loss: -0.6680, acc 0.9960\n",
            "epoch 3777: train D loss: 0.6845, train F loss: -0.6655, acc 0.9952\n",
            "epoch 3778: train D loss: 0.6857, train F loss: -0.6682, acc 0.9946\n",
            "epoch 3779: train D loss: 0.6885, train F loss: -0.6699, acc 0.9952\n",
            "epoch 3780: train D loss: 0.6828, train F loss: -0.6628, acc 0.9952\n",
            "epoch 3781: train D loss: 0.6840, train F loss: -0.6654, acc 0.9956\n",
            "epoch 3782: train D loss: 0.6846, train F loss: -0.6671, acc 0.9956\n",
            "epoch 3783: train D loss: 0.6855, train F loss: -0.6707, acc 0.9960\n",
            "epoch 3784: train D loss: 0.6881, train F loss: -0.6745, acc 0.9958\n",
            "epoch 3785: train D loss: 0.6846, train F loss: -0.6643, acc 0.9952\n",
            "epoch 3786: train D loss: 0.6861, train F loss: -0.6695, acc 0.9942\n",
            "epoch 3787: train D loss: 0.6872, train F loss: -0.6694, acc 0.9956\n",
            "epoch 3788: train D loss: 0.6862, train F loss: -0.6624, acc 0.9942\n",
            "epoch 3789: train D loss: 0.6854, train F loss: -0.6655, acc 0.9946\n",
            "epoch 3790: train D loss: 0.6880, train F loss: -0.6581, acc 0.9934\n",
            "epoch 3791: train D loss: 0.6875, train F loss: -0.6462, acc 0.9910\n",
            "epoch 3792: train D loss: 0.6861, train F loss: -0.6562, acc 0.9936\n",
            "epoch 3793: train D loss: 0.6832, train F loss: -0.6666, acc 0.9950\n",
            "epoch 3794: train D loss: 0.6839, train F loss: -0.6615, acc 0.9940\n",
            "epoch 3795: train D loss: 0.6837, train F loss: -0.6638, acc 0.9952\n",
            "epoch 3796: train D loss: 0.6830, train F loss: -0.6647, acc 0.9954\n",
            "epoch 3797: train D loss: 0.6848, train F loss: -0.6719, acc 0.9968\n",
            "epoch 3798: train D loss: 0.6871, train F loss: -0.6678, acc 0.9942\n",
            "epoch 3799: train D loss: 0.6840, train F loss: -0.6565, acc 0.9940\n",
            "epoch 3800: train D loss: 0.6867, train F loss: -0.6450, acc 0.9896\n",
            "epoch 3801: train D loss: 0.6847, train F loss: -0.6620, acc 0.9934\n",
            "epoch 3802: train D loss: 0.6818, train F loss: -0.6620, acc 0.9950\n",
            "epoch 3803: train D loss: 0.6862, train F loss: -0.6655, acc 0.9934\n",
            "epoch 3804: train D loss: 0.6821, train F loss: -0.6586, acc 0.9950\n",
            "epoch 3805: train D loss: 0.6802, train F loss: -0.6605, acc 0.9952\n",
            "epoch 3806: train D loss: 0.6849, train F loss: -0.6704, acc 0.9962\n",
            "epoch 3807: train D loss: 0.6823, train F loss: -0.6694, acc 0.9960\n",
            "epoch 3808: train D loss: 0.6858, train F loss: -0.6672, acc 0.9960\n",
            "epoch 3809: train D loss: 0.6866, train F loss: -0.6705, acc 0.9960\n",
            "epoch 3810: train D loss: 0.6834, train F loss: -0.6656, acc 0.9944\n",
            "epoch 3811: train D loss: 0.6857, train F loss: -0.6527, acc 0.9932\n",
            "epoch 3812: train D loss: 0.6830, train F loss: -0.6421, acc 0.9932\n",
            "epoch 3813: train D loss: 0.6846, train F loss: -0.6680, acc 0.9942\n",
            "epoch 3814: train D loss: 0.6835, train F loss: -0.6676, acc 0.9952\n",
            "epoch 3815: train D loss: 0.6850, train F loss: -0.6714, acc 0.9966\n",
            "epoch 3816: train D loss: 0.6866, train F loss: -0.6721, acc 0.9956\n",
            "epoch 3817: train D loss: 0.6862, train F loss: -0.6740, acc 0.9970\n",
            "epoch 3818: train D loss: 0.6833, train F loss: -0.6671, acc 0.9958\n",
            "epoch 3819: train D loss: 0.6832, train F loss: -0.6525, acc 0.9934\n",
            "epoch 3820: train D loss: 0.6841, train F loss: -0.6643, acc 0.9952\n",
            "epoch 3821: train D loss: 0.6818, train F loss: -0.6635, acc 0.9948\n",
            "epoch 3822: train D loss: 0.6839, train F loss: -0.6714, acc 0.9970\n",
            "epoch 3823: train D loss: 0.6867, train F loss: -0.6674, acc 0.9948\n",
            "epoch 3824: train D loss: 0.6836, train F loss: -0.6641, acc 0.9952\n",
            "epoch 3825: train D loss: 0.6843, train F loss: -0.6403, acc 0.9904\n",
            "epoch 3826: train D loss: 0.6832, train F loss: -0.6642, acc 0.9942\n",
            "epoch 3827: train D loss: 0.6836, train F loss: -0.6650, acc 0.9950\n",
            "epoch 3828: train D loss: 0.6813, train F loss: -0.6603, acc 0.9960\n",
            "epoch 3829: train D loss: 0.6827, train F loss: -0.6631, acc 0.9954\n",
            "epoch 3830: train D loss: 0.6809, train F loss: -0.6570, acc 0.9944\n",
            "epoch 3831: train D loss: 0.6879, train F loss: -0.6739, acc 0.9958\n",
            "epoch 3832: train D loss: 0.6834, train F loss: -0.6695, acc 0.9962\n",
            "epoch 3833: train D loss: 0.6888, train F loss: -0.6743, acc 0.9962\n",
            "epoch 3834: train D loss: 0.6861, train F loss: -0.6745, acc 0.9970\n",
            "epoch 3835: train D loss: 0.6850, train F loss: -0.6711, acc 0.9964\n",
            "epoch 3836: train D loss: 0.6841, train F loss: -0.6710, acc 0.9964\n",
            "epoch 3837: train D loss: 0.6845, train F loss: -0.6729, acc 0.9968\n",
            "epoch 3838: train D loss: 0.6844, train F loss: -0.6620, acc 0.9944\n",
            "epoch 3839: train D loss: 0.6865, train F loss: -0.6720, acc 0.9958\n",
            "epoch 3840: train D loss: 0.6846, train F loss: -0.6716, acc 0.9958\n",
            "epoch 3841: train D loss: 0.6864, train F loss: -0.6699, acc 0.9950\n",
            "epoch 3842: train D loss: 0.6864, train F loss: -0.6701, acc 0.9946\n",
            "epoch 3843: train D loss: 0.6890, train F loss: -0.6439, acc 0.9912\n",
            "epoch 3844: train D loss: 0.6834, train F loss: -0.6631, acc 0.9936\n",
            "epoch 3845: train D loss: 0.6828, train F loss: -0.6662, acc 0.9950\n",
            "epoch 3846: train D loss: 0.6879, train F loss: -0.6632, acc 0.9932\n",
            "epoch 3847: train D loss: 0.6871, train F loss: -0.6717, acc 0.9954\n",
            "epoch 3848: train D loss: 0.6844, train F loss: -0.6707, acc 0.9962\n",
            "epoch 3849: train D loss: 0.6832, train F loss: -0.6391, acc 0.9922\n",
            "epoch 3850: train D loss: 0.6851, train F loss: -0.6637, acc 0.9944\n",
            "epoch 3851: train D loss: 0.6864, train F loss: -0.6725, acc 0.9964\n",
            "epoch 3852: train D loss: 0.6832, train F loss: -0.6660, acc 0.9946\n",
            "epoch 3853: train D loss: 0.6847, train F loss: -0.6669, acc 0.9960\n",
            "epoch 3854: train D loss: 0.6828, train F loss: -0.6653, acc 0.9950\n",
            "epoch 3855: train D loss: 0.6870, train F loss: -0.6729, acc 0.9960\n",
            "epoch 3856: train D loss: 0.6862, train F loss: -0.6715, acc 0.9954\n",
            "epoch 3857: train D loss: 0.6876, train F loss: -0.6677, acc 0.9950\n",
            "epoch 3858: train D loss: 0.6841, train F loss: -0.6706, acc 0.9962\n",
            "epoch 3859: train D loss: 0.6889, train F loss: -0.6610, acc 0.9942\n",
            "epoch 3860: train D loss: 0.6854, train F loss: -0.6657, acc 0.9940\n",
            "epoch 3861: train D loss: 0.6865, train F loss: -0.6726, acc 0.9956\n",
            "epoch 3862: train D loss: 0.6854, train F loss: -0.6690, acc 0.9958\n",
            "epoch 3863: train D loss: 0.6863, train F loss: -0.6728, acc 0.9966\n",
            "epoch 3864: train D loss: 0.6861, train F loss: -0.6743, acc 0.9966\n",
            "epoch 3865: train D loss: 0.6866, train F loss: -0.6670, acc 0.9942\n",
            "epoch 3866: train D loss: 0.6853, train F loss: -0.6598, acc 0.9922\n",
            "epoch 3867: train D loss: 0.6866, train F loss: -0.6703, acc 0.9956\n",
            "epoch 3868: train D loss: 0.6842, train F loss: -0.6629, acc 0.9940\n",
            "epoch 3869: train D loss: 0.6859, train F loss: -0.6618, acc 0.9932\n",
            "epoch 3870: train D loss: 0.6854, train F loss: -0.6727, acc 0.9958\n",
            "epoch 3871: train D loss: 0.6897, train F loss: -0.6647, acc 0.9932\n",
            "epoch 3872: train D loss: 0.6850, train F loss: -0.6677, acc 0.9952\n",
            "epoch 3873: train D loss: 0.6826, train F loss: -0.6643, acc 0.9952\n",
            "epoch 3874: train D loss: 0.6861, train F loss: -0.6722, acc 0.9958\n",
            "epoch 3875: train D loss: 0.6883, train F loss: -0.6750, acc 0.9966\n",
            "epoch 3876: train D loss: 0.6870, train F loss: -0.6731, acc 0.9958\n",
            "epoch 3877: train D loss: 0.6866, train F loss: -0.6639, acc 0.9958\n",
            "epoch 3878: train D loss: 0.6870, train F loss: -0.6531, acc 0.9934\n",
            "epoch 3879: train D loss: 0.6847, train F loss: -0.6668, acc 0.9956\n",
            "epoch 3880: train D loss: 0.6852, train F loss: -0.6630, acc 0.9940\n",
            "epoch 3881: train D loss: 0.6865, train F loss: -0.6707, acc 0.9950\n",
            "epoch 3882: train D loss: 0.6859, train F loss: -0.6698, acc 0.9958\n",
            "epoch 3883: train D loss: 0.6848, train F loss: -0.6711, acc 0.9966\n",
            "epoch 3884: train D loss: 0.6840, train F loss: -0.6702, acc 0.9964\n",
            "epoch 3885: train D loss: 0.6847, train F loss: -0.6673, acc 0.9956\n",
            "epoch 3886: train D loss: 0.6898, train F loss: -0.6379, acc 0.9900\n",
            "epoch 3887: train D loss: 0.6863, train F loss: -0.6646, acc 0.9936\n",
            "epoch 3888: train D loss: 0.6831, train F loss: -0.6646, acc 0.9950\n",
            "epoch 3889: train D loss: 0.6848, train F loss: -0.6708, acc 0.9968\n",
            "epoch 3890: train D loss: 0.6847, train F loss: -0.6723, acc 0.9970\n",
            "epoch 3891: train D loss: 0.6866, train F loss: -0.6722, acc 0.9962\n",
            "epoch 3892: train D loss: 0.6840, train F loss: -0.6665, acc 0.9946\n",
            "epoch 3893: train D loss: 0.6846, train F loss: -0.6721, acc 0.9966\n",
            "epoch 3894: train D loss: 0.6835, train F loss: -0.6682, acc 0.9960\n",
            "epoch 3895: train D loss: 0.6851, train F loss: -0.6721, acc 0.9968\n",
            "epoch 3896: train D loss: 0.6868, train F loss: -0.6699, acc 0.9950\n",
            "epoch 3897: train D loss: 0.6842, train F loss: -0.6693, acc 0.9962\n",
            "epoch 3898: train D loss: 0.6874, train F loss: -0.6645, acc 0.9940\n",
            "epoch 3899: train D loss: 0.6866, train F loss: -0.6689, acc 0.9954\n",
            "epoch 3900: train D loss: 0.6864, train F loss: -0.6688, acc 0.9954\n",
            "epoch 3901: train D loss: 0.6846, train F loss: -0.6674, acc 0.9952\n",
            "epoch 3902: train D loss: 0.6876, train F loss: -0.6691, acc 0.9946\n",
            "epoch 3903: train D loss: 0.6884, train F loss: -0.6676, acc 0.9942\n",
            "epoch 3904: train D loss: 0.6855, train F loss: -0.6656, acc 0.9950\n",
            "epoch 3905: train D loss: 0.6833, train F loss: -0.6657, acc 0.9952\n",
            "epoch 3906: train D loss: 0.6846, train F loss: -0.6672, acc 0.9946\n",
            "epoch 3907: train D loss: 0.6851, train F loss: -0.6664, acc 0.9946\n",
            "epoch 3908: train D loss: 0.6858, train F loss: -0.6728, acc 0.9962\n",
            "epoch 3909: train D loss: 0.6822, train F loss: -0.6618, acc 0.9944\n",
            "epoch 3910: train D loss: 0.6870, train F loss: -0.6713, acc 0.9962\n",
            "epoch 3911: train D loss: 0.6875, train F loss: -0.6684, acc 0.9944\n",
            "epoch 3912: train D loss: 0.6859, train F loss: -0.6645, acc 0.9936\n",
            "epoch 3913: train D loss: 0.6851, train F loss: -0.6662, acc 0.9946\n",
            "epoch 3914: train D loss: 0.6840, train F loss: -0.6713, acc 0.9966\n",
            "epoch 3915: train D loss: 0.6866, train F loss: -0.6701, acc 0.9962\n",
            "epoch 3916: train D loss: 0.6842, train F loss: -0.6715, acc 0.9962\n",
            "epoch 3917: train D loss: 0.6874, train F loss: -0.6748, acc 0.9962\n",
            "epoch 3918: train D loss: 0.6841, train F loss: -0.6681, acc 0.9962\n",
            "epoch 3919: train D loss: 0.6856, train F loss: -0.6524, acc 0.9910\n",
            "epoch 3920: train D loss: 0.6858, train F loss: -0.6680, acc 0.9952\n",
            "epoch 3921: train D loss: 0.6859, train F loss: -0.6678, acc 0.9954\n",
            "epoch 3922: train D loss: 0.6856, train F loss: -0.6695, acc 0.9960\n",
            "epoch 3923: train D loss: 0.6828, train F loss: -0.6608, acc 0.9954\n",
            "epoch 3924: train D loss: 0.6882, train F loss: -0.6554, acc 0.9910\n",
            "epoch 3925: train D loss: 0.6837, train F loss: -0.6661, acc 0.9948\n",
            "epoch 3926: train D loss: 0.6835, train F loss: -0.6652, acc 0.9954\n",
            "epoch 3927: train D loss: 0.6869, train F loss: -0.6665, acc 0.9952\n",
            "epoch 3928: train D loss: 0.6844, train F loss: -0.6676, acc 0.9958\n",
            "epoch 3929: train D loss: 0.6869, train F loss: -0.6756, acc 0.9968\n",
            "epoch 3930: train D loss: 0.6860, train F loss: -0.6695, acc 0.9956\n",
            "epoch 3931: train D loss: 0.6842, train F loss: -0.6648, acc 0.9952\n",
            "epoch 3932: train D loss: 0.6859, train F loss: -0.6681, acc 0.9956\n",
            "epoch 3933: train D loss: 0.6882, train F loss: -0.6736, acc 0.9964\n",
            "epoch 3934: train D loss: 0.6853, train F loss: -0.6689, acc 0.9954\n",
            "epoch 3935: train D loss: 0.6844, train F loss: -0.6689, acc 0.9954\n",
            "epoch 3936: train D loss: 0.6878, train F loss: -0.6687, acc 0.9942\n",
            "epoch 3937: train D loss: 0.6848, train F loss: -0.6534, acc 0.9944\n",
            "epoch 3938: train D loss: 0.6820, train F loss: -0.6532, acc 0.9928\n",
            "epoch 3939: train D loss: 0.6803, train F loss: -0.6658, acc 0.9962\n",
            "epoch 3940: train D loss: 0.6845, train F loss: -0.6663, acc 0.9956\n",
            "epoch 3941: train D loss: 0.6850, train F loss: -0.6714, acc 0.9960\n",
            "epoch 3942: train D loss: 0.6854, train F loss: -0.6730, acc 0.9966\n",
            "epoch 3943: train D loss: 0.6859, train F loss: -0.6690, acc 0.9962\n",
            "epoch 3944: train D loss: 0.6861, train F loss: -0.6282, acc 0.9894\n",
            "epoch 3945: train D loss: 0.6816, train F loss: -0.6637, acc 0.9950\n",
            "epoch 3946: train D loss: 0.6853, train F loss: -0.6681, acc 0.9962\n",
            "epoch 3947: train D loss: 0.6869, train F loss: -0.6691, acc 0.9954\n",
            "epoch 3948: train D loss: 0.6858, train F loss: -0.6720, acc 0.9966\n",
            "epoch 3949: train D loss: 0.6823, train F loss: -0.6712, acc 0.9970\n",
            "epoch 3950: train D loss: 0.6847, train F loss: -0.6708, acc 0.9960\n",
            "epoch 3951: train D loss: 0.6856, train F loss: -0.6624, acc 0.9934\n",
            "epoch 3952: train D loss: 0.6832, train F loss: -0.6679, acc 0.9956\n",
            "epoch 3953: train D loss: 0.6859, train F loss: -0.6683, acc 0.9958\n",
            "epoch 3954: train D loss: 0.6852, train F loss: -0.6697, acc 0.9956\n",
            "epoch 3955: train D loss: 0.6885, train F loss: -0.6755, acc 0.9962\n",
            "epoch 3956: train D loss: 0.6854, train F loss: -0.6668, acc 0.9950\n",
            "epoch 3957: train D loss: 0.6870, train F loss: -0.6702, acc 0.9956\n",
            "epoch 3958: train D loss: 0.6868, train F loss: -0.6700, acc 0.9954\n",
            "epoch 3959: train D loss: 0.6846, train F loss: -0.6586, acc 0.9938\n",
            "epoch 3960: train D loss: 0.6871, train F loss: -0.6693, acc 0.9942\n",
            "epoch 3961: train D loss: 0.6855, train F loss: -0.6673, acc 0.9952\n",
            "epoch 3962: train D loss: 0.6869, train F loss: -0.6743, acc 0.9962\n",
            "epoch 3963: train D loss: 0.6877, train F loss: -0.6705, acc 0.9952\n",
            "epoch 3964: train D loss: 0.6862, train F loss: -0.6721, acc 0.9960\n",
            "epoch 3965: train D loss: 0.6872, train F loss: -0.6765, acc 0.9972\n",
            "epoch 3966: train D loss: 0.6855, train F loss: -0.6727, acc 0.9966\n",
            "epoch 3967: train D loss: 0.6862, train F loss: -0.6739, acc 0.9968\n",
            "epoch 3968: train D loss: 0.6875, train F loss: -0.6698, acc 0.9950\n",
            "epoch 3969: train D loss: 0.6840, train F loss: -0.6595, acc 0.9944\n",
            "epoch 3970: train D loss: 0.6847, train F loss: -0.6695, acc 0.9958\n",
            "epoch 3971: train D loss: 0.6846, train F loss: -0.6710, acc 0.9962\n",
            "epoch 3972: train D loss: 0.6884, train F loss: -0.6654, acc 0.9944\n",
            "epoch 3973: train D loss: 0.6858, train F loss: -0.6652, acc 0.9954\n",
            "epoch 3974: train D loss: 0.6863, train F loss: -0.6716, acc 0.9954\n",
            "epoch 3975: train D loss: 0.6877, train F loss: -0.6731, acc 0.9956\n",
            "epoch 3976: train D loss: 0.6872, train F loss: -0.6682, acc 0.9948\n",
            "epoch 3977: train D loss: 0.6893, train F loss: -0.6735, acc 0.9962\n",
            "epoch 3978: train D loss: 0.6866, train F loss: -0.6698, acc 0.9958\n",
            "epoch 3979: train D loss: 0.6854, train F loss: -0.6581, acc 0.9932\n",
            "epoch 3980: train D loss: 0.6870, train F loss: -0.6706, acc 0.9952\n",
            "epoch 3981: train D loss: 0.6859, train F loss: -0.6697, acc 0.9950\n",
            "epoch 3982: train D loss: 0.6848, train F loss: -0.6704, acc 0.9964\n",
            "epoch 3983: train D loss: 0.6853, train F loss: -0.6685, acc 0.9944\n",
            "epoch 3984: train D loss: 0.6837, train F loss: -0.6546, acc 0.9918\n",
            "epoch 3985: train D loss: 0.6856, train F loss: -0.6664, acc 0.9946\n",
            "epoch 3986: train D loss: 0.6854, train F loss: -0.6649, acc 0.9944\n",
            "epoch 3987: train D loss: 0.6870, train F loss: -0.6705, acc 0.9950\n",
            "epoch 3988: train D loss: 0.6837, train F loss: -0.6712, acc 0.9968\n",
            "epoch 3989: train D loss: 0.6859, train F loss: -0.6741, acc 0.9960\n",
            "epoch 3990: train D loss: 0.6857, train F loss: -0.6746, acc 0.9968\n",
            "epoch 3991: train D loss: 0.6857, train F loss: -0.6544, acc 0.9922\n",
            "epoch 3992: train D loss: 0.6857, train F loss: -0.6501, acc 0.9900\n",
            "epoch 3993: train D loss: 0.6852, train F loss: -0.6558, acc 0.9932\n",
            "epoch 3994: train D loss: 0.6859, train F loss: -0.6578, acc 0.9922\n",
            "epoch 3995: train D loss: 0.6850, train F loss: -0.6699, acc 0.9964\n",
            "epoch 3996: train D loss: 0.6836, train F loss: -0.6705, acc 0.9966\n",
            "epoch 3997: train D loss: 0.6857, train F loss: -0.6704, acc 0.9956\n",
            "epoch 3998: train D loss: 0.6871, train F loss: -0.6721, acc 0.9960\n",
            "epoch 3999: train D loss: 0.6836, train F loss: -0.6689, acc 0.9962\n",
            "epoch 4000: train D loss: 0.6847, train F loss: -0.6646, acc 0.9948\n",
            "epoch 4001: train D loss: 0.6869, train F loss: -0.6723, acc 0.9960\n",
            "epoch 4002: train D loss: 0.6863, train F loss: -0.6665, acc 0.9950\n",
            "epoch 4003: train D loss: 0.6856, train F loss: -0.6692, acc 0.9954\n",
            "epoch 4004: train D loss: 0.6874, train F loss: -0.6469, acc 0.9924\n",
            "epoch 4005: train D loss: 0.6872, train F loss: -0.6587, acc 0.9930\n",
            "epoch 4006: train D loss: 0.6841, train F loss: -0.6662, acc 0.9944\n",
            "epoch 4007: train D loss: 0.6834, train F loss: -0.6689, acc 0.9956\n",
            "epoch 4008: train D loss: 0.6864, train F loss: -0.6695, acc 0.9956\n",
            "epoch 4009: train D loss: 0.6814, train F loss: -0.6641, acc 0.9954\n",
            "epoch 4010: train D loss: 0.6835, train F loss: -0.6704, acc 0.9968\n",
            "epoch 4011: train D loss: 0.6865, train F loss: -0.6640, acc 0.9956\n",
            "epoch 4012: train D loss: 0.6823, train F loss: -0.6613, acc 0.9944\n",
            "epoch 4013: train D loss: 0.6869, train F loss: -0.6706, acc 0.9958\n",
            "epoch 4014: train D loss: 0.6857, train F loss: -0.6732, acc 0.9970\n",
            "epoch 4015: train D loss: 0.6857, train F loss: -0.6649, acc 0.9946\n",
            "epoch 4016: train D loss: 0.6833, train F loss: -0.6622, acc 0.9952\n",
            "epoch 4017: train D loss: 0.6839, train F loss: -0.6641, acc 0.9944\n",
            "epoch 4018: train D loss: 0.6847, train F loss: -0.6633, acc 0.9952\n",
            "epoch 4019: train D loss: 0.6833, train F loss: -0.6657, acc 0.9940\n",
            "epoch 4020: train D loss: 0.6888, train F loss: -0.6706, acc 0.9938\n",
            "epoch 4021: train D loss: 0.6870, train F loss: -0.6745, acc 0.9966\n",
            "epoch 4022: train D loss: 0.6836, train F loss: -0.6705, acc 0.9960\n",
            "epoch 4023: train D loss: 0.6856, train F loss: -0.6706, acc 0.9962\n",
            "epoch 4024: train D loss: 0.6892, train F loss: -0.6688, acc 0.9940\n",
            "epoch 4025: train D loss: 0.6866, train F loss: -0.6693, acc 0.9956\n",
            "epoch 4026: train D loss: 0.6872, train F loss: -0.6711, acc 0.9958\n",
            "epoch 4027: train D loss: 0.6840, train F loss: -0.6643, acc 0.9950\n",
            "epoch 4028: train D loss: 0.6856, train F loss: -0.6662, acc 0.9954\n",
            "epoch 4029: train D loss: 0.6859, train F loss: -0.6732, acc 0.9966\n",
            "epoch 4030: train D loss: 0.6872, train F loss: -0.6722, acc 0.9952\n",
            "epoch 4031: train D loss: 0.6842, train F loss: -0.6707, acc 0.9954\n",
            "epoch 4032: train D loss: 0.6884, train F loss: -0.6730, acc 0.9962\n",
            "epoch 4033: train D loss: 0.6861, train F loss: -0.6610, acc 0.9940\n",
            "epoch 4034: train D loss: 0.6859, train F loss: -0.6668, acc 0.9948\n",
            "epoch 4035: train D loss: 0.6872, train F loss: -0.6665, acc 0.9944\n",
            "epoch 4036: train D loss: 0.6824, train F loss: -0.6661, acc 0.9954\n",
            "epoch 4037: train D loss: 0.6860, train F loss: -0.6693, acc 0.9946\n",
            "epoch 4038: train D loss: 0.6845, train F loss: -0.6661, acc 0.9940\n",
            "epoch 4039: train D loss: 0.6884, train F loss: -0.6676, acc 0.9948\n",
            "epoch 4040: train D loss: 0.6853, train F loss: -0.6731, acc 0.9964\n",
            "epoch 4041: train D loss: 0.6847, train F loss: -0.6634, acc 0.9950\n",
            "epoch 4042: train D loss: 0.6848, train F loss: -0.6710, acc 0.9962\n",
            "epoch 4043: train D loss: 0.6866, train F loss: -0.6732, acc 0.9960\n",
            "epoch 4044: train D loss: 0.6882, train F loss: -0.6740, acc 0.9958\n",
            "epoch 4045: train D loss: 0.6854, train F loss: -0.6723, acc 0.9960\n",
            "epoch 4046: train D loss: 0.6858, train F loss: -0.6623, acc 0.9948\n",
            "epoch 4047: train D loss: 0.6870, train F loss: -0.6691, acc 0.9948\n",
            "epoch 4048: train D loss: 0.6892, train F loss: -0.6666, acc 0.9944\n",
            "epoch 4049: train D loss: 0.6855, train F loss: -0.6654, acc 0.9946\n",
            "epoch 4050: train D loss: 0.6841, train F loss: -0.6682, acc 0.9950\n",
            "epoch 4051: train D loss: 0.6844, train F loss: -0.6691, acc 0.9958\n",
            "epoch 4052: train D loss: 0.6838, train F loss: -0.6661, acc 0.9946\n",
            "epoch 4053: train D loss: 0.6854, train F loss: -0.6672, acc 0.9932\n",
            "epoch 4054: train D loss: 0.6850, train F loss: -0.6594, acc 0.9940\n",
            "epoch 4055: train D loss: 0.6857, train F loss: -0.6722, acc 0.9960\n",
            "epoch 4056: train D loss: 0.6846, train F loss: -0.6670, acc 0.9950\n",
            "epoch 4057: train D loss: 0.6882, train F loss: -0.6725, acc 0.9960\n",
            "epoch 4058: train D loss: 0.6871, train F loss: -0.6713, acc 0.9954\n",
            "epoch 4059: train D loss: 0.6840, train F loss: -0.6683, acc 0.9956\n",
            "epoch 4060: train D loss: 0.6849, train F loss: -0.6720, acc 0.9964\n",
            "epoch 4061: train D loss: 0.6848, train F loss: -0.6699, acc 0.9960\n",
            "epoch 4062: train D loss: 0.6855, train F loss: -0.6544, acc 0.9916\n",
            "epoch 4063: train D loss: 0.6870, train F loss: -0.6448, acc 0.9934\n",
            "epoch 4064: train D loss: 0.6853, train F loss: -0.6587, acc 0.9924\n",
            "epoch 4065: train D loss: 0.6795, train F loss: -0.6623, acc 0.9950\n",
            "epoch 4066: train D loss: 0.6847, train F loss: -0.6669, acc 0.9956\n",
            "epoch 4067: train D loss: 0.6849, train F loss: -0.6670, acc 0.9958\n",
            "epoch 4068: train D loss: 0.6847, train F loss: -0.6711, acc 0.9964\n",
            "epoch 4069: train D loss: 0.6859, train F loss: -0.6712, acc 0.9962\n",
            "epoch 4070: train D loss: 0.6880, train F loss: -0.6676, acc 0.9942\n",
            "epoch 4071: train D loss: 0.6875, train F loss: -0.6679, acc 0.9948\n",
            "epoch 4072: train D loss: 0.6857, train F loss: -0.6718, acc 0.9960\n",
            "epoch 4073: train D loss: 0.6872, train F loss: -0.6705, acc 0.9960\n",
            "epoch 4074: train D loss: 0.6865, train F loss: -0.6761, acc 0.9970\n",
            "epoch 4075: train D loss: 0.6869, train F loss: -0.6682, acc 0.9946\n",
            "epoch 4076: train D loss: 0.6877, train F loss: -0.6736, acc 0.9956\n",
            "epoch 4077: train D loss: 0.6871, train F loss: -0.6730, acc 0.9960\n",
            "epoch 4078: train D loss: 0.6871, train F loss: -0.6643, acc 0.9940\n",
            "epoch 4079: train D loss: 0.6878, train F loss: -0.6630, acc 0.9944\n",
            "epoch 4080: train D loss: 0.6846, train F loss: -0.6692, acc 0.9958\n",
            "epoch 4081: train D loss: 0.6859, train F loss: -0.6684, acc 0.9954\n",
            "epoch 4082: train D loss: 0.6854, train F loss: -0.6705, acc 0.9954\n",
            "epoch 4083: train D loss: 0.6869, train F loss: -0.6691, acc 0.9956\n",
            "epoch 4084: train D loss: 0.6861, train F loss: -0.6673, acc 0.9944\n",
            "epoch 4085: train D loss: 0.6858, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4086: train D loss: 0.6816, train F loss: -0.6626, acc 0.9952\n",
            "epoch 4087: train D loss: 0.6862, train F loss: -0.6608, acc 0.9938\n",
            "epoch 4088: train D loss: 0.6841, train F loss: -0.6636, acc 0.9936\n",
            "epoch 4089: train D loss: 0.6879, train F loss: -0.6700, acc 0.9958\n",
            "epoch 4090: train D loss: 0.6851, train F loss: -0.6742, acc 0.9976\n",
            "epoch 4091: train D loss: 0.6884, train F loss: -0.6737, acc 0.9948\n",
            "epoch 4092: train D loss: 0.6868, train F loss: -0.6732, acc 0.9958\n",
            "epoch 4093: train D loss: 0.6861, train F loss: -0.6612, acc 0.9950\n",
            "epoch 4094: train D loss: 0.6831, train F loss: -0.6674, acc 0.9956\n",
            "epoch 4095: train D loss: 0.6869, train F loss: -0.6701, acc 0.9942\n",
            "epoch 4096: train D loss: 0.6844, train F loss: -0.6700, acc 0.9960\n",
            "epoch 4097: train D loss: 0.6867, train F loss: -0.6658, acc 0.9944\n",
            "epoch 4098: train D loss: 0.6839, train F loss: -0.6629, acc 0.9964\n",
            "epoch 4099: train D loss: 0.6850, train F loss: -0.6623, acc 0.9948\n",
            "epoch 4100: train D loss: 0.6839, train F loss: -0.6617, acc 0.9940\n",
            "epoch 4101: train D loss: 0.6834, train F loss: -0.6699, acc 0.9966\n",
            "epoch 4102: train D loss: 0.6877, train F loss: -0.6721, acc 0.9954\n",
            "epoch 4103: train D loss: 0.6853, train F loss: -0.6711, acc 0.9952\n",
            "epoch 4104: train D loss: 0.6860, train F loss: -0.6722, acc 0.9954\n",
            "epoch 4105: train D loss: 0.6849, train F loss: -0.6650, acc 0.9948\n",
            "epoch 4106: train D loss: 0.6838, train F loss: -0.6627, acc 0.9946\n",
            "epoch 4107: train D loss: 0.6888, train F loss: -0.6706, acc 0.9946\n",
            "epoch 4108: train D loss: 0.6844, train F loss: -0.6681, acc 0.9950\n",
            "epoch 4109: train D loss: 0.6862, train F loss: -0.6702, acc 0.9960\n",
            "epoch 4110: train D loss: 0.6858, train F loss: -0.6673, acc 0.9946\n",
            "epoch 4111: train D loss: 0.6852, train F loss: -0.6705, acc 0.9958\n",
            "epoch 4112: train D loss: 0.6845, train F loss: -0.6601, acc 0.9924\n",
            "epoch 4113: train D loss: 0.6845, train F loss: -0.6462, acc 0.9896\n",
            "epoch 4114: train D loss: 0.6834, train F loss: -0.6617, acc 0.9944\n",
            "epoch 4115: train D loss: 0.6844, train F loss: -0.6663, acc 0.9948\n",
            "epoch 4116: train D loss: 0.6826, train F loss: -0.6678, acc 0.9956\n",
            "epoch 4117: train D loss: 0.6859, train F loss: -0.6724, acc 0.9962\n",
            "epoch 4118: train D loss: 0.6819, train F loss: -0.6696, acc 0.9968\n",
            "epoch 4119: train D loss: 0.6878, train F loss: -0.6743, acc 0.9954\n",
            "epoch 4120: train D loss: 0.6847, train F loss: -0.6726, acc 0.9964\n",
            "epoch 4121: train D loss: 0.6870, train F loss: -0.6706, acc 0.9954\n",
            "epoch 4122: train D loss: 0.6890, train F loss: -0.6733, acc 0.9950\n",
            "epoch 4123: train D loss: 0.6874, train F loss: -0.6625, acc 0.9934\n",
            "epoch 4124: train D loss: 0.6845, train F loss: -0.6649, acc 0.9956\n",
            "epoch 4125: train D loss: 0.6863, train F loss: -0.6619, acc 0.9936\n",
            "epoch 4126: train D loss: 0.6847, train F loss: -0.6692, acc 0.9950\n",
            "epoch 4127: train D loss: 0.6837, train F loss: -0.6699, acc 0.9952\n",
            "epoch 4128: train D loss: 0.6878, train F loss: -0.6740, acc 0.9958\n",
            "epoch 4129: train D loss: 0.6857, train F loss: -0.6653, acc 0.9946\n",
            "epoch 4130: train D loss: 0.6853, train F loss: -0.6667, acc 0.9948\n",
            "epoch 4131: train D loss: 0.6859, train F loss: -0.6666, acc 0.9946\n",
            "epoch 4132: train D loss: 0.6856, train F loss: -0.6710, acc 0.9954\n",
            "epoch 4133: train D loss: 0.6807, train F loss: -0.6665, acc 0.9954\n",
            "epoch 4134: train D loss: 0.6865, train F loss: -0.6575, acc 0.9932\n",
            "epoch 4135: train D loss: 0.6844, train F loss: -0.6644, acc 0.9942\n",
            "epoch 4136: train D loss: 0.6856, train F loss: -0.6669, acc 0.9938\n",
            "epoch 4137: train D loss: 0.6895, train F loss: -0.6721, acc 0.9956\n",
            "epoch 4138: train D loss: 0.6851, train F loss: -0.6739, acc 0.9968\n",
            "epoch 4139: train D loss: 0.6860, train F loss: -0.6699, acc 0.9956\n",
            "epoch 4140: train D loss: 0.6850, train F loss: -0.6416, acc 0.9936\n",
            "epoch 4141: train D loss: 0.6828, train F loss: -0.6468, acc 0.9910\n",
            "epoch 4142: train D loss: 0.6833, train F loss: -0.6566, acc 0.9934\n",
            "epoch 4143: train D loss: 0.6868, train F loss: -0.6707, acc 0.9960\n",
            "epoch 4144: train D loss: 0.6835, train F loss: -0.6674, acc 0.9960\n",
            "epoch 4145: train D loss: 0.6864, train F loss: -0.6700, acc 0.9950\n",
            "epoch 4146: train D loss: 0.6851, train F loss: -0.6720, acc 0.9962\n",
            "epoch 4147: train D loss: 0.6855, train F loss: -0.6749, acc 0.9972\n",
            "epoch 4148: train D loss: 0.6854, train F loss: -0.6747, acc 0.9968\n",
            "epoch 4149: train D loss: 0.6860, train F loss: -0.6693, acc 0.9952\n",
            "epoch 4150: train D loss: 0.6832, train F loss: -0.6581, acc 0.9932\n",
            "epoch 4151: train D loss: 0.6882, train F loss: -0.6753, acc 0.9970\n",
            "epoch 4152: train D loss: 0.6857, train F loss: -0.6699, acc 0.9956\n",
            "epoch 4153: train D loss: 0.6849, train F loss: -0.6662, acc 0.9954\n",
            "epoch 4154: train D loss: 0.6876, train F loss: -0.6689, acc 0.9936\n",
            "epoch 4155: train D loss: 0.6880, train F loss: -0.6778, acc 0.9974\n",
            "epoch 4156: train D loss: 0.6843, train F loss: -0.6694, acc 0.9960\n",
            "epoch 4157: train D loss: 0.6878, train F loss: -0.6748, acc 0.9960\n",
            "epoch 4158: train D loss: 0.6866, train F loss: -0.6723, acc 0.9968\n",
            "epoch 4159: train D loss: 0.6858, train F loss: -0.6683, acc 0.9954\n",
            "epoch 4160: train D loss: 0.6887, train F loss: -0.6738, acc 0.9948\n",
            "epoch 4161: train D loss: 0.6880, train F loss: -0.6743, acc 0.9956\n",
            "epoch 4162: train D loss: 0.6858, train F loss: -0.6591, acc 0.9946\n",
            "epoch 4163: train D loss: 0.6832, train F loss: -0.6672, acc 0.9954\n",
            "epoch 4164: train D loss: 0.6841, train F loss: -0.6689, acc 0.9962\n",
            "epoch 4165: train D loss: 0.6870, train F loss: -0.6721, acc 0.9954\n",
            "epoch 4166: train D loss: 0.6866, train F loss: -0.6731, acc 0.9960\n",
            "epoch 4167: train D loss: 0.6846, train F loss: -0.6705, acc 0.9962\n",
            "epoch 4168: train D loss: 0.6879, train F loss: -0.6562, acc 0.9944\n",
            "epoch 4169: train D loss: 0.6862, train F loss: -0.6728, acc 0.9964\n",
            "epoch 4170: train D loss: 0.6850, train F loss: -0.6724, acc 0.9970\n",
            "epoch 4171: train D loss: 0.6859, train F loss: -0.6394, acc 0.9910\n",
            "epoch 4172: train D loss: 0.6818, train F loss: -0.6583, acc 0.9938\n",
            "epoch 4173: train D loss: 0.6866, train F loss: -0.6511, acc 0.9920\n",
            "epoch 4174: train D loss: 0.6875, train F loss: -0.6607, acc 0.9928\n",
            "epoch 4175: train D loss: 0.6828, train F loss: -0.6576, acc 0.9936\n",
            "epoch 4176: train D loss: 0.6827, train F loss: -0.6656, acc 0.9956\n",
            "epoch 4177: train D loss: 0.6884, train F loss: -0.6715, acc 0.9950\n",
            "epoch 4178: train D loss: 0.6864, train F loss: -0.6624, acc 0.9932\n",
            "epoch 4179: train D loss: 0.6837, train F loss: -0.6666, acc 0.9946\n",
            "epoch 4180: train D loss: 0.6812, train F loss: -0.6652, acc 0.9964\n",
            "epoch 4181: train D loss: 0.6833, train F loss: -0.6546, acc 0.9938\n",
            "epoch 4182: train D loss: 0.6860, train F loss: -0.6581, acc 0.9934\n",
            "epoch 4183: train D loss: 0.6859, train F loss: -0.6701, acc 0.9954\n",
            "epoch 4184: train D loss: 0.6850, train F loss: -0.6715, acc 0.9964\n",
            "epoch 4185: train D loss: 0.6825, train F loss: -0.6694, acc 0.9958\n",
            "epoch 4186: train D loss: 0.6835, train F loss: -0.6716, acc 0.9968\n",
            "epoch 4187: train D loss: 0.6859, train F loss: -0.6686, acc 0.9960\n",
            "epoch 4188: train D loss: 0.6842, train F loss: -0.6672, acc 0.9954\n",
            "epoch 4189: train D loss: 0.6837, train F loss: -0.6713, acc 0.9968\n",
            "epoch 4190: train D loss: 0.6826, train F loss: -0.6658, acc 0.9958\n",
            "epoch 4191: train D loss: 0.6867, train F loss: -0.6685, acc 0.9954\n",
            "epoch 4192: train D loss: 0.6850, train F loss: -0.6668, acc 0.9958\n",
            "epoch 4193: train D loss: 0.6856, train F loss: -0.6680, acc 0.9954\n",
            "epoch 4194: train D loss: 0.6843, train F loss: -0.6609, acc 0.9928\n",
            "epoch 4195: train D loss: 0.6857, train F loss: -0.6689, acc 0.9952\n",
            "epoch 4196: train D loss: 0.6862, train F loss: -0.6709, acc 0.9950\n",
            "epoch 4197: train D loss: 0.6848, train F loss: -0.6689, acc 0.9950\n",
            "epoch 4198: train D loss: 0.6838, train F loss: -0.6721, acc 0.9966\n",
            "epoch 4199: train D loss: 0.6823, train F loss: -0.6669, acc 0.9958\n",
            "epoch 4200: train D loss: 0.6850, train F loss: -0.6692, acc 0.9958\n",
            "epoch 4201: train D loss: 0.6818, train F loss: -0.6661, acc 0.9958\n",
            "epoch 4202: train D loss: 0.6883, train F loss: -0.6591, acc 0.9926\n",
            "epoch 4203: train D loss: 0.6850, train F loss: -0.6660, acc 0.9948\n",
            "epoch 4204: train D loss: 0.6834, train F loss: -0.6674, acc 0.9956\n",
            "epoch 4205: train D loss: 0.6890, train F loss: -0.6465, acc 0.9896\n",
            "epoch 4206: train D loss: 0.6879, train F loss: -0.6567, acc 0.9898\n",
            "epoch 4207: train D loss: 0.6865, train F loss: -0.6631, acc 0.9944\n",
            "epoch 4208: train D loss: 0.6829, train F loss: -0.6616, acc 0.9944\n",
            "epoch 4209: train D loss: 0.6834, train F loss: -0.6668, acc 0.9956\n",
            "epoch 4210: train D loss: 0.6823, train F loss: -0.6663, acc 0.9960\n",
            "epoch 4211: train D loss: 0.6860, train F loss: -0.6721, acc 0.9964\n",
            "epoch 4212: train D loss: 0.6833, train F loss: -0.6706, acc 0.9968\n",
            "epoch 4213: train D loss: 0.6884, train F loss: -0.6750, acc 0.9958\n",
            "epoch 4214: train D loss: 0.6842, train F loss: -0.6694, acc 0.9956\n",
            "epoch 4215: train D loss: 0.6874, train F loss: -0.6673, acc 0.9950\n",
            "epoch 4216: train D loss: 0.6847, train F loss: -0.6706, acc 0.9954\n",
            "epoch 4217: train D loss: 0.6899, train F loss: -0.6759, acc 0.9956\n",
            "epoch 4218: train D loss: 0.6870, train F loss: -0.6718, acc 0.9962\n",
            "epoch 4219: train D loss: 0.6885, train F loss: -0.6645, acc 0.9942\n",
            "epoch 4220: train D loss: 0.6865, train F loss: -0.6567, acc 0.9930\n",
            "epoch 4221: train D loss: 0.6836, train F loss: -0.6655, acc 0.9958\n",
            "epoch 4222: train D loss: 0.6872, train F loss: -0.6638, acc 0.9942\n",
            "epoch 4223: train D loss: 0.6862, train F loss: -0.6727, acc 0.9964\n",
            "epoch 4224: train D loss: 0.6872, train F loss: -0.6696, acc 0.9958\n",
            "epoch 4225: train D loss: 0.6857, train F loss: -0.6746, acc 0.9964\n",
            "epoch 4226: train D loss: 0.6865, train F loss: -0.6645, acc 0.9944\n",
            "epoch 4227: train D loss: 0.6839, train F loss: -0.6658, acc 0.9946\n",
            "epoch 4228: train D loss: 0.6835, train F loss: -0.6518, acc 0.9924\n",
            "epoch 4229: train D loss: 0.6819, train F loss: -0.6636, acc 0.9950\n",
            "epoch 4230: train D loss: 0.6810, train F loss: -0.6622, acc 0.9958\n",
            "epoch 4231: train D loss: 0.6852, train F loss: -0.6717, acc 0.9964\n",
            "epoch 4232: train D loss: 0.6816, train F loss: -0.6702, acc 0.9964\n",
            "epoch 4233: train D loss: 0.6894, train F loss: -0.6777, acc 0.9968\n",
            "epoch 4234: train D loss: 0.6858, train F loss: -0.6756, acc 0.9970\n",
            "epoch 4235: train D loss: 0.6896, train F loss: -0.6784, acc 0.9970\n",
            "epoch 4236: train D loss: 0.6878, train F loss: -0.6778, acc 0.9970\n",
            "epoch 4237: train D loss: 0.6873, train F loss: -0.6655, acc 0.9940\n",
            "epoch 4238: train D loss: 0.6865, train F loss: -0.6688, acc 0.9944\n",
            "epoch 4239: train D loss: 0.6828, train F loss: -0.6693, acc 0.9964\n",
            "epoch 4240: train D loss: 0.6862, train F loss: -0.6704, acc 0.9960\n",
            "epoch 4241: train D loss: 0.6866, train F loss: -0.6654, acc 0.9954\n",
            "epoch 4242: train D loss: 0.6880, train F loss: -0.6698, acc 0.9948\n",
            "epoch 4243: train D loss: 0.6844, train F loss: -0.6712, acc 0.9960\n",
            "epoch 4244: train D loss: 0.6865, train F loss: -0.6753, acc 0.9968\n",
            "epoch 4245: train D loss: 0.6875, train F loss: -0.6756, acc 0.9964\n",
            "epoch 4246: train D loss: 0.6859, train F loss: -0.6670, acc 0.9936\n",
            "epoch 4247: train D loss: 0.6881, train F loss: -0.6639, acc 0.9940\n",
            "epoch 4248: train D loss: 0.6848, train F loss: -0.6473, acc 0.9906\n",
            "epoch 4249: train D loss: 0.6862, train F loss: -0.6703, acc 0.9956\n",
            "epoch 4250: train D loss: 0.6848, train F loss: -0.6398, acc 0.9912\n",
            "epoch 4251: train D loss: 0.6834, train F loss: -0.6663, acc 0.9948\n",
            "epoch 4252: train D loss: 0.6880, train F loss: -0.6626, acc 0.9944\n",
            "epoch 4253: train D loss: 0.6844, train F loss: -0.6714, acc 0.9958\n",
            "epoch 4254: train D loss: 0.6834, train F loss: -0.6689, acc 0.9956\n",
            "epoch 4255: train D loss: 0.6855, train F loss: -0.6674, acc 0.9952\n",
            "epoch 4256: train D loss: 0.6887, train F loss: -0.6707, acc 0.9956\n",
            "epoch 4257: train D loss: 0.6830, train F loss: -0.6654, acc 0.9944\n",
            "epoch 4258: train D loss: 0.6864, train F loss: -0.6694, acc 0.9956\n",
            "epoch 4259: train D loss: 0.6869, train F loss: -0.6675, acc 0.9960\n",
            "epoch 4260: train D loss: 0.6872, train F loss: -0.6583, acc 0.9916\n",
            "epoch 4261: train D loss: 0.6877, train F loss: -0.6572, acc 0.9946\n",
            "epoch 4262: train D loss: 0.6849, train F loss: -0.6667, acc 0.9952\n",
            "epoch 4263: train D loss: 0.6854, train F loss: -0.6736, acc 0.9966\n",
            "epoch 4264: train D loss: 0.6888, train F loss: -0.6772, acc 0.9970\n",
            "epoch 4265: train D loss: 0.6870, train F loss: -0.6736, acc 0.9956\n",
            "epoch 4266: train D loss: 0.6878, train F loss: -0.6755, acc 0.9958\n",
            "epoch 4267: train D loss: 0.6853, train F loss: -0.6703, acc 0.9946\n",
            "epoch 4268: train D loss: 0.6854, train F loss: -0.6679, acc 0.9950\n",
            "epoch 4269: train D loss: 0.6871, train F loss: -0.6662, acc 0.9940\n",
            "epoch 4270: train D loss: 0.6867, train F loss: -0.6762, acc 0.9968\n",
            "epoch 4271: train D loss: 0.6863, train F loss: -0.6761, acc 0.9970\n",
            "epoch 4272: train D loss: 0.6847, train F loss: -0.6726, acc 0.9958\n",
            "epoch 4273: train D loss: 0.6852, train F loss: -0.6732, acc 0.9964\n",
            "epoch 4274: train D loss: 0.6898, train F loss: -0.6713, acc 0.9946\n",
            "epoch 4275: train D loss: 0.6866, train F loss: -0.6739, acc 0.9970\n",
            "epoch 4276: train D loss: 0.6854, train F loss: -0.6727, acc 0.9964\n",
            "epoch 4277: train D loss: 0.6862, train F loss: -0.6727, acc 0.9966\n",
            "epoch 4278: train D loss: 0.6887, train F loss: -0.6639, acc 0.9942\n",
            "epoch 4279: train D loss: 0.6837, train F loss: -0.6626, acc 0.9942\n",
            "epoch 4280: train D loss: 0.6856, train F loss: -0.6584, acc 0.9934\n",
            "epoch 4281: train D loss: 0.6857, train F loss: -0.6705, acc 0.9956\n",
            "epoch 4282: train D loss: 0.6889, train F loss: -0.6666, acc 0.9956\n",
            "epoch 4283: train D loss: 0.6866, train F loss: -0.6609, acc 0.9922\n",
            "epoch 4284: train D loss: 0.6853, train F loss: -0.6516, acc 0.9918\n",
            "epoch 4285: train D loss: 0.6822, train F loss: -0.6603, acc 0.9942\n",
            "epoch 4286: train D loss: 0.6857, train F loss: -0.6598, acc 0.9938\n",
            "epoch 4287: train D loss: 0.6879, train F loss: -0.6682, acc 0.9944\n",
            "epoch 4288: train D loss: 0.6834, train F loss: -0.6707, acc 0.9960\n",
            "epoch 4289: train D loss: 0.6865, train F loss: -0.6750, acc 0.9966\n",
            "epoch 4290: train D loss: 0.6858, train F loss: -0.6738, acc 0.9960\n",
            "epoch 4291: train D loss: 0.6867, train F loss: -0.6773, acc 0.9974\n",
            "epoch 4292: train D loss: 0.6864, train F loss: -0.6697, acc 0.9962\n",
            "epoch 4293: train D loss: 0.6874, train F loss: -0.6726, acc 0.9958\n",
            "epoch 4294: train D loss: 0.6878, train F loss: -0.6745, acc 0.9958\n",
            "epoch 4295: train D loss: 0.6864, train F loss: -0.6691, acc 0.9958\n",
            "epoch 4296: train D loss: 0.6859, train F loss: -0.6541, acc 0.9926\n",
            "epoch 4297: train D loss: 0.6813, train F loss: -0.6572, acc 0.9926\n",
            "epoch 4298: train D loss: 0.6830, train F loss: -0.6689, acc 0.9968\n",
            "epoch 4299: train D loss: 0.6876, train F loss: -0.6753, acc 0.9962\n",
            "epoch 4300: train D loss: 0.6850, train F loss: -0.6713, acc 0.9964\n",
            "epoch 4301: train D loss: 0.6854, train F loss: -0.6563, acc 0.9942\n",
            "epoch 4302: train D loss: 0.6865, train F loss: -0.6693, acc 0.9946\n",
            "epoch 4303: train D loss: 0.6846, train F loss: -0.6699, acc 0.9962\n",
            "epoch 4304: train D loss: 0.6851, train F loss: -0.6683, acc 0.9956\n",
            "epoch 4305: train D loss: 0.6858, train F loss: -0.6561, acc 0.9936\n",
            "epoch 4306: train D loss: 0.6874, train F loss: -0.6663, acc 0.9944\n",
            "epoch 4307: train D loss: 0.6858, train F loss: -0.6642, acc 0.9934\n",
            "epoch 4308: train D loss: 0.6843, train F loss: -0.6662, acc 0.9950\n",
            "epoch 4309: train D loss: 0.6853, train F loss: -0.6605, acc 0.9926\n",
            "epoch 4310: train D loss: 0.6873, train F loss: -0.6682, acc 0.9944\n",
            "epoch 4311: train D loss: 0.6822, train F loss: -0.6665, acc 0.9948\n",
            "epoch 4312: train D loss: 0.6832, train F loss: -0.6688, acc 0.9962\n",
            "epoch 4313: train D loss: 0.6866, train F loss: -0.6730, acc 0.9962\n",
            "epoch 4314: train D loss: 0.6852, train F loss: -0.6707, acc 0.9958\n",
            "epoch 4315: train D loss: 0.6854, train F loss: -0.6716, acc 0.9964\n",
            "epoch 4316: train D loss: 0.6855, train F loss: -0.6731, acc 0.9966\n",
            "epoch 4317: train D loss: 0.6842, train F loss: -0.6677, acc 0.9952\n",
            "epoch 4318: train D loss: 0.6885, train F loss: -0.6715, acc 0.9952\n",
            "epoch 4319: train D loss: 0.6880, train F loss: -0.6564, acc 0.9924\n",
            "epoch 4320: train D loss: 0.6894, train F loss: -0.6663, acc 0.9930\n",
            "epoch 4321: train D loss: 0.6868, train F loss: -0.6543, acc 0.9924\n",
            "epoch 4322: train D loss: 0.6847, train F loss: -0.6617, acc 0.9934\n",
            "epoch 4323: train D loss: 0.6844, train F loss: -0.6634, acc 0.9934\n",
            "epoch 4324: train D loss: 0.6854, train F loss: -0.6713, acc 0.9966\n",
            "epoch 4325: train D loss: 0.6845, train F loss: -0.6700, acc 0.9958\n",
            "epoch 4326: train D loss: 0.6841, train F loss: -0.6680, acc 0.9956\n",
            "epoch 4327: train D loss: 0.6880, train F loss: -0.6742, acc 0.9970\n",
            "epoch 4328: train D loss: 0.6874, train F loss: -0.6730, acc 0.9956\n",
            "epoch 4329: train D loss: 0.6882, train F loss: -0.6767, acc 0.9966\n",
            "epoch 4330: train D loss: 0.6850, train F loss: -0.6739, acc 0.9966\n",
            "epoch 4331: train D loss: 0.6881, train F loss: -0.6726, acc 0.9958\n",
            "epoch 4332: train D loss: 0.6836, train F loss: -0.6198, acc 0.9894\n",
            "epoch 4333: train D loss: 0.6790, train F loss: -0.6495, acc 0.9934\n",
            "epoch 4334: train D loss: 0.6858, train F loss: -0.6679, acc 0.9946\n",
            "epoch 4335: train D loss: 0.6892, train F loss: -0.6693, acc 0.9954\n",
            "epoch 4336: train D loss: 0.6827, train F loss: -0.6644, acc 0.9944\n",
            "epoch 4337: train D loss: 0.6841, train F loss: -0.6705, acc 0.9968\n",
            "epoch 4338: train D loss: 0.6835, train F loss: -0.6637, acc 0.9954\n",
            "epoch 4339: train D loss: 0.6836, train F loss: -0.6690, acc 0.9964\n",
            "epoch 4340: train D loss: 0.6839, train F loss: -0.6687, acc 0.9960\n",
            "epoch 4341: train D loss: 0.6861, train F loss: -0.6738, acc 0.9962\n",
            "epoch 4342: train D loss: 0.6876, train F loss: -0.6748, acc 0.9964\n",
            "epoch 4343: train D loss: 0.6863, train F loss: -0.6693, acc 0.9958\n",
            "epoch 4344: train D loss: 0.6893, train F loss: -0.6748, acc 0.9956\n",
            "epoch 4345: train D loss: 0.6850, train F loss: -0.6738, acc 0.9966\n",
            "epoch 4346: train D loss: 0.6843, train F loss: -0.6717, acc 0.9964\n",
            "epoch 4347: train D loss: 0.6851, train F loss: -0.6680, acc 0.9950\n",
            "epoch 4348: train D loss: 0.6883, train F loss: -0.6704, acc 0.9960\n",
            "epoch 4349: train D loss: 0.6849, train F loss: -0.6717, acc 0.9952\n",
            "epoch 4350: train D loss: 0.6848, train F loss: -0.6706, acc 0.9962\n",
            "epoch 4351: train D loss: 0.6857, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4352: train D loss: 0.6831, train F loss: -0.6675, acc 0.9958\n",
            "epoch 4353: train D loss: 0.6867, train F loss: -0.6699, acc 0.9954\n",
            "epoch 4354: train D loss: 0.6858, train F loss: -0.6447, acc 0.9910\n",
            "epoch 4355: train D loss: 0.6845, train F loss: -0.6632, acc 0.9950\n",
            "epoch 4356: train D loss: 0.6842, train F loss: -0.6649, acc 0.9952\n",
            "epoch 4357: train D loss: 0.6855, train F loss: -0.6675, acc 0.9948\n",
            "epoch 4358: train D loss: 0.6874, train F loss: -0.6602, acc 0.9936\n",
            "epoch 4359: train D loss: 0.6814, train F loss: -0.6697, acc 0.9966\n",
            "epoch 4360: train D loss: 0.6877, train F loss: -0.6731, acc 0.9960\n",
            "epoch 4361: train D loss: 0.6872, train F loss: -0.6745, acc 0.9966\n",
            "epoch 4362: train D loss: 0.6847, train F loss: -0.6629, acc 0.9952\n",
            "epoch 4363: train D loss: 0.6844, train F loss: -0.6655, acc 0.9944\n",
            "epoch 4364: train D loss: 0.6838, train F loss: -0.6727, acc 0.9958\n",
            "epoch 4365: train D loss: 0.6851, train F loss: -0.6702, acc 0.9960\n",
            "epoch 4366: train D loss: 0.6856, train F loss: -0.6663, acc 0.9948\n",
            "epoch 4367: train D loss: 0.6855, train F loss: -0.6668, acc 0.9952\n",
            "epoch 4368: train D loss: 0.6864, train F loss: -0.6716, acc 0.9958\n",
            "epoch 4369: train D loss: 0.6846, train F loss: -0.6689, acc 0.9954\n",
            "epoch 4370: train D loss: 0.6868, train F loss: -0.6697, acc 0.9956\n",
            "epoch 4371: train D loss: 0.6866, train F loss: -0.6596, acc 0.9936\n",
            "epoch 4372: train D loss: 0.6859, train F loss: -0.6704, acc 0.9958\n",
            "epoch 4373: train D loss: 0.6866, train F loss: -0.6726, acc 0.9962\n",
            "epoch 4374: train D loss: 0.6872, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4375: train D loss: 0.6879, train F loss: -0.6552, acc 0.9914\n",
            "epoch 4376: train D loss: 0.6840, train F loss: -0.6627, acc 0.9948\n",
            "epoch 4377: train D loss: 0.6846, train F loss: -0.6694, acc 0.9958\n",
            "epoch 4378: train D loss: 0.6852, train F loss: -0.6723, acc 0.9960\n",
            "epoch 4379: train D loss: 0.6823, train F loss: -0.6649, acc 0.9956\n",
            "epoch 4380: train D loss: 0.6880, train F loss: -0.6733, acc 0.9952\n",
            "epoch 4381: train D loss: 0.6862, train F loss: -0.6748, acc 0.9966\n",
            "epoch 4382: train D loss: 0.6858, train F loss: -0.6671, acc 0.9946\n",
            "epoch 4383: train D loss: 0.6844, train F loss: -0.6463, acc 0.9938\n",
            "epoch 4384: train D loss: 0.6858, train F loss: -0.6701, acc 0.9962\n",
            "epoch 4385: train D loss: 0.6811, train F loss: -0.6652, acc 0.9956\n",
            "epoch 4386: train D loss: 0.6856, train F loss: -0.6687, acc 0.9952\n",
            "epoch 4387: train D loss: 0.6849, train F loss: -0.6659, acc 0.9954\n",
            "epoch 4388: train D loss: 0.6831, train F loss: -0.6617, acc 0.9950\n",
            "epoch 4389: train D loss: 0.6857, train F loss: -0.6669, acc 0.9952\n",
            "epoch 4390: train D loss: 0.6851, train F loss: -0.6732, acc 0.9970\n",
            "epoch 4391: train D loss: 0.6867, train F loss: -0.6713, acc 0.9956\n",
            "epoch 4392: train D loss: 0.6867, train F loss: -0.6737, acc 0.9964\n",
            "epoch 4393: train D loss: 0.6866, train F loss: -0.6743, acc 0.9964\n",
            "epoch 4394: train D loss: 0.6854, train F loss: -0.6686, acc 0.9956\n",
            "epoch 4395: train D loss: 0.6883, train F loss: -0.6447, acc 0.9902\n",
            "epoch 4396: train D loss: 0.6876, train F loss: -0.5730, acc 0.9904\n",
            "epoch 4397: train D loss: 0.6831, train F loss: -0.6598, acc 0.9938\n",
            "epoch 4398: train D loss: 0.6819, train F loss: -0.6567, acc 0.9946\n",
            "epoch 4399: train D loss: 0.6842, train F loss: -0.6685, acc 0.9960\n",
            "epoch 4400: train D loss: 0.6834, train F loss: -0.6645, acc 0.9952\n",
            "epoch 4401: train D loss: 0.6856, train F loss: -0.6710, acc 0.9954\n",
            "epoch 4402: train D loss: 0.6867, train F loss: -0.6700, acc 0.9956\n",
            "epoch 4403: train D loss: 0.6872, train F loss: -0.6644, acc 0.9958\n",
            "epoch 4404: train D loss: 0.6871, train F loss: -0.6734, acc 0.9958\n",
            "epoch 4405: train D loss: 0.6852, train F loss: -0.6721, acc 0.9966\n",
            "epoch 4406: train D loss: 0.6836, train F loss: -0.6671, acc 0.9958\n",
            "epoch 4407: train D loss: 0.6864, train F loss: -0.6674, acc 0.9948\n",
            "epoch 4408: train D loss: 0.6859, train F loss: -0.6695, acc 0.9952\n",
            "epoch 4409: train D loss: 0.6856, train F loss: -0.5994, acc 0.9868\n",
            "epoch 4410: train D loss: 0.6846, train F loss: -0.6540, acc 0.9902\n",
            "epoch 4411: train D loss: 0.6828, train F loss: -0.6669, acc 0.9958\n",
            "epoch 4412: train D loss: 0.6833, train F loss: -0.6712, acc 0.9964\n",
            "epoch 4413: train D loss: 0.6816, train F loss: -0.6712, acc 0.9974\n",
            "epoch 4414: train D loss: 0.6848, train F loss: -0.6713, acc 0.9956\n",
            "epoch 4415: train D loss: 0.6843, train F loss: -0.6734, acc 0.9966\n",
            "epoch 4416: train D loss: 0.6831, train F loss: -0.6718, acc 0.9966\n",
            "epoch 4417: train D loss: 0.6853, train F loss: -0.6707, acc 0.9960\n",
            "epoch 4418: train D loss: 0.6875, train F loss: -0.6759, acc 0.9970\n",
            "epoch 4419: train D loss: 0.6840, train F loss: -0.6722, acc 0.9964\n",
            "epoch 4420: train D loss: 0.6861, train F loss: -0.6730, acc 0.9964\n",
            "epoch 4421: train D loss: 0.6859, train F loss: -0.6735, acc 0.9964\n",
            "epoch 4422: train D loss: 0.6842, train F loss: -0.6694, acc 0.9962\n",
            "epoch 4423: train D loss: 0.6870, train F loss: -0.6652, acc 0.9934\n",
            "epoch 4424: train D loss: 0.6841, train F loss: -0.6654, acc 0.9960\n",
            "epoch 4425: train D loss: 0.6866, train F loss: -0.6757, acc 0.9968\n",
            "epoch 4426: train D loss: 0.6844, train F loss: -0.6700, acc 0.9960\n",
            "epoch 4427: train D loss: 0.6846, train F loss: -0.6695, acc 0.9960\n",
            "epoch 4428: train D loss: 0.6862, train F loss: -0.6704, acc 0.9960\n",
            "epoch 4429: train D loss: 0.6884, train F loss: -0.6760, acc 0.9966\n",
            "epoch 4430: train D loss: 0.6856, train F loss: -0.6747, acc 0.9968\n",
            "epoch 4431: train D loss: 0.6864, train F loss: -0.6726, acc 0.9962\n",
            "epoch 4432: train D loss: 0.6875, train F loss: -0.6726, acc 0.9960\n",
            "epoch 4433: train D loss: 0.6888, train F loss: -0.6724, acc 0.9948\n",
            "epoch 4434: train D loss: 0.6879, train F loss: -0.6748, acc 0.9962\n",
            "epoch 4435: train D loss: 0.6867, train F loss: -0.6718, acc 0.9958\n",
            "epoch 4436: train D loss: 0.6862, train F loss: -0.6710, acc 0.9962\n",
            "epoch 4437: train D loss: 0.6859, train F loss: -0.6733, acc 0.9958\n",
            "epoch 4438: train D loss: 0.6873, train F loss: -0.6564, acc 0.9932\n",
            "epoch 4439: train D loss: 0.6869, train F loss: -0.6631, acc 0.9932\n",
            "epoch 4440: train D loss: 0.6837, train F loss: -0.6636, acc 0.9954\n",
            "epoch 4441: train D loss: 0.6847, train F loss: -0.6683, acc 0.9956\n",
            "epoch 4442: train D loss: 0.6868, train F loss: -0.6724, acc 0.9960\n",
            "epoch 4443: train D loss: 0.6865, train F loss: -0.6569, acc 0.9936\n",
            "epoch 4444: train D loss: 0.6867, train F loss: -0.6735, acc 0.9958\n",
            "epoch 4445: train D loss: 0.6830, train F loss: -0.6196, acc 0.9882\n",
            "epoch 4446: train D loss: 0.6827, train F loss: -0.6552, acc 0.9934\n",
            "epoch 4447: train D loss: 0.6833, train F loss: -0.6664, acc 0.9960\n",
            "epoch 4448: train D loss: 0.6843, train F loss: -0.6348, acc 0.9904\n",
            "epoch 4449: train D loss: 0.6854, train F loss: -0.6670, acc 0.9954\n",
            "epoch 4450: train D loss: 0.6843, train F loss: -0.6707, acc 0.9956\n",
            "epoch 4451: train D loss: 0.6840, train F loss: -0.6709, acc 0.9966\n",
            "epoch 4452: train D loss: 0.6848, train F loss: -0.6667, acc 0.9950\n",
            "epoch 4453: train D loss: 0.6856, train F loss: -0.6708, acc 0.9958\n",
            "epoch 4454: train D loss: 0.6844, train F loss: -0.6714, acc 0.9960\n",
            "epoch 4455: train D loss: 0.6857, train F loss: -0.6741, acc 0.9964\n",
            "epoch 4456: train D loss: 0.6843, train F loss: -0.6690, acc 0.9956\n",
            "epoch 4457: train D loss: 0.6862, train F loss: -0.6657, acc 0.9944\n",
            "epoch 4458: train D loss: 0.6856, train F loss: -0.6615, acc 0.9940\n",
            "epoch 4459: train D loss: 0.6857, train F loss: -0.6654, acc 0.9952\n",
            "epoch 4460: train D loss: 0.6857, train F loss: -0.6682, acc 0.9952\n",
            "epoch 4461: train D loss: 0.6862, train F loss: -0.6653, acc 0.9948\n",
            "epoch 4462: train D loss: 0.6841, train F loss: -0.6533, acc 0.9946\n",
            "epoch 4463: train D loss: 0.6841, train F loss: -0.6699, acc 0.9966\n",
            "epoch 4464: train D loss: 0.6888, train F loss: -0.6765, acc 0.9968\n",
            "epoch 4465: train D loss: 0.6850, train F loss: -0.6737, acc 0.9968\n",
            "epoch 4466: train D loss: 0.6840, train F loss: -0.6685, acc 0.9952\n",
            "epoch 4467: train D loss: 0.6858, train F loss: -0.6674, acc 0.9960\n",
            "epoch 4468: train D loss: 0.6865, train F loss: -0.6693, acc 0.9952\n",
            "epoch 4469: train D loss: 0.6855, train F loss: -0.6578, acc 0.9938\n",
            "epoch 4470: train D loss: 0.6882, train F loss: -0.6746, acc 0.9964\n",
            "epoch 4471: train D loss: 0.6862, train F loss: -0.6723, acc 0.9964\n",
            "epoch 4472: train D loss: 0.6867, train F loss: -0.6732, acc 0.9958\n",
            "epoch 4473: train D loss: 0.6835, train F loss: -0.6713, acc 0.9968\n",
            "epoch 4474: train D loss: 0.6863, train F loss: -0.6754, acc 0.9972\n",
            "epoch 4475: train D loss: 0.6887, train F loss: -0.6689, acc 0.9950\n",
            "epoch 4476: train D loss: 0.6876, train F loss: -0.6759, acc 0.9956\n",
            "epoch 4477: train D loss: 0.6865, train F loss: -0.6668, acc 0.9940\n",
            "epoch 4478: train D loss: 0.6862, train F loss: -0.6650, acc 0.9938\n",
            "epoch 4479: train D loss: 0.6885, train F loss: -0.6689, acc 0.9958\n",
            "epoch 4480: train D loss: 0.6855, train F loss: -0.6748, acc 0.9970\n",
            "epoch 4481: train D loss: 0.6866, train F loss: -0.6648, acc 0.9946\n",
            "epoch 4482: train D loss: 0.6892, train F loss: -0.6687, acc 0.9944\n",
            "epoch 4483: train D loss: 0.6839, train F loss: -0.6673, acc 0.9950\n",
            "epoch 4484: train D loss: 0.6884, train F loss: -0.6716, acc 0.9952\n",
            "epoch 4485: train D loss: 0.6867, train F loss: -0.6735, acc 0.9964\n",
            "epoch 4486: train D loss: 0.6847, train F loss: -0.6615, acc 0.9944\n",
            "epoch 4487: train D loss: 0.6829, train F loss: -0.6657, acc 0.9948\n",
            "epoch 4488: train D loss: 0.6862, train F loss: -0.6751, acc 0.9966\n",
            "epoch 4489: train D loss: 0.6869, train F loss: -0.6585, acc 0.9912\n",
            "epoch 4490: train D loss: 0.6874, train F loss: -0.6715, acc 0.9950\n",
            "epoch 4491: train D loss: 0.6876, train F loss: -0.6706, acc 0.9954\n",
            "epoch 4492: train D loss: 0.6844, train F loss: -0.6642, acc 0.9940\n",
            "epoch 4493: train D loss: 0.6853, train F loss: -0.6677, acc 0.9948\n",
            "epoch 4494: train D loss: 0.6870, train F loss: -0.6706, acc 0.9954\n",
            "epoch 4495: train D loss: 0.6851, train F loss: -0.6727, acc 0.9968\n",
            "epoch 4496: train D loss: 0.6844, train F loss: -0.6659, acc 0.9960\n",
            "epoch 4497: train D loss: 0.6836, train F loss: -0.6688, acc 0.9956\n",
            "epoch 4498: train D loss: 0.6850, train F loss: -0.6603, acc 0.9954\n",
            "epoch 4499: train D loss: 0.6875, train F loss: -0.6459, acc 0.9892\n",
            "epoch 4500: train D loss: 0.6829, train F loss: -0.6638, acc 0.9942\n",
            "epoch 4501: train D loss: 0.6838, train F loss: -0.6697, acc 0.9956\n",
            "epoch 4502: train D loss: 0.6833, train F loss: -0.6718, acc 0.9968\n",
            "epoch 4503: train D loss: 0.6827, train F loss: -0.6696, acc 0.9954\n",
            "epoch 4504: train D loss: 0.6867, train F loss: -0.6692, acc 0.9962\n",
            "epoch 4505: train D loss: 0.6876, train F loss: -0.6745, acc 0.9962\n",
            "epoch 4506: train D loss: 0.6864, train F loss: -0.6589, acc 0.9940\n",
            "epoch 4507: train D loss: 0.6866, train F loss: -0.6627, acc 0.9926\n",
            "epoch 4508: train D loss: 0.6843, train F loss: -0.6661, acc 0.9944\n",
            "epoch 4509: train D loss: 0.6833, train F loss: -0.6690, acc 0.9956\n",
            "epoch 4510: train D loss: 0.6865, train F loss: -0.6763, acc 0.9976\n",
            "epoch 4511: train D loss: 0.6874, train F loss: -0.6717, acc 0.9960\n",
            "epoch 4512: train D loss: 0.6853, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4513: train D loss: 0.6873, train F loss: -0.6738, acc 0.9958\n",
            "epoch 4514: train D loss: 0.6882, train F loss: -0.6768, acc 0.9962\n",
            "epoch 4515: train D loss: 0.6860, train F loss: -0.6737, acc 0.9966\n",
            "epoch 4516: train D loss: 0.6851, train F loss: -0.6742, acc 0.9966\n",
            "epoch 4517: train D loss: 0.6865, train F loss: -0.6659, acc 0.9944\n",
            "epoch 4518: train D loss: 0.6845, train F loss: -0.6711, acc 0.9966\n",
            "epoch 4519: train D loss: 0.6886, train F loss: -0.6711, acc 0.9950\n",
            "epoch 4520: train D loss: 0.6840, train F loss: -0.6670, acc 0.9946\n",
            "epoch 4521: train D loss: 0.6892, train F loss: -0.6718, acc 0.9948\n",
            "epoch 4522: train D loss: 0.6865, train F loss: -0.6701, acc 0.9950\n",
            "epoch 4523: train D loss: 0.6859, train F loss: -0.6726, acc 0.9962\n",
            "epoch 4524: train D loss: 0.6856, train F loss: -0.6702, acc 0.9958\n",
            "epoch 4525: train D loss: 0.6841, train F loss: -0.6632, acc 0.9942\n",
            "epoch 4526: train D loss: 0.6844, train F loss: -0.6687, acc 0.9954\n",
            "epoch 4527: train D loss: 0.6852, train F loss: -0.6729, acc 0.9966\n",
            "epoch 4528: train D loss: 0.6839, train F loss: -0.6699, acc 0.9966\n",
            "epoch 4529: train D loss: 0.6876, train F loss: -0.6719, acc 0.9952\n",
            "epoch 4530: train D loss: 0.6879, train F loss: -0.6718, acc 0.9968\n",
            "epoch 4531: train D loss: 0.6881, train F loss: -0.6751, acc 0.9962\n",
            "epoch 4532: train D loss: 0.6889, train F loss: -0.6652, acc 0.9940\n",
            "epoch 4533: train D loss: 0.6838, train F loss: -0.6663, acc 0.9948\n",
            "epoch 4534: train D loss: 0.6844, train F loss: -0.6637, acc 0.9950\n",
            "epoch 4535: train D loss: 0.6862, train F loss: -0.6711, acc 0.9952\n",
            "epoch 4536: train D loss: 0.6863, train F loss: -0.6747, acc 0.9966\n",
            "epoch 4537: train D loss: 0.6856, train F loss: -0.6721, acc 0.9962\n",
            "epoch 4538: train D loss: 0.6882, train F loss: -0.6662, acc 0.9944\n",
            "epoch 4539: train D loss: 0.6846, train F loss: -0.6700, acc 0.9954\n",
            "epoch 4540: train D loss: 0.6867, train F loss: -0.6685, acc 0.9950\n",
            "epoch 4541: train D loss: 0.6828, train F loss: -0.6663, acc 0.9960\n",
            "epoch 4542: train D loss: 0.6837, train F loss: -0.6693, acc 0.9956\n",
            "epoch 4543: train D loss: 0.6865, train F loss: -0.6652, acc 0.9946\n",
            "epoch 4544: train D loss: 0.6837, train F loss: -0.6632, acc 0.9942\n",
            "epoch 4545: train D loss: 0.6848, train F loss: -0.6599, acc 0.9938\n",
            "epoch 4546: train D loss: 0.6880, train F loss: -0.6640, acc 0.9938\n",
            "epoch 4547: train D loss: 0.6863, train F loss: -0.6607, acc 0.9946\n",
            "epoch 4548: train D loss: 0.6887, train F loss: -0.6610, acc 0.9916\n",
            "epoch 4549: train D loss: 0.6841, train F loss: -0.6636, acc 0.9932\n",
            "epoch 4550: train D loss: 0.6854, train F loss: -0.6657, acc 0.9946\n",
            "epoch 4551: train D loss: 0.6834, train F loss: -0.6655, acc 0.9950\n",
            "epoch 4552: train D loss: 0.6856, train F loss: -0.6707, acc 0.9964\n",
            "epoch 4553: train D loss: 0.6873, train F loss: -0.6740, acc 0.9964\n",
            "epoch 4554: train D loss: 0.6847, train F loss: -0.6746, acc 0.9966\n",
            "epoch 4555: train D loss: 0.6859, train F loss: -0.6712, acc 0.9958\n",
            "epoch 4556: train D loss: 0.6860, train F loss: -0.6584, acc 0.9932\n",
            "epoch 4557: train D loss: 0.6860, train F loss: -0.6645, acc 0.9938\n",
            "epoch 4558: train D loss: 0.6877, train F loss: -0.6726, acc 0.9950\n",
            "epoch 4559: train D loss: 0.6881, train F loss: -0.6632, acc 0.9940\n",
            "epoch 4560: train D loss: 0.6854, train F loss: -0.6454, acc 0.9888\n",
            "epoch 4561: train D loss: 0.6830, train F loss: -0.6651, acc 0.9956\n",
            "epoch 4562: train D loss: 0.6850, train F loss: -0.6681, acc 0.9954\n",
            "epoch 4563: train D loss: 0.6856, train F loss: -0.6672, acc 0.9940\n",
            "epoch 4564: train D loss: 0.6843, train F loss: -0.6681, acc 0.9952\n",
            "epoch 4565: train D loss: 0.6834, train F loss: -0.6698, acc 0.9958\n",
            "epoch 4566: train D loss: 0.6840, train F loss: -0.6720, acc 0.9970\n",
            "epoch 4567: train D loss: 0.6847, train F loss: -0.6666, acc 0.9956\n",
            "epoch 4568: train D loss: 0.6845, train F loss: -0.6651, acc 0.9954\n",
            "epoch 4569: train D loss: 0.6858, train F loss: -0.6678, acc 0.9956\n",
            "epoch 4570: train D loss: 0.6837, train F loss: -0.6644, acc 0.9958\n",
            "epoch 4571: train D loss: 0.6818, train F loss: -0.6668, acc 0.9968\n",
            "epoch 4572: train D loss: 0.6875, train F loss: -0.6728, acc 0.9954\n",
            "epoch 4573: train D loss: 0.6878, train F loss: -0.6764, acc 0.9962\n",
            "epoch 4574: train D loss: 0.6811, train F loss: -0.6666, acc 0.9960\n",
            "epoch 4575: train D loss: 0.6853, train F loss: -0.6698, acc 0.9956\n",
            "epoch 4576: train D loss: 0.6859, train F loss: -0.6732, acc 0.9956\n",
            "epoch 4577: train D loss: 0.6869, train F loss: -0.6635, acc 0.9944\n",
            "epoch 4578: train D loss: 0.6854, train F loss: -0.6706, acc 0.9966\n",
            "epoch 4579: train D loss: 0.6856, train F loss: -0.6684, acc 0.9950\n",
            "epoch 4580: train D loss: 0.6863, train F loss: -0.6685, acc 0.9952\n",
            "epoch 4581: train D loss: 0.6884, train F loss: -0.6674, acc 0.9940\n",
            "epoch 4582: train D loss: 0.6879, train F loss: -0.6765, acc 0.9962\n",
            "epoch 4583: train D loss: 0.6879, train F loss: -0.6752, acc 0.9962\n",
            "epoch 4584: train D loss: 0.6882, train F loss: -0.6735, acc 0.9954\n",
            "epoch 4585: train D loss: 0.6867, train F loss: -0.6661, acc 0.9952\n",
            "epoch 4586: train D loss: 0.6875, train F loss: -0.6734, acc 0.9962\n",
            "epoch 4587: train D loss: 0.6872, train F loss: -0.6694, acc 0.9954\n",
            "epoch 4588: train D loss: 0.6859, train F loss: -0.6660, acc 0.9940\n",
            "epoch 4589: train D loss: 0.6848, train F loss: -0.6741, acc 0.9964\n",
            "epoch 4590: train D loss: 0.6868, train F loss: -0.6674, acc 0.9958\n",
            "epoch 4591: train D loss: 0.6876, train F loss: -0.6728, acc 0.9946\n",
            "epoch 4592: train D loss: 0.6856, train F loss: -0.6692, acc 0.9952\n",
            "epoch 4593: train D loss: 0.6864, train F loss: -0.6478, acc 0.9914\n",
            "epoch 4594: train D loss: 0.6831, train F loss: -0.6638, acc 0.9944\n",
            "epoch 4595: train D loss: 0.6871, train F loss: -0.6648, acc 0.9946\n",
            "epoch 4596: train D loss: 0.6844, train F loss: -0.6659, acc 0.9946\n",
            "epoch 4597: train D loss: 0.6832, train F loss: -0.6643, acc 0.9952\n",
            "epoch 4598: train D loss: 0.6823, train F loss: -0.6663, acc 0.9946\n",
            "epoch 4599: train D loss: 0.6869, train F loss: -0.6746, acc 0.9960\n",
            "epoch 4600: train D loss: 0.6866, train F loss: -0.6704, acc 0.9952\n",
            "epoch 4601: train D loss: 0.6870, train F loss: -0.6659, acc 0.9938\n",
            "epoch 4602: train D loss: 0.6858, train F loss: -0.6664, acc 0.9950\n",
            "epoch 4603: train D loss: 0.6864, train F loss: -0.6675, acc 0.9958\n",
            "epoch 4604: train D loss: 0.6866, train F loss: -0.6665, acc 0.9944\n",
            "epoch 4605: train D loss: 0.6865, train F loss: -0.6712, acc 0.9952\n",
            "epoch 4606: train D loss: 0.6844, train F loss: -0.6695, acc 0.9956\n",
            "epoch 4607: train D loss: 0.6843, train F loss: -0.6722, acc 0.9968\n",
            "epoch 4608: train D loss: 0.6826, train F loss: -0.6685, acc 0.9964\n",
            "epoch 4609: train D loss: 0.6831, train F loss: -0.6664, acc 0.9952\n",
            "epoch 4610: train D loss: 0.6892, train F loss: -0.6678, acc 0.9954\n",
            "epoch 4611: train D loss: 0.6876, train F loss: -0.6618, acc 0.9918\n",
            "epoch 4612: train D loss: 0.6827, train F loss: -0.6673, acc 0.9952\n",
            "epoch 4613: train D loss: 0.6846, train F loss: -0.6717, acc 0.9966\n",
            "epoch 4614: train D loss: 0.6861, train F loss: -0.6603, acc 0.9932\n",
            "epoch 4615: train D loss: 0.6850, train F loss: -0.6713, acc 0.9964\n",
            "epoch 4616: train D loss: 0.6867, train F loss: -0.6699, acc 0.9952\n",
            "epoch 4617: train D loss: 0.6875, train F loss: -0.6685, acc 0.9946\n",
            "epoch 4618: train D loss: 0.6850, train F loss: -0.6644, acc 0.9952\n",
            "epoch 4619: train D loss: 0.6873, train F loss: -0.6675, acc 0.9952\n",
            "epoch 4620: train D loss: 0.6872, train F loss: -0.6721, acc 0.9956\n",
            "epoch 4621: train D loss: 0.6884, train F loss: -0.6730, acc 0.9954\n",
            "epoch 4622: train D loss: 0.6847, train F loss: -0.6747, acc 0.9974\n",
            "epoch 4623: train D loss: 0.6852, train F loss: -0.6735, acc 0.9966\n",
            "epoch 4624: train D loss: 0.6835, train F loss: -0.6697, acc 0.9960\n",
            "epoch 4625: train D loss: 0.6857, train F loss: -0.6742, acc 0.9964\n",
            "epoch 4626: train D loss: 0.6858, train F loss: -0.6692, acc 0.9946\n",
            "epoch 4627: train D loss: 0.6850, train F loss: -0.6725, acc 0.9964\n",
            "epoch 4628: train D loss: 0.6876, train F loss: -0.6710, acc 0.9952\n",
            "epoch 4629: train D loss: 0.6873, train F loss: -0.6683, acc 0.9952\n",
            "epoch 4630: train D loss: 0.6882, train F loss: -0.6641, acc 0.9940\n",
            "epoch 4631: train D loss: 0.6867, train F loss: -0.6682, acc 0.9944\n",
            "epoch 4632: train D loss: 0.6846, train F loss: -0.6531, acc 0.9942\n",
            "epoch 4633: train D loss: 0.6874, train F loss: -0.6726, acc 0.9958\n",
            "epoch 4634: train D loss: 0.6837, train F loss: -0.6685, acc 0.9962\n",
            "epoch 4635: train D loss: 0.6837, train F loss: -0.6640, acc 0.9948\n",
            "epoch 4636: train D loss: 0.6837, train F loss: -0.6665, acc 0.9956\n",
            "epoch 4637: train D loss: 0.6853, train F loss: -0.6668, acc 0.9948\n",
            "epoch 4638: train D loss: 0.6871, train F loss: -0.6705, acc 0.9958\n",
            "epoch 4639: train D loss: 0.6853, train F loss: -0.6726, acc 0.9966\n",
            "epoch 4640: train D loss: 0.6869, train F loss: -0.6727, acc 0.9960\n",
            "epoch 4641: train D loss: 0.6861, train F loss: -0.6664, acc 0.9950\n",
            "epoch 4642: train D loss: 0.6856, train F loss: -0.6708, acc 0.9962\n",
            "epoch 4643: train D loss: 0.6850, train F loss: -0.6721, acc 0.9966\n",
            "epoch 4644: train D loss: 0.6837, train F loss: -0.6678, acc 0.9952\n",
            "epoch 4645: train D loss: 0.6858, train F loss: -0.6568, acc 0.9928\n",
            "epoch 4646: train D loss: 0.6864, train F loss: -0.6597, acc 0.9938\n",
            "epoch 4647: train D loss: 0.6871, train F loss: -0.6643, acc 0.9946\n",
            "epoch 4648: train D loss: 0.6872, train F loss: -0.6697, acc 0.9948\n",
            "epoch 4649: train D loss: 0.6861, train F loss: -0.6718, acc 0.9962\n",
            "epoch 4650: train D loss: 0.6859, train F loss: -0.6679, acc 0.9948\n",
            "epoch 4651: train D loss: 0.6879, train F loss: -0.6693, acc 0.9958\n",
            "epoch 4652: train D loss: 0.6869, train F loss: -0.6631, acc 0.9936\n",
            "epoch 4653: train D loss: 0.6869, train F loss: -0.6588, acc 0.9940\n",
            "epoch 4654: train D loss: 0.6845, train F loss: -0.6652, acc 0.9938\n",
            "epoch 4655: train D loss: 0.6850, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4656: train D loss: 0.6845, train F loss: -0.6651, acc 0.9954\n",
            "epoch 4657: train D loss: 0.6869, train F loss: -0.6669, acc 0.9940\n",
            "epoch 4658: train D loss: 0.6851, train F loss: -0.6698, acc 0.9964\n",
            "epoch 4659: train D loss: 0.6859, train F loss: -0.6720, acc 0.9960\n",
            "epoch 4660: train D loss: 0.6867, train F loss: -0.6709, acc 0.9958\n",
            "epoch 4661: train D loss: 0.6878, train F loss: -0.6758, acc 0.9962\n",
            "epoch 4662: train D loss: 0.6840, train F loss: -0.6654, acc 0.9948\n",
            "epoch 4663: train D loss: 0.6891, train F loss: -0.6701, acc 0.9950\n",
            "epoch 4664: train D loss: 0.6879, train F loss: -0.6646, acc 0.9940\n",
            "epoch 4665: train D loss: 0.6863, train F loss: -0.6667, acc 0.9942\n",
            "epoch 4666: train D loss: 0.6863, train F loss: -0.6710, acc 0.9956\n",
            "epoch 4667: train D loss: 0.6860, train F loss: -0.6723, acc 0.9952\n",
            "epoch 4668: train D loss: 0.6831, train F loss: -0.6340, acc 0.9908\n",
            "epoch 4669: train D loss: 0.6853, train F loss: -0.6686, acc 0.9950\n",
            "epoch 4670: train D loss: 0.6864, train F loss: -0.6675, acc 0.9944\n",
            "epoch 4671: train D loss: 0.6856, train F loss: -0.6683, acc 0.9950\n",
            "epoch 4672: train D loss: 0.6843, train F loss: -0.6730, acc 0.9970\n",
            "epoch 4673: train D loss: 0.6876, train F loss: -0.6733, acc 0.9964\n",
            "epoch 4674: train D loss: 0.6847, train F loss: -0.6734, acc 0.9970\n",
            "epoch 4675: train D loss: 0.6867, train F loss: -0.6714, acc 0.9962\n",
            "epoch 4676: train D loss: 0.6839, train F loss: -0.6642, acc 0.9946\n",
            "epoch 4677: train D loss: 0.6823, train F loss: -0.6627, acc 0.9954\n",
            "epoch 4678: train D loss: 0.6858, train F loss: -0.6715, acc 0.9958\n",
            "epoch 4679: train D loss: 0.6882, train F loss: -0.6636, acc 0.9946\n",
            "epoch 4680: train D loss: 0.6892, train F loss: -0.6744, acc 0.9954\n",
            "epoch 4681: train D loss: 0.6848, train F loss: -0.6701, acc 0.9954\n",
            "epoch 4682: train D loss: 0.6848, train F loss: -0.6564, acc 0.9926\n",
            "epoch 4683: train D loss: 0.6827, train F loss: -0.6666, acc 0.9954\n",
            "epoch 4684: train D loss: 0.6869, train F loss: -0.6722, acc 0.9962\n",
            "epoch 4685: train D loss: 0.6846, train F loss: -0.6660, acc 0.9956\n",
            "epoch 4686: train D loss: 0.6886, train F loss: -0.6713, acc 0.9948\n",
            "epoch 4687: train D loss: 0.6837, train F loss: -0.6682, acc 0.9960\n",
            "epoch 4688: train D loss: 0.6880, train F loss: -0.6766, acc 0.9966\n",
            "epoch 4689: train D loss: 0.6848, train F loss: -0.6670, acc 0.9958\n",
            "epoch 4690: train D loss: 0.6868, train F loss: -0.6549, acc 0.9936\n",
            "epoch 4691: train D loss: 0.6864, train F loss: -0.6720, acc 0.9962\n",
            "epoch 4692: train D loss: 0.6856, train F loss: -0.6673, acc 0.9946\n",
            "epoch 4693: train D loss: 0.6869, train F loss: -0.6675, acc 0.9962\n",
            "epoch 4694: train D loss: 0.6856, train F loss: -0.6690, acc 0.9956\n",
            "epoch 4695: train D loss: 0.6870, train F loss: -0.6704, acc 0.9948\n",
            "epoch 4696: train D loss: 0.6882, train F loss: -0.6674, acc 0.9946\n",
            "epoch 4697: train D loss: 0.6854, train F loss: -0.6731, acc 0.9966\n",
            "epoch 4698: train D loss: 0.6874, train F loss: -0.6697, acc 0.9946\n",
            "epoch 4699: train D loss: 0.6865, train F loss: -0.6670, acc 0.9946\n",
            "epoch 4700: train D loss: 0.6864, train F loss: -0.6685, acc 0.9950\n",
            "epoch 4701: train D loss: 0.6842, train F loss: -0.6699, acc 0.9958\n",
            "epoch 4702: train D loss: 0.6866, train F loss: -0.6668, acc 0.9960\n",
            "epoch 4703: train D loss: 0.6852, train F loss: -0.6606, acc 0.9942\n",
            "epoch 4704: train D loss: 0.6846, train F loss: -0.6648, acc 0.9944\n",
            "epoch 4705: train D loss: 0.6882, train F loss: -0.6635, acc 0.9944\n",
            "epoch 4706: train D loss: 0.6870, train F loss: -0.6634, acc 0.9938\n",
            "epoch 4707: train D loss: 0.6861, train F loss: -0.6737, acc 0.9964\n",
            "epoch 4708: train D loss: 0.6898, train F loss: -0.6780, acc 0.9962\n",
            "epoch 4709: train D loss: 0.6890, train F loss: -0.6789, acc 0.9972\n",
            "epoch 4710: train D loss: 0.6873, train F loss: -0.6733, acc 0.9958\n",
            "epoch 4711: train D loss: 0.6878, train F loss: -0.6713, acc 0.9952\n",
            "epoch 4712: train D loss: 0.6883, train F loss: -0.6716, acc 0.9958\n",
            "epoch 4713: train D loss: 0.6848, train F loss: -0.6710, acc 0.9952\n",
            "epoch 4714: train D loss: 0.6854, train F loss: -0.6721, acc 0.9954\n",
            "epoch 4715: train D loss: 0.6838, train F loss: -0.6636, acc 0.9966\n",
            "epoch 4716: train D loss: 0.6848, train F loss: -0.6729, acc 0.9960\n",
            "epoch 4717: train D loss: 0.6885, train F loss: -0.6706, acc 0.9956\n",
            "epoch 4718: train D loss: 0.6834, train F loss: -0.6725, acc 0.9972\n",
            "epoch 4719: train D loss: 0.6853, train F loss: -0.6714, acc 0.9960\n",
            "epoch 4720: train D loss: 0.6874, train F loss: -0.6485, acc 0.9928\n",
            "epoch 4721: train D loss: 0.6844, train F loss: -0.6519, acc 0.9916\n",
            "epoch 4722: train D loss: 0.6815, train F loss: -0.6555, acc 0.9924\n",
            "epoch 4723: train D loss: 0.6828, train F loss: -0.6645, acc 0.9950\n",
            "epoch 4724: train D loss: 0.6854, train F loss: -0.6637, acc 0.9952\n",
            "epoch 4725: train D loss: 0.6861, train F loss: -0.6671, acc 0.9946\n",
            "epoch 4726: train D loss: 0.6850, train F loss: -0.6740, acc 0.9968\n",
            "epoch 4727: train D loss: 0.6879, train F loss: -0.6657, acc 0.9942\n",
            "epoch 4728: train D loss: 0.6851, train F loss: -0.6665, acc 0.9944\n",
            "epoch 4729: train D loss: 0.6859, train F loss: -0.6603, acc 0.9940\n",
            "epoch 4730: train D loss: 0.6847, train F loss: -0.6695, acc 0.9958\n",
            "epoch 4731: train D loss: 0.6864, train F loss: -0.6679, acc 0.9958\n",
            "epoch 4732: train D loss: 0.6859, train F loss: -0.6691, acc 0.9954\n",
            "epoch 4733: train D loss: 0.6854, train F loss: -0.6736, acc 0.9966\n",
            "epoch 4734: train D loss: 0.6827, train F loss: -0.6677, acc 0.9958\n",
            "epoch 4735: train D loss: 0.6833, train F loss: -0.6719, acc 0.9972\n",
            "epoch 4736: train D loss: 0.6854, train F loss: -0.6707, acc 0.9962\n",
            "epoch 4737: train D loss: 0.6858, train F loss: -0.6188, acc 0.9870\n",
            "epoch 4738: train D loss: 0.6845, train F loss: -0.6573, acc 0.9926\n",
            "epoch 4739: train D loss: 0.6865, train F loss: -0.6715, acc 0.9958\n",
            "epoch 4740: train D loss: 0.6868, train F loss: -0.6744, acc 0.9960\n",
            "epoch 4741: train D loss: 0.6870, train F loss: -0.6745, acc 0.9964\n",
            "epoch 4742: train D loss: 0.6849, train F loss: -0.6710, acc 0.9954\n",
            "epoch 4743: train D loss: 0.6861, train F loss: -0.6759, acc 0.9970\n",
            "epoch 4744: train D loss: 0.6859, train F loss: -0.6752, acc 0.9966\n",
            "epoch 4745: train D loss: 0.6870, train F loss: -0.6747, acc 0.9966\n",
            "epoch 4746: train D loss: 0.6843, train F loss: -0.6680, acc 0.9956\n",
            "epoch 4747: train D loss: 0.6886, train F loss: -0.6769, acc 0.9966\n",
            "epoch 4748: train D loss: 0.6876, train F loss: -0.6721, acc 0.9960\n",
            "epoch 4749: train D loss: 0.6893, train F loss: -0.6749, acc 0.9958\n",
            "epoch 4750: train D loss: 0.6877, train F loss: -0.6674, acc 0.9944\n",
            "epoch 4751: train D loss: 0.6852, train F loss: -0.6686, acc 0.9944\n",
            "epoch 4752: train D loss: 0.6848, train F loss: -0.6719, acc 0.9968\n",
            "epoch 4753: train D loss: 0.6834, train F loss: -0.6633, acc 0.9956\n",
            "epoch 4754: train D loss: 0.6862, train F loss: -0.6694, acc 0.9950\n",
            "epoch 4755: train D loss: 0.6869, train F loss: -0.6735, acc 0.9962\n",
            "epoch 4756: train D loss: 0.6862, train F loss: -0.6712, acc 0.9958\n",
            "epoch 4757: train D loss: 0.6844, train F loss: -0.6698, acc 0.9952\n",
            "epoch 4758: train D loss: 0.6883, train F loss: -0.6665, acc 0.9950\n",
            "epoch 4759: train D loss: 0.6873, train F loss: -0.6645, acc 0.9946\n",
            "epoch 4760: train D loss: 0.6849, train F loss: -0.6638, acc 0.9942\n",
            "epoch 4761: train D loss: 0.6871, train F loss: -0.6679, acc 0.9956\n",
            "epoch 4762: train D loss: 0.6806, train F loss: -0.6615, acc 0.9962\n",
            "epoch 4763: train D loss: 0.6808, train F loss: -0.6522, acc 0.9946\n",
            "epoch 4764: train D loss: 0.6852, train F loss: -0.6679, acc 0.9954\n",
            "epoch 4765: train D loss: 0.6855, train F loss: -0.6610, acc 0.9944\n",
            "epoch 4766: train D loss: 0.6798, train F loss: -0.6670, acc 0.9964\n",
            "epoch 4767: train D loss: 0.6855, train F loss: -0.6693, acc 0.9956\n",
            "epoch 4768: train D loss: 0.6858, train F loss: -0.6734, acc 0.9970\n",
            "epoch 4769: train D loss: 0.6871, train F loss: -0.6753, acc 0.9964\n",
            "epoch 4770: train D loss: 0.6880, train F loss: -0.6731, acc 0.9956\n",
            "epoch 4771: train D loss: 0.6839, train F loss: -0.6730, acc 0.9968\n",
            "epoch 4772: train D loss: 0.6862, train F loss: -0.6683, acc 0.9956\n",
            "epoch 4773: train D loss: 0.6879, train F loss: -0.6603, acc 0.9940\n",
            "epoch 4774: train D loss: 0.6863, train F loss: -0.6657, acc 0.9944\n",
            "epoch 4775: train D loss: 0.6844, train F loss: -0.6722, acc 0.9966\n",
            "epoch 4776: train D loss: 0.6825, train F loss: -0.6669, acc 0.9966\n",
            "epoch 4777: train D loss: 0.6840, train F loss: -0.6693, acc 0.9952\n",
            "epoch 4778: train D loss: 0.6890, train F loss: -0.6736, acc 0.9952\n",
            "epoch 4779: train D loss: 0.6881, train F loss: -0.6744, acc 0.9960\n",
            "epoch 4780: train D loss: 0.6860, train F loss: -0.6717, acc 0.9958\n",
            "epoch 4781: train D loss: 0.6875, train F loss: -0.6713, acc 0.9958\n",
            "epoch 4782: train D loss: 0.6851, train F loss: -0.6749, acc 0.9968\n",
            "epoch 4783: train D loss: 0.6865, train F loss: -0.6756, acc 0.9964\n",
            "epoch 4784: train D loss: 0.6868, train F loss: -0.6755, acc 0.9966\n",
            "epoch 4785: train D loss: 0.6880, train F loss: -0.6669, acc 0.9950\n",
            "epoch 4786: train D loss: 0.6873, train F loss: -0.6725, acc 0.9950\n",
            "epoch 4787: train D loss: 0.6850, train F loss: -0.6559, acc 0.9934\n",
            "epoch 4788: train D loss: 0.6890, train F loss: -0.6709, acc 0.9944\n",
            "epoch 4789: train D loss: 0.6888, train F loss: -0.6720, acc 0.9952\n",
            "epoch 4790: train D loss: 0.6865, train F loss: -0.6707, acc 0.9960\n",
            "epoch 4791: train D loss: 0.6864, train F loss: -0.6695, acc 0.9952\n",
            "epoch 4792: train D loss: 0.6870, train F loss: -0.6666, acc 0.9940\n",
            "epoch 4793: train D loss: 0.6875, train F loss: -0.6758, acc 0.9964\n",
            "epoch 4794: train D loss: 0.6841, train F loss: -0.6695, acc 0.9956\n",
            "epoch 4795: train D loss: 0.6852, train F loss: -0.6660, acc 0.9948\n",
            "epoch 4796: train D loss: 0.6854, train F loss: -0.6664, acc 0.9946\n",
            "epoch 4797: train D loss: 0.6854, train F loss: -0.6729, acc 0.9962\n",
            "epoch 4798: train D loss: 0.6860, train F loss: -0.6679, acc 0.9954\n",
            "epoch 4799: train D loss: 0.6862, train F loss: -0.6716, acc 0.9956\n",
            "epoch 4800: train D loss: 0.6843, train F loss: -0.6590, acc 0.9942\n",
            "epoch 4801: train D loss: 0.6877, train F loss: -0.6745, acc 0.9958\n",
            "epoch 4802: train D loss: 0.6869, train F loss: -0.6775, acc 0.9972\n",
            "epoch 4803: train D loss: 0.6881, train F loss: -0.6751, acc 0.9960\n",
            "epoch 4804: train D loss: 0.6862, train F loss: -0.6698, acc 0.9958\n",
            "epoch 4805: train D loss: 0.6843, train F loss: -0.6666, acc 0.9950\n",
            "epoch 4806: train D loss: 0.6866, train F loss: -0.6730, acc 0.9958\n",
            "epoch 4807: train D loss: 0.6876, train F loss: -0.6733, acc 0.9956\n",
            "epoch 4808: train D loss: 0.6839, train F loss: -0.6725, acc 0.9964\n",
            "epoch 4809: train D loss: 0.6863, train F loss: -0.6736, acc 0.9956\n",
            "epoch 4810: train D loss: 0.6860, train F loss: -0.6555, acc 0.9938\n",
            "epoch 4811: train D loss: 0.6855, train F loss: -0.6094, acc 0.9862\n",
            "epoch 4812: train D loss: 0.6869, train F loss: -0.6657, acc 0.9946\n",
            "epoch 4813: train D loss: 0.6858, train F loss: -0.6653, acc 0.9946\n",
            "epoch 4814: train D loss: 0.6849, train F loss: -0.6662, acc 0.9954\n",
            "epoch 4815: train D loss: 0.6864, train F loss: -0.6707, acc 0.9954\n",
            "epoch 4816: train D loss: 0.6845, train F loss: -0.6568, acc 0.9932\n",
            "epoch 4817: train D loss: 0.6841, train F loss: -0.6683, acc 0.9956\n",
            "epoch 4818: train D loss: 0.6866, train F loss: -0.6751, acc 0.9972\n",
            "epoch 4819: train D loss: 0.6858, train F loss: -0.6733, acc 0.9970\n",
            "epoch 4820: train D loss: 0.6859, train F loss: -0.6718, acc 0.9966\n",
            "epoch 4821: train D loss: 0.6858, train F loss: -0.6682, acc 0.9958\n",
            "epoch 4822: train D loss: 0.6861, train F loss: -0.6741, acc 0.9964\n",
            "epoch 4823: train D loss: 0.6853, train F loss: -0.6686, acc 0.9950\n",
            "epoch 4824: train D loss: 0.6867, train F loss: -0.6720, acc 0.9956\n",
            "epoch 4825: train D loss: 0.6848, train F loss: -0.6712, acc 0.9960\n",
            "epoch 4826: train D loss: 0.6876, train F loss: -0.6736, acc 0.9962\n",
            "epoch 4827: train D loss: 0.6876, train F loss: -0.6755, acc 0.9958\n",
            "epoch 4828: train D loss: 0.6866, train F loss: -0.6662, acc 0.9946\n",
            "epoch 4829: train D loss: 0.6882, train F loss: -0.6738, acc 0.9962\n",
            "epoch 4830: train D loss: 0.6891, train F loss: -0.6658, acc 0.9942\n",
            "epoch 4831: train D loss: 0.6859, train F loss: -0.6686, acc 0.9942\n",
            "epoch 4832: train D loss: 0.6828, train F loss: -0.6685, acc 0.9954\n",
            "epoch 4833: train D loss: 0.6849, train F loss: -0.6679, acc 0.9954\n",
            "epoch 4834: train D loss: 0.6883, train F loss: -0.6705, acc 0.9954\n",
            "epoch 4835: train D loss: 0.6819, train F loss: -0.6678, acc 0.9974\n",
            "epoch 4836: train D loss: 0.6855, train F loss: -0.6683, acc 0.9952\n",
            "epoch 4837: train D loss: 0.6852, train F loss: -0.6749, acc 0.9970\n",
            "epoch 4838: train D loss: 0.6858, train F loss: -0.6664, acc 0.9954\n",
            "epoch 4839: train D loss: 0.6853, train F loss: -0.6670, acc 0.9944\n",
            "epoch 4840: train D loss: 0.6887, train F loss: -0.6752, acc 0.9952\n",
            "epoch 4841: train D loss: 0.6889, train F loss: -0.6725, acc 0.9950\n",
            "epoch 4842: train D loss: 0.6848, train F loss: -0.6725, acc 0.9962\n",
            "epoch 4843: train D loss: 0.6877, train F loss: -0.6648, acc 0.9946\n",
            "epoch 4844: train D loss: 0.6897, train F loss: -0.6692, acc 0.9938\n",
            "epoch 4845: train D loss: 0.6865, train F loss: -0.6714, acc 0.9958\n",
            "epoch 4846: train D loss: 0.6851, train F loss: -0.6708, acc 0.9958\n",
            "epoch 4847: train D loss: 0.6869, train F loss: -0.6670, acc 0.9936\n",
            "epoch 4848: train D loss: 0.6863, train F loss: -0.6467, acc 0.9922\n",
            "epoch 4849: train D loss: 0.6841, train F loss: -0.6662, acc 0.9960\n",
            "epoch 4850: train D loss: 0.6850, train F loss: -0.6691, acc 0.9950\n",
            "epoch 4851: train D loss: 0.6868, train F loss: -0.6750, acc 0.9970\n",
            "epoch 4852: train D loss: 0.6838, train F loss: -0.6640, acc 0.9944\n",
            "epoch 4853: train D loss: 0.6857, train F loss: -0.6656, acc 0.9950\n",
            "epoch 4854: train D loss: 0.6859, train F loss: -0.6621, acc 0.9944\n",
            "epoch 4855: train D loss: 0.6876, train F loss: -0.6706, acc 0.9950\n",
            "epoch 4856: train D loss: 0.6862, train F loss: -0.6749, acc 0.9962\n",
            "epoch 4857: train D loss: 0.6871, train F loss: -0.6736, acc 0.9962\n",
            "epoch 4858: train D loss: 0.6861, train F loss: -0.6755, acc 0.9968\n",
            "epoch 4859: train D loss: 0.6874, train F loss: -0.6731, acc 0.9958\n",
            "epoch 4860: train D loss: 0.6877, train F loss: -0.6743, acc 0.9956\n",
            "epoch 4861: train D loss: 0.6841, train F loss: -0.6695, acc 0.9956\n",
            "epoch 4862: train D loss: 0.6857, train F loss: -0.6718, acc 0.9960\n",
            "epoch 4863: train D loss: 0.6880, train F loss: -0.6734, acc 0.9962\n",
            "epoch 4864: train D loss: 0.6893, train F loss: -0.6673, acc 0.9950\n",
            "epoch 4865: train D loss: 0.6839, train F loss: -0.6671, acc 0.9952\n",
            "epoch 4866: train D loss: 0.6861, train F loss: -0.6701, acc 0.9958\n",
            "epoch 4867: train D loss: 0.6852, train F loss: -0.6604, acc 0.9936\n",
            "epoch 4868: train D loss: 0.6866, train F loss: -0.6625, acc 0.9946\n",
            "epoch 4869: train D loss: 0.6862, train F loss: -0.6722, acc 0.9946\n",
            "epoch 4870: train D loss: 0.6857, train F loss: -0.6540, acc 0.9938\n",
            "epoch 4871: train D loss: 0.6838, train F loss: -0.6672, acc 0.9952\n",
            "epoch 4872: train D loss: 0.6870, train F loss: -0.6725, acc 0.9958\n",
            "epoch 4873: train D loss: 0.6869, train F loss: -0.6713, acc 0.9962\n",
            "epoch 4874: train D loss: 0.6834, train F loss: -0.6692, acc 0.9956\n",
            "epoch 4875: train D loss: 0.6873, train F loss: -0.6749, acc 0.9968\n",
            "epoch 4876: train D loss: 0.6868, train F loss: -0.6717, acc 0.9962\n",
            "epoch 4877: train D loss: 0.6866, train F loss: -0.6693, acc 0.9950\n",
            "epoch 4878: train D loss: 0.6846, train F loss: -0.6693, acc 0.9962\n",
            "epoch 4879: train D loss: 0.6905, train F loss: -0.6737, acc 0.9950\n",
            "epoch 4880: train D loss: 0.6874, train F loss: -0.6724, acc 0.9958\n",
            "epoch 4881: train D loss: 0.6883, train F loss: -0.6711, acc 0.9956\n",
            "epoch 4882: train D loss: 0.6846, train F loss: -0.6514, acc 0.9950\n",
            "epoch 4883: train D loss: 0.6882, train F loss: -0.6638, acc 0.9926\n",
            "epoch 4884: train D loss: 0.6887, train F loss: -0.6702, acc 0.9946\n",
            "epoch 4885: train D loss: 0.6852, train F loss: -0.6672, acc 0.9950\n",
            "epoch 4886: train D loss: 0.6854, train F loss: -0.6730, acc 0.9962\n",
            "epoch 4887: train D loss: 0.6877, train F loss: -0.6688, acc 0.9956\n",
            "epoch 4888: train D loss: 0.6873, train F loss: -0.6752, acc 0.9968\n",
            "epoch 4889: train D loss: 0.6858, train F loss: -0.6659, acc 0.9948\n",
            "epoch 4890: train D loss: 0.6859, train F loss: -0.6692, acc 0.9948\n",
            "epoch 4891: train D loss: 0.6862, train F loss: -0.6738, acc 0.9962\n",
            "epoch 4892: train D loss: 0.6862, train F loss: -0.6679, acc 0.9954\n",
            "epoch 4893: train D loss: 0.6850, train F loss: -0.6697, acc 0.9962\n",
            "epoch 4894: train D loss: 0.6884, train F loss: -0.6787, acc 0.9974\n",
            "epoch 4895: train D loss: 0.6884, train F loss: -0.6701, acc 0.9940\n",
            "epoch 4896: train D loss: 0.6855, train F loss: -0.6753, acc 0.9968\n",
            "epoch 4897: train D loss: 0.6908, train F loss: -0.6411, acc 0.9884\n",
            "epoch 4898: train D loss: 0.6855, train F loss: -0.6686, acc 0.9952\n",
            "epoch 4899: train D loss: 0.6849, train F loss: -0.6665, acc 0.9944\n",
            "epoch 4900: train D loss: 0.6843, train F loss: -0.6670, acc 0.9950\n",
            "epoch 4901: train D loss: 0.6900, train F loss: -0.6700, acc 0.9950\n",
            "epoch 4902: train D loss: 0.6850, train F loss: -0.6659, acc 0.9934\n",
            "epoch 4903: train D loss: 0.6863, train F loss: -0.6732, acc 0.9962\n",
            "epoch 4904: train D loss: 0.6840, train F loss: -0.6711, acc 0.9968\n",
            "epoch 4905: train D loss: 0.6819, train F loss: -0.6628, acc 0.9956\n",
            "epoch 4906: train D loss: 0.6864, train F loss: -0.6691, acc 0.9960\n",
            "epoch 4907: train D loss: 0.6871, train F loss: -0.6755, acc 0.9966\n",
            "epoch 4908: train D loss: 0.6841, train F loss: -0.6715, acc 0.9962\n",
            "epoch 4909: train D loss: 0.6851, train F loss: -0.6728, acc 0.9962\n",
            "epoch 4910: train D loss: 0.6868, train F loss: -0.6711, acc 0.9964\n",
            "epoch 4911: train D loss: 0.6869, train F loss: -0.6667, acc 0.9950\n",
            "epoch 4912: train D loss: 0.6858, train F loss: -0.6744, acc 0.9962\n",
            "epoch 4913: train D loss: 0.6851, train F loss: -0.6734, acc 0.9962\n",
            "epoch 4914: train D loss: 0.6879, train F loss: -0.6708, acc 0.9956\n",
            "epoch 4915: train D loss: 0.6864, train F loss: -0.6750, acc 0.9968\n",
            "epoch 4916: train D loss: 0.6850, train F loss: -0.6495, acc 0.9924\n",
            "epoch 4917: train D loss: 0.6866, train F loss: -0.6734, acc 0.9960\n",
            "epoch 4918: train D loss: 0.6824, train F loss: -0.6633, acc 0.9942\n",
            "epoch 4919: train D loss: 0.6871, train F loss: -0.6679, acc 0.9940\n",
            "epoch 4920: train D loss: 0.6870, train F loss: -0.6641, acc 0.9948\n",
            "epoch 4921: train D loss: 0.6882, train F loss: -0.6706, acc 0.9940\n",
            "epoch 4922: train D loss: 0.6877, train F loss: -0.6711, acc 0.9958\n",
            "epoch 4923: train D loss: 0.6859, train F loss: -0.6746, acc 0.9968\n",
            "epoch 4924: train D loss: 0.6874, train F loss: -0.6554, acc 0.9924\n",
            "epoch 4925: train D loss: 0.6887, train F loss: -0.6666, acc 0.9944\n",
            "epoch 4926: train D loss: 0.6846, train F loss: -0.6545, acc 0.9926\n",
            "epoch 4927: train D loss: 0.6861, train F loss: -0.6692, acc 0.9960\n",
            "epoch 4928: train D loss: 0.6840, train F loss: -0.6682, acc 0.9958\n",
            "epoch 4929: train D loss: 0.6903, train F loss: -0.6496, acc 0.9900\n",
            "epoch 4930: train D loss: 0.6895, train F loss: -0.6638, acc 0.9918\n",
            "epoch 4931: train D loss: 0.6844, train F loss: -0.6632, acc 0.9948\n",
            "epoch 4932: train D loss: 0.6843, train F loss: -0.6695, acc 0.9958\n",
            "epoch 4933: train D loss: 0.6867, train F loss: -0.6686, acc 0.9944\n",
            "epoch 4934: train D loss: 0.6856, train F loss: -0.6649, acc 0.9948\n",
            "epoch 4935: train D loss: 0.6881, train F loss: -0.6710, acc 0.9954\n",
            "epoch 4936: train D loss: 0.6856, train F loss: -0.6722, acc 0.9958\n",
            "epoch 4937: train D loss: 0.6880, train F loss: -0.6750, acc 0.9968\n",
            "epoch 4938: train D loss: 0.6866, train F loss: -0.6706, acc 0.9954\n",
            "epoch 4939: train D loss: 0.6863, train F loss: -0.6737, acc 0.9960\n",
            "epoch 4940: train D loss: 0.6851, train F loss: -0.6757, acc 0.9970\n",
            "epoch 4941: train D loss: 0.6861, train F loss: -0.6731, acc 0.9960\n",
            "epoch 4942: train D loss: 0.6881, train F loss: -0.6763, acc 0.9966\n",
            "epoch 4943: train D loss: 0.6877, train F loss: -0.6760, acc 0.9964\n",
            "epoch 4944: train D loss: 0.6872, train F loss: -0.6788, acc 0.9976\n",
            "epoch 4945: train D loss: 0.6876, train F loss: -0.6598, acc 0.9954\n",
            "epoch 4946: train D loss: 0.6864, train F loss: -0.6663, acc 0.9940\n",
            "epoch 4947: train D loss: 0.6840, train F loss: -0.6676, acc 0.9952\n",
            "epoch 4948: train D loss: 0.6846, train F loss: -0.6707, acc 0.9956\n",
            "epoch 4949: train D loss: 0.6852, train F loss: -0.6709, acc 0.9954\n",
            "epoch 4950: train D loss: 0.6870, train F loss: -0.6737, acc 0.9964\n",
            "epoch 4951: train D loss: 0.6883, train F loss: -0.6499, acc 0.9916\n",
            "epoch 4952: train D loss: 0.6861, train F loss: -0.6611, acc 0.9930\n",
            "epoch 4953: train D loss: 0.6832, train F loss: -0.6724, acc 0.9968\n",
            "epoch 4954: train D loss: 0.6874, train F loss: -0.6713, acc 0.9948\n",
            "epoch 4955: train D loss: 0.6890, train F loss: -0.6710, acc 0.9950\n",
            "epoch 4956: train D loss: 0.6854, train F loss: -0.6717, acc 0.9962\n",
            "epoch 4957: train D loss: 0.6871, train F loss: -0.6746, acc 0.9964\n",
            "epoch 4958: train D loss: 0.6875, train F loss: -0.6716, acc 0.9950\n",
            "epoch 4959: train D loss: 0.6844, train F loss: -0.6701, acc 0.9954\n",
            "epoch 4960: train D loss: 0.6856, train F loss: -0.6594, acc 0.9934\n",
            "epoch 4961: train D loss: 0.6882, train F loss: -0.6728, acc 0.9950\n",
            "epoch 4962: train D loss: 0.6839, train F loss: -0.6717, acc 0.9966\n",
            "epoch 4963: train D loss: 0.6834, train F loss: -0.6669, acc 0.9948\n",
            "epoch 4964: train D loss: 0.6875, train F loss: -0.6727, acc 0.9960\n",
            "epoch 4965: train D loss: 0.6880, train F loss: -0.6712, acc 0.9958\n",
            "epoch 4966: train D loss: 0.6879, train F loss: -0.6615, acc 0.9928\n",
            "epoch 4967: train D loss: 0.6843, train F loss: -0.6569, acc 0.9922\n",
            "epoch 4968: train D loss: 0.6853, train F loss: -0.6721, acc 0.9958\n",
            "epoch 4969: train D loss: 0.6866, train F loss: -0.6766, acc 0.9972\n",
            "epoch 4970: train D loss: 0.6875, train F loss: -0.6608, acc 0.9950\n",
            "epoch 4971: train D loss: 0.6848, train F loss: -0.6698, acc 0.9962\n",
            "epoch 4972: train D loss: 0.6867, train F loss: -0.6718, acc 0.9952\n",
            "epoch 4973: train D loss: 0.6858, train F loss: -0.6621, acc 0.9942\n",
            "epoch 4974: train D loss: 0.6872, train F loss: -0.6620, acc 0.9942\n",
            "epoch 4975: train D loss: 0.6847, train F loss: -0.6679, acc 0.9960\n",
            "epoch 4976: train D loss: 0.6850, train F loss: -0.6729, acc 0.9962\n",
            "epoch 4977: train D loss: 0.6860, train F loss: -0.6728, acc 0.9964\n",
            "epoch 4978: train D loss: 0.6872, train F loss: -0.6719, acc 0.9956\n",
            "epoch 4979: train D loss: 0.6861, train F loss: -0.6709, acc 0.9956\n",
            "epoch 4980: train D loss: 0.6871, train F loss: -0.6647, acc 0.9952\n",
            "epoch 4981: train D loss: 0.6881, train F loss: -0.6753, acc 0.9962\n",
            "epoch 4982: train D loss: 0.6848, train F loss: -0.6723, acc 0.9960\n",
            "epoch 4983: train D loss: 0.6815, train F loss: -0.6664, acc 0.9954\n",
            "epoch 4984: train D loss: 0.6873, train F loss: -0.6767, acc 0.9966\n",
            "epoch 4985: train D loss: 0.6913, train F loss: -0.6771, acc 0.9958\n",
            "epoch 4986: train D loss: 0.6895, train F loss: -0.6709, acc 0.9954\n",
            "epoch 4987: train D loss: 0.6886, train F loss: -0.6532, acc 0.9920\n",
            "epoch 4988: train D loss: 0.6860, train F loss: -0.6684, acc 0.9960\n",
            "epoch 4989: train D loss: 0.6862, train F loss: -0.6655, acc 0.9944\n",
            "epoch 4990: train D loss: 0.6834, train F loss: -0.6672, acc 0.9958\n",
            "epoch 4991: train D loss: 0.6871, train F loss: -0.6693, acc 0.9956\n",
            "epoch 4992: train D loss: 0.6848, train F loss: -0.6728, acc 0.9966\n",
            "epoch 4993: train D loss: 0.6848, train F loss: -0.6734, acc 0.9964\n",
            "epoch 4994: train D loss: 0.6884, train F loss: -0.6751, acc 0.9954\n",
            "epoch 4995: train D loss: 0.6842, train F loss: -0.6652, acc 0.9952\n",
            "epoch 4996: train D loss: 0.6861, train F loss: -0.6702, acc 0.9948\n",
            "epoch 4997: train D loss: 0.6885, train F loss: -0.6736, acc 0.9962\n",
            "epoch 4998: train D loss: 0.6846, train F loss: -0.6555, acc 0.9934\n",
            "epoch 4999: train D loss: 0.6871, train F loss: -0.6708, acc 0.9960\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 5000\n",
        "\n",
        "# train num_epoch\n",
        "for epoch in range(num_epoch):\n",
        "    # You should chooose lamnda cleverly.\n",
        "    lamb = adaptive_lambda(epoch, num_epoch)\n",
        "    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)\n",
        "\n",
        "    torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n",
        "\n",
        "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(feature_extractor.state_dict(), f'extractor_model_final.bin')\n",
        "torch.save(label_predictor.state_dict(), f'predictor_model_final.bin')\n",
        "torch.save(domain_classifier.state_dict(), f'domain_classifier_final.bin')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_-0iSSje4w"
      },
      "source": [
        "# Inference\n",
        "\n",
        "We use pandas to generate our csv file.\n",
        "\n",
        "BTW, the performance of the model trained for 200 epoches might be unstable. You can train for more epoches for a more stable performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33,
          "referenced_widgets": [
            "33f2860c16b24fc2b00024c65762cdea",
            "588fabe7b91542deb1c03fb01542f5b7"
          ]
        },
        "id": "Wly5AgH2jePv",
        "outputId": "915abd58-54c1-4dbf-b035-b19e266fefc8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a4f9a0d35604ce08b660d7cd56b0c49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result = []\n",
        "label_predictor.eval()\n",
        "feature_extractor.eval()\n",
        "with Progress(TextColumn(\"[progress.description]{task.description}\"),\n",
        "              BarColumn(),\n",
        "              TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
        "              TimeRemainingColumn(),\n",
        "              TimeElapsedColumn()) as progress:\n",
        "    test_tqdm = progress.add_task(description=\"inference progress\", total=len(test_dataloader))\n",
        "    for i, (test_data, _) in (enumerate(test_dataloader)):\n",
        "        test_data = test_data.cuda()\n",
        "\n",
        "        class_logits = label_predictor(feature_extractor(test_data))\n",
        "\n",
        "        x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
        "        result.append(x)\n",
        "        progress.advance(test_tqdm)\n",
        "\n",
        "import pandas as pd\n",
        "result = np.concatenate(result)\n",
        "\n",
        "# Generate your submission\n",
        "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
        "df.to_csv('DaNN_submission.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_X8v0wojx1jD"
      },
      "source": [
        "# Visualization\n",
        "We use t-SNE plot to observe the distribution of extracted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KkRtbyEOyYSN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import manifold"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-8NNrBEUjjz3"
      },
      "source": [
        "## Step1: Load checkpoint and evaluate to get extracted features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "lgO5O7uZjcCw"
      },
      "outputs": [],
      "source": [
        "# Hints:\n",
        "# Set features_extractor to eval mode\n",
        "# Load saved checkpoints\n",
        "# Start evaluation and collect features and labels\n",
        "label_predictor.load_state_dict(torch.load('predictor_model_mid.bin'))\n",
        "feature_extractor.load_state_dict(torch.load('extractor_model_mid.bin'))\n",
        "label_predictor.eval()\n",
        "feature_extractor.eval()\n",
        "for i, (test_data, _) in enumerate(test_dataloader):\n",
        "    test_data = test_data.cuda()\n",
        "    feature = feature_extractor(test_data)\n",
        "    class_logits = label_predictor(feature)\n",
        "    \n",
        "    feature = feature.detach().cpu().numpy()\n",
        "    label = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
        "    \n",
        "    \n",
        "    if i==0: \n",
        "        features = feature\n",
        "        labels = label\n",
        "    else: \n",
        "        features = np.concatenate((features, feature))\n",
        "        labels = np.concatenate((labels, label))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb9TmH3Wkh5P"
      },
      "source": [
        "## Step2: Apply t-SNE and normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "edkTrdlri1MS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 100000 samples in 0.019s...\n",
            "[t-SNE] Computed neighbors for 100000 samples in 12.296s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 6000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 7000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 8000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 9000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 10000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 11000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 12000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 13000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 14000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 15000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 16000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 17000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 18000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 19000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 20000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 21000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 22000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 23000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 24000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 25000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 26000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 27000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 28000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 29000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 30000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 31000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 32000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 33000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 34000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 35000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 36000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 37000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 38000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 39000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 40000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 41000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 42000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 43000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 44000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 45000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 46000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 47000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 48000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 49000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 50000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 51000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 52000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 53000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 54000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 55000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 56000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 57000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 58000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 59000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 60000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 61000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 62000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 63000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 64000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 65000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 66000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 67000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 68000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 69000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 70000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 71000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 72000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 73000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 74000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 75000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 76000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 77000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 78000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 79000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 80000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 81000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 82000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 83000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 84000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 85000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 86000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 87000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 88000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 89000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 90000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 91000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 92000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 93000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 94000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 95000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 96000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 97000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 98000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 99000 / 100000\n",
            "[t-SNE] Computed conditional probabilities for sample 100000 / 100000\n",
            "[t-SNE] Mean sigma: 2.077763\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 109.679138\n",
            "[t-SNE] KL divergence after 1000 iterations: 3.565822\n"
          ]
        }
      ],
      "source": [
        "# process extracted features with t-SNE\n",
        "X_tsne = manifold.TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(features)\n",
        "\n",
        "# Normalization the processed features \n",
        "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
        "X_norm = (X_tsne - x_min) / (x_max - x_min)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zoujX3uxk79a"
      },
      "source": [
        "## Step3: Visualization with matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "V1dgPoDkjrLc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 2)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data Visualization\n",
        "# Use matplotlib to plot the distribution\n",
        "# The shape of X_norm is (N,2)\n",
        "X_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "tx = X_norm[:,0]\n",
        "ty = X_norm[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# initialize a matplotlib plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# for every class, we'll add a scatter plot separately\n",
        "for label in range(9):\n",
        "    # find the samples of the current class in the data\n",
        "    indices = [i for i, l in enumerate(labels) if l == label]\n",
        "\n",
        "    # extract the coordinates of the points of this class only\n",
        "    current_tx = np.take(tx, indices)\n",
        "    current_ty = np.take(ty, indices)\n",
        "\n",
        "    # convert the class color to matplotlib format\n",
        "    color = plt.cm.Set3(label)\n",
        "\n",
        "    # add a scatter plot with the corresponding color and label\n",
        "    ax.scatter(current_tx, current_ty, color=color, label=label)\n",
        "\n",
        "# build a legend using the labels we set previously\n",
        "ax.legend(loc='best')\n",
        "\n",
        "# finally, show the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_extractor.load_state_dict(torch.load('extractor_model_mid.bin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5120\n",
            "10016\n"
          ]
        }
      ],
      "source": [
        "# Hints:\n",
        "# Set features_extractor to eval mode\n",
        "# Start evaluation and collect features and labels\n",
        "domain_classifier.eval()\n",
        "feature_extractor.eval()\n",
        "for i, (test_data, _) in enumerate(test_dataloader):\n",
        "    test_data = test_data.cuda()\n",
        "    feature = feature_extractor(test_data)\n",
        "    \n",
        "    feature = feature.detach().cpu().numpy()\n",
        "    label = np.zeros(feature.shape[0])\n",
        "    \n",
        "    \n",
        "    if i==0: \n",
        "        features = feature\n",
        "        labels = label\n",
        "    else: \n",
        "        features = np.concatenate((features, feature))\n",
        "        labels = np.concatenate((labels, label))\n",
        "        \n",
        "    if len(labels) >= 5000: break\n",
        "        \n",
        "print(len(labels))\n",
        "        \n",
        "for i, (target_data, _) in enumerate(target_dataloader):\n",
        "    target_data = target_data.cuda()\n",
        "    feature = feature_extractor(target_data)\n",
        "    \n",
        "    feature = feature.detach().cpu().numpy()\n",
        "    label = np.ones(feature.shape[0])\n",
        "    \n",
        "    \n",
        "\n",
        "    features = np.concatenate((features, feature))\n",
        "    labels = np.concatenate((labels, label))\n",
        "        \n",
        "    if len(labels) >= 10000: break\n",
        "        \n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 10016 samples in 0.002s...\n",
            "[t-SNE] Computed neighbors for 10016 samples in 0.861s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 6000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 7000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 8000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 9000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 10000 / 10016\n",
            "[t-SNE] Computed conditional probabilities for sample 10016 / 10016\n",
            "[t-SNE] Mean sigma: 2.506041\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 86.767181\n",
            "[t-SNE] KL divergence after 1000 iterations: 2.079753\n"
          ]
        }
      ],
      "source": [
        "# process extracted features with t-SNE \n",
        "X_tsne = manifold.TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(features)\n",
        "\n",
        "# Normalization the processed features \n",
        "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
        "X_norm = (X_tsne - x_min) / (x_max - x_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5120\n",
            "4896\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tx = X_norm[:,0]\n",
        "ty = X_norm[:,1]\n",
        "\n",
        "# Data Visualization\n",
        "# Use matplotlib to plot the distribution\n",
        "# The shape of X_norm is (N,2)\n",
        "\n",
        "# initialize a matplotlib plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# for every class, we'll add a scatter plot separately\n",
        "for label in range(2):\n",
        "    # find the samples of the current class in the data\n",
        "    indices = [i for i, l in enumerate(labels) if l == label]\n",
        "    print(len(indices))\n",
        "    # extract the coordinates of the points of this class only\n",
        "    current_tx = np.take(tx, indices)\n",
        "    current_ty = np.take(ty, indices)\n",
        "\n",
        "    # convert the class color to matplotlib format\n",
        "    color = plt.cm.Set3(label)\n",
        "\n",
        "    # add a scatter plot with the corresponding color and label\n",
        "    ax.scatter(current_tx, current_ty, color=color, label=label)\n",
        "\n",
        "# build a legend using the labels we set previously\n",
        "ax.legend(loc='best')\n",
        "\n",
        "# finally, show the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CpheoH_rvFbO"
      },
      "source": [
        "# Training Statistics\n",
        "\n",
        "- Number of parameters:\n",
        "  - Feature Extractor: 2, 142, 336\n",
        "  - Label Predictor: 530, 442\n",
        "  - Domain Classifier: 1, 055, 233\n",
        "\n",
        "- Simple\n",
        " - Training time on colab: ~ 1 hr\n",
        "- Medium\n",
        " - Training time on colab: 2 ~ 4 hr\n",
        "- Strong\n",
        " - Training time on colab: 5 ~ 6 hrs\n",
        "- Boss\n",
        " - **Unmeasurable**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GYO8InxavGsy"
      },
      "source": [
        "# Learning Curve (Strong Baseline)\n",
        "* This method is slightly different from colab.\n",
        "\n",
        "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
        "\n",
        "# Accuracy Curve (Strong Baseline)\n",
        "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
        "\n",
        "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s6UfXzef-wNl"
      },
      "source": [
        "# Q&A\n",
        "\n",
        "If there is any problem related to Domain Adaptation, please email to b08902047@ntu.edu.tw / mlta-2023-spring@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4TMXG_YCqVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c865516acc64642862cfee4a814a3ae": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ea4e9e6cd3794596932e43e41caa30f9",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">epoch progress <span style=\"color: #f92672; text-decoration-color: #f92672\"></span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:10</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:31:38</span>\nbatch_progress <span style=\"color: #f92672; text-decoration-color: #f92672\"></span> <span style=\"color: #800080; text-decoration-color: #800080\"> 99%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:09</span>\n</pre>\n",
                  "text/plain": "epoch progress \u001b[38;2;249;38;114m\u001b[0m\u001b[38;2;249;38;114m\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:10\u001b[0m \u001b[33m0:31:38\u001b[0m\nbatch_progress \u001b[38;2;249;38;114m\u001b[0m\u001b[38;2;249;38;114m\u001b[0m \u001b[35m 99%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:09\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "33f2860c16b24fc2b00024c65762cdea": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_588fabe7b91542deb1c03fb01542f5b7",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">inference progress <span style=\"color: #f92672; text-decoration-color: #f92672\"></span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:52</span>\n</pre>\n",
                  "text/plain": "inference progress \u001b[38;2;249;38;114m\u001b[0m\u001b[38;2;249;38;114m\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:52\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "588fabe7b91542deb1c03fb01542f5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4e9e6cd3794596932e43e41caa30f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
